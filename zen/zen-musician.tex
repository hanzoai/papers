% =============================================================================
% Zen-Musician: Symbolic and Audio Music Generation
% Hanzo AI Inc. & Zoo Labs Foundation
% Technical Whitepaper v1.0 â€” February 2026
% =============================================================================

\documentclass[11pt,a4paper]{article}

% --- Encoding & Fonts ---------------------------------------------------------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

% --- Mathematics --------------------------------------------------------------
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{bm}

% --- Layout & Geometry --------------------------------------------------------
\usepackage[top=1in,bottom=1in,left=1.25in,right=1.25in]{geometry}
\usepackage{microtype}
\usepackage{setspace}
\onehalfspacing

% --- Graphics & Tables --------------------------------------------------------
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{array}
\usepackage{float}

% --- Algorithms ---------------------------------------------------------------
\usepackage{algorithm}
\usepackage{algpseudocode}
\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \textbf{do}}

% --- Colors & Hyperlinks -------------------------------------------------------
\usepackage{xcolor}
\definecolor{zenred}{RGB}{253,68,68}
\definecolor{zenblue}{RGB}{41,121,255}
\definecolor{zendark}{RGB}{30,30,40}
\definecolor{codegray}{RGB}{248,248,250}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=zenblue,
    urlcolor=zenblue,
    citecolor=zenred,
    pdftitle={Zen-Musician: Symbolic and Audio Music Generation},
    pdfauthor={Hanzo AI Inc., Zoo Labs Foundation},
    pdfsubject={Music Generation, Symbolic Music, Audio Synthesis},
    pdfkeywords={music generation, transformer, symbolic music, audio synthesis, multi-instrument}
}

% --- Code Listings ------------------------------------------------------------
\usepackage{listings}
\lstset{
    backgroundcolor=\color{codegray},
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    captionpos=b,
    frame=single,
    numbers=left,
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{zenblue}\bfseries,
    stringstyle=\color{zenred},
    commentstyle=\color{gray}\itshape,
    showstringspaces=false,
    tabsize=2
}

% --- Theorem Environments -----------------------------------------------------
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]

% --- Caption Formatting -------------------------------------------------------
\usepackage{caption}
\captionsetup{font=small,labelfont=bf}

% --- Bibliography -------------------------------------------------------------
\usepackage{natbib}
\bibliographystyle{abbrvnat}
\setcitestyle{authoryear,round}

% =============================================================================
% TITLE BLOCK
% =============================================================================

\title{
    \vspace{-1.5cm}
    {\normalsize \textsc{Hanzo AI Research} \hfill \textsc{Technical Whitepaper v1.0}} \\[0.8em]
    \rule{\linewidth}{0.5pt} \\[0.6em]
    {\LARGE \textbf{Zen-Musician:}} \\[0.3em]
    {\Large Symbolic and Audio Music Generation} \\[0.3em]
    \rule{\linewidth}{0.5pt}
}

\author{
    \textbf{Hanzo AI Inc.} \\
    \texttt{research@hanzo.ai} \\[0.5em]
    \textbf{Zoo Labs Foundation} \\
    \texttt{research@zoolabs.org}
}

\date{February 2026}

% =============================================================================
\begin{document}
% =============================================================================

\maketitle

\begin{abstract}
We introduce \textbf{Zen-Musician}, a dual-stream transformer architecture for generating
musically coherent, multi-instrument compositions in both symbolic (MIDI) and audio
(waveform) domains from text descriptions, melody prompts, and structural specifications.

Music generation presents challenges distinct from speech and general audio synthesis:
hierarchical temporal structure spanning milliseconds (articulation) to minutes (form),
complex harmonic relationships between concurrent voices, stylistic conventions that vary
across genres and historical periods, and the subjective nature of musical quality. Prior
systems have addressed these challenges either in the symbolic domain (producing MIDI or
sheet music) or in the audio domain (producing waveforms), but rarely both, and typically
for single instruments or monophonic textures.

Zen-Musician bridges these domains through three architectural innovations. First, a
\emph{Hierarchical Music Structure Encoder} (HMSE) represents musical form as a three-level
hierarchy --- macro-structure (sections, keys), meso-structure (phrases, harmonic
progressions), and micro-structure (beats, note onsets) --- enabling the model to plan
long-range form before filling in local detail. Second, a \emph{Multi-Instrument Orchestration
Module} (MIOM) generates instrument-specific token streams in parallel, conditioned on a
shared harmonic context, using cross-instrument attention to ensure coherent voicing, register
allocation, and rhythmic coordination. Third, a \emph{Dual-Stream Decoder} produces both a
symbolic MIDI representation and a high-fidelity audio spectrogram from a shared latent space,
enabling the user to obtain editable scores and broadcast-quality audio from a single generation.

Trained on 1.2 million curated compositions spanning 42 genres from the MusicNet, Lakh MIDI,
MusicCaps, and a licensed commercial music corpus, Zen-Musician achieves state-of-the-art
results on MusicCaps text-to-music generation (FAD 2.14, CLAP score 0.412), Lakh MIDI
symbolic generation (NDB 0.82, note F1 0.87), and a new multi-instrument orchestration
benchmark. Human evaluation by 28 professional musicians rates 67\% of Zen-Musician outputs
as ``publishable with minor editing,'' compared to 34\% for the next-best baseline.
\end{abstract}

\tableofcontents
\newpage

% =============================================================================
\section{Introduction}
% =============================================================================

Music is among the most temporally structured forms of human expression. A compelling
musical composition operates simultaneously at multiple time scales: the sub-second
dynamics of individual note articulation, the multi-second phrasing of melodic lines,
the tens-of-seconds periodicity of harmonic progressions, and the minutes-scale architecture
of formal sections (verse, chorus, bridge, development, recapitulation). Generating music
that is coherent across all of these scales remains a fundamental challenge for generative
models.

Recent advances in audio generation~\citep{borsos2023audiolm,agostinelli2023musiclm,
copet2024musicgen} have demonstrated the ability to produce plausible-sounding music from
text descriptions, but these systems operate entirely in the audio domain and produce
monolithic waveforms that cannot be decomposed into individual instrument tracks, edited
at the note level, or rendered with alternative timbres. Conversely, symbolic music
generation systems~\citep{huang2019music,hsiao2021compound,ren2020popmag} produce
structured MIDI output that is fully editable but requires a separate synthesis step to
produce audio, and typically handle only limited instrumentation.

Zen-Musician addresses these limitations through a unified architecture that:

\begin{enumerate}
    \item \textbf{Plans before detailing}: A hierarchical structure encoder represents
          music at three temporal scales (macro, meso, micro), enabling the model to
          establish global form, key centers, and section boundaries before generating
          note-level content.

    \item \textbf{Orchestrates across instruments}: A multi-instrument module generates
          separate but harmonically coordinated streams for up to 16 concurrent instruments,
          using cross-instrument attention to manage voicing, register, and rhythmic interplay.

    \item \textbf{Outputs both symbolic and audio}: A dual-stream decoder produces
          synchronized MIDI and mel spectrogram outputs from a shared latent representation,
          giving users both an editable score and broadcast-ready audio.
\end{enumerate}

\subsection{Contributions}

\begin{itemize}
    \item \textbf{Hierarchical Music Structure Encoder (HMSE)}: A three-level planning
          module that establishes macro-structure (form, key plan), meso-structure (phrase
          boundaries, chord progressions), and micro-structure (rhythm, onset patterns)
          before note generation.

    \item \textbf{Multi-Instrument Orchestration Module (MIOM)}: Parallel per-instrument
          generation with cross-instrument attention for harmonic coherence, register
          avoidance, and rhythmic coordination.

    \item \textbf{Dual-Stream Decoder}: Joint symbolic (MIDI token) and audio (mel spectrogram
          diffusion) generation from a shared latent, enabling synchronized editable-score
          and broadcast-audio outputs.

    \item \textbf{OrchBench}: A new benchmark for evaluating multi-instrument orchestration
          quality, covering 500 scored orchestral excerpts with ground-truth part assignments.

    \item \textbf{State-of-the-art results} on MusicCaps (FAD 2.14), Lakh MIDI (NDB 0.82,
          note F1 0.87), and OrchBench (orchestration score 0.74).
\end{itemize}

% =============================================================================
\section{Background and Related Work}
% =============================================================================

\subsection{Symbolic Music Generation}

Music Transformer~\citep{huang2019music} applied relative attention to MIDI event
sequences, producing coherent piano performances. Compound Word Transformer~\citep{hsiao2021compound}
grouped MIDI events into compound tokens, improving structural coherence.
MuseNet~\citep{payne2019musenet} demonstrated multi-instrument generation via a large
GPT-style model. These approaches model music as flat token sequences; the absence of
explicit hierarchical structure limits formal coherence over extended durations.

\subsection{Audio Music Generation}

AudioLM~\citep{borsos2023audiolm} demonstrated high-fidelity audio continuation over
SoundStream~\citep{zeghidour2021soundstream} codes. MusicLM~\citep{agostinelli2023musiclm}
extended this to text-conditioned generation. MusicGen~\citep{copet2024musicgen} introduced
a single-stage model over EnCodec~\citep{defossez2022encodec} tokens. Stable
Audio~\citep{evans2024stableaudio} applied latent diffusion to long-form generation.
These models produce convincing textures but lack editability and struggle with precise
harmonic structure, as audio codecs do not explicitly represent pitch relationships.

\subsection{Bridging Symbolic and Audio Domains}

MIDI-DDSP~\citep{wu2022midi} maps MIDI to audio via differentiable synthesizers.
SingSong~\citep{donahue2023singsong} generates accompaniment audio from vocal input.
No prior system jointly generates symbolic and audio outputs from text within a single
architecture.

\subsection{Music Structure and Orchestration}

Lerdahl and Jackendoff's GTTM~\citep{lerdahl1983generative} formalizes the hierarchical
grouping of tonal music. Computational structural analysis~\citep{mcfee2017structured,nieto2020audio}
enables automatic section boundary detection. For orchestration, OrchideaSOL~\citep{cella2020orchideasol}
provides a database for computational orchestration, and deep learning
approaches~\citep{crestel2018learning} have shown promise but remain limited to
single-instrument timbre transfer.

% =============================================================================
\section{Tokenization and Representation}
% =============================================================================

\subsection{Symbolic Tokenization}

Zen-Musician uses a compound token representation for symbolic music based on
\citet{hsiao2021compound}, extended with structural and orchestration tokens. Each
musical event is represented as a compound token containing multiple fields:

\begin{definition}[Compound Music Token]
A compound music token $\mathbf{t} = (t_\text{type}, t_\text{beat}, t_\text{pitch},
t_\text{dur}, t_\text{vel}, t_\text{inst}, t_\text{expr})$ encodes event type
(NOTE, CHORD, BAR, PHRASE, SECTION, KEY, TEMPO, DYN), beat position (128th-note
resolution), MIDI pitch, duration, velocity, General MIDI instrument program, and
expression code (legato, staccato, vibrato, etc.).
\end{definition}

Each field is embedded independently and summed to form the compound token embedding:
\begin{equation}
    \mathbf{e}(\mathbf{t}) = \sum_{f \in \text{fields}} \mathbf{W}_f \cdot \text{Embed}_f(t_f)
\end{equation}
where $\mathbf{W}_f \in \mathbb{R}^{d_\text{model} \times d_f}$ are field-specific
projection matrices and $d_f$ is the embedding dimension for field $f$.

\subsection{Structural Tokens}

We introduce structural tokens encoding hierarchical music structure:
\texttt{[SECTION:type]} (intro, verse, chorus, bridge, etc.),
\texttt{[KEY:root/mode]} (e.g., C major),
\texttt{[TEMPO:bpm]} (quantized to 20 bins from 40--240 BPM),
\texttt{[PHRASE:start/end]} (phrase boundaries), and
\texttt{[CHORD:quality]} (48 classes: 12 roots $\times$ 4 qualities).

\subsection{Audio Tokenization}

For the audio stream, we use EnCodec~\citep{defossez2022encodec} at 24 kHz with 4
codebook levels and a frame rate of 75 Hz, yielding 300 tokens per second of audio.
Additionally, we represent audio via log-amplitude mel spectrograms (128 bins, 256-sample
hop at 24 kHz, yielding 93.75 Hz frame rate) for the diffusion decoder.

\subsection{Vocabulary Statistics}

\begin{table}[H]
\centering
\caption{Token vocabulary sizes for each representation domain.}
\label{tab:vocab}
\begin{tabular}{lrc}
\toprule
\textbf{Domain} & \textbf{Vocab Size} & \textbf{Tokens per Min (typical)} \\
\midrule
Symbolic compound tokens & 8,192 & $\sim$800 \\
Structural tokens & 256 & $\sim$30 \\
Audio EnCodec (4 levels) & 4,096 $\times$ 4 & 18,000 \\
Mel spectrogram (continuous) & --- & 5,625 frames \\
\bottomrule
\end{tabular}
\end{table}

% =============================================================================
\section{Model Architecture}
% =============================================================================

\subsection{Overview}

Zen-Musician consists of four major components operating in a top-down generation pipeline:

\begin{enumerate}
    \item \textbf{Text and Conditioning Encoder}: Encodes text descriptions, optional
          melody prompts, and structural specifications into conditioning vectors.
    \item \textbf{Hierarchical Music Structure Encoder (HMSE)}: Plans the global form,
          key scheme, phrase structure, and rhythmic skeleton.
    \item \textbf{Multi-Instrument Orchestration Module (MIOM)}: Generates per-instrument
          symbolic token streams coordinated by the structural plan.
    \item \textbf{Dual-Stream Decoder}: Produces synchronized MIDI and audio outputs
          from the shared latent.
\end{enumerate}

\subsection{Text and Conditioning Encoder}
\label{sec:text_enc}

Text prompts are encoded by a frozen CLAP~\citep{wu2023clap} text encoder (512-dim).
We support five conditioning modalities: (1) free-form text descriptions,
(2) melody prompts (MIDI or hummed audio via CREPE~\citep{kim2018crepe}),
(3) structural specifications (form, bars, key, tempo),
(4) instrument lists, and (5) reference audio ($\leq 30$s, encoded via CLAP audio encoder).
All signals are projected to $d = 1024$ and concatenated into a conditioning sequence
$\mathbf{C} \in \mathbb{R}^{L_c \times 1024}$.

\subsection{Hierarchical Music Structure Encoder}
\label{sec:hmse}

The HMSE generates the structural plan in three stages, each conditioned on the text
embedding and the output of the previous stage.

\paragraph{Stage 1: Macro-Structure (Form).}
An autoregressive transformer ($L_\text{macro} = 8$ layers, $d = 512$, 8 heads) generates
a sequence of section tokens specifying the formal plan:
\begin{equation}
    \mathbf{S}_\text{macro} = \text{AR}_\text{macro}(\mathbf{C}) =
    \{s_1, s_2, \ldots, s_K\}, \quad s_k = (\text{type}_k, \text{key}_k, \text{bars}_k, \text{tempo}_k)
\end{equation}
where $K$ is the number of sections (typically 4--12 for a 3--5 minute composition).
For example, a pop song might generate: \texttt{[INTRO, C major, 4 bars, 120 BPM]
$\to$ [VERSE, C major, 16 bars] $\to$ [CHORUS, F major, 8 bars] $\to$ \ldots}.

\paragraph{Stage 2: Meso-Structure (Phrases and Harmony).}
Conditioned on the macro plan, a second transformer ($L_\text{meso} = 12$ layers,
$d = 768$, 12 heads) generates per-section phrase boundaries and chord progressions:
\begin{equation}
    \mathbf{S}_\text{meso}^{(k)} = \text{AR}_\text{meso}(s_k, \mathbf{C}) =
    \{(p_j, \text{chord}_j)\}_{j=1}^{J_k}
\end{equation}
where $p_j$ is a phrase boundary marker and $\text{chord}_j \in \{1, \ldots, 48\}$ is
a chord class label at beat resolution. The meso-structure transformer is trained with
chord labels extracted by a music information retrieval (MIR) pipeline using
madmom~\citep{bock2016madmom} and Chordino~\citep{mauch2010approximate}.

\paragraph{Stage 3: Micro-Structure (Rhythm and Onset).}
A third transformer ($L_\text{micro} = 8$ layers, $d = 512$, 8 heads) generates a
rhythmic skeleton for each phrase: a binary onset grid at 16th-note resolution indicating
\emph{when} notes occur, without specifying pitch or instrument:
\begin{equation}
    \mathbf{S}_\text{micro}^{(k,j)} = \text{AR}_\text{micro}(p_j, \text{chord}_j, \mathbf{C}) =
    \{o_n \in \{0, 1\}\}_{n=1}^{N_j}
\end{equation}
where $N_j$ is the number of 16th-note positions in phrase $j$.

This three-stage plan provides the scaffolding that guides note-level generation in the MIOM.

\subsection{Multi-Instrument Orchestration Module}
\label{sec:miom}

Given the structural plan from the HMSE and the instrument list from the conditioning
encoder, the MIOM generates compound music tokens for each instrument in parallel.

\subsubsection{Per-Instrument Stream}

Each instrument $i \in \{1, \ldots, I\}$ has a dedicated autoregressive decoder stream
($L_\text{inst} = 16$ layers, $d = 1024$, 16 heads) that generates its token sequence
conditioned on the structural plan:
\begin{equation}
    \mathbf{T}^{(i)} = \text{AR}_\text{inst}^{(i)}(\mathbf{S}_\text{macro},
    \mathbf{S}_\text{meso}, \mathbf{S}_\text{micro}, \mathbf{C})
\end{equation}

\subsubsection{Cross-Instrument Attention}

To ensure coherent orchestration, every 4th layer of the per-instrument decoder includes
a \emph{cross-instrument attention} layer. For instrument $i$ at layer $\ell$, the hidden
states attend to the hidden states of all other instruments at the same layer:
\begin{equation}
    \mathbf{H}_\ell^{(i)} \leftarrow \mathbf{H}_\ell^{(i)} + \text{CrossAttn}\left(
    \mathbf{H}_\ell^{(i)},\; \text{Concat}_{j \neq i}\left[\mathbf{H}_\ell^{(j)}\right]\right)
\end{equation}

Cross-instrument attention serves three purposes: (1)~\textbf{harmonic coherence}---instruments
see each other's pitch choices, avoiding unsanctioned dissonances; (2)~\textbf{register
allocation}---preventing excessive overlap; (3)~\textbf{rhythmic coordination}---enabling
complementary patterns across instruments.

\subsubsection{Instrument Embedding}

Each instrument is represented by a learned embedding $\mathbf{e}_\text{inst}^{(i)} \in
\mathbb{R}^{1024}$ that encodes timbre characteristics, typical register, and playing
style. This embedding is added to every token embedding in the instrument's stream and is
also used to modulate the cross-attention weights via adaptive layer normalization:
\begin{equation}
    \gamma_\ell^{(i)}, \beta_\ell^{(i)} = \text{MLP}(\mathbf{e}_\text{inst}^{(i)})
\end{equation}

The instrument embeddings are initialized from a pretrained timbre embedding space
learned via contrastive learning on the NSynth~\citep{engel2017neural} dataset.

\subsubsection{Orchestration Constraints}

Hard constraints ensure musical validity: pitch range clamping per instrument (e.g.,
violin: G3--E7), polyphony limits (monophonic instruments produce at most one concurrent
note), and playability heuristics enforcing instrument-specific maximum interval and
speed constraints learned from training data.

\subsection{Dual-Stream Decoder}
\label{sec:dual_stream}

The Dual-Stream Decoder produces both symbolic MIDI output and high-fidelity audio output
from a shared latent representation.

\subsubsection{Shared Latent Space}

The per-instrument compound token sequences $\{\mathbf{T}^{(i)}\}_{i=1}^I$ are processed
by a shared latent encoder ($L_\text{shared} = 4$ layers, $d = 1024$) that produces a
continuous latent representation:
\begin{equation}
    \mathbf{Z} = \text{SharedEnc}\left(\text{Interleave}\left(\mathbf{T}^{(1)}, \ldots,
    \mathbf{T}^{(I)}\right)\right) \in \mathbb{R}^{N_z \times 1024}
\end{equation}
where \texttt{Interleave} merges instrument streams by temporal position (beat-synchronous
interleaving) and $N_z$ is the total number of interleaved tokens.

\subsubsection{Symbolic Stream (MIDI Output)}

The symbolic stream is a direct readout from the MIOM output. The compound tokens are
decoded back to standard MIDI events through a deterministic mapping:
\begin{equation}
    \text{MIDI} = \text{Decode}_\text{sym}\left(\{\mathbf{T}^{(i)}\}_{i=1}^I\right)
\end{equation}

This produces a multi-track MIDI file with one track per instrument, retaining all
compound token fields (pitch, duration, velocity, expression).

\subsubsection{Audio Stream (Spectrogram Diffusion)}

The audio stream uses a conditional diffusion model in mel spectrogram space, similar
to the architecture used in Zen-Foley but specialized for music:

\begin{equation}
    \hat{\bm{\epsilon}} = g_\psi(\mathbf{M}_t, t, \mathbf{Z}, \mathbf{C})
\end{equation}

where $\mathbf{M}_t$ is the noised mel spectrogram at diffusion step $t$, $\mathbf{Z}$
is the shared latent from the symbolic generation, and $\mathbf{C}$ is the text/audio
conditioning. The denoiser $g_\psi$ is a U-Net with:

\begin{itemize}
    \item \textbf{FiLM conditioning} from $\mathbf{Z}$ at every resolution scale, providing
          note-level guidance to the spectrogram generation
    \item \textbf{Cross-attention} to the text conditioning $\mathbf{C}$ at the bottleneck
    \item \textbf{Temporal position encoding} via sinusoidal embeddings aligned to the
          symbolic beat grid
\end{itemize}

The mel spectrogram is generated at 128 bins, 24 kHz sample rate, 256-sample hop
(93.75 Hz frame rate). A fine-tuned HiFi-GAN vocoder~\citep{kong2020hifi} converts
the mel spectrogram to waveform.

\subsubsection{Consistency Loss}

To ensure that the symbolic and audio streams remain synchronized, we apply a
\emph{consistency loss} during training that aligns the note onsets in the symbolic
stream with energy peaks in the generated spectrogram:
\begin{equation}
    \mathcal{L}_\text{consist} = \sum_{i,n} \left\| \sigma\left(\mathbf{M}_{t_n^{(i)}}^{\text{gen}}\right) -
    \sigma\left(\mathbf{M}_{t_n^{(i)}}^{\text{real}}\right) \right\|_1
\end{equation}
where $t_n^{(i)}$ are the onset times of note $n$ in instrument $i$ and $\sigma$ is
a soft energy envelope extractor.

\subsection{Model Configurations}

\begin{table}[H]
\centering
\caption{Zen-Musician component specifications.}
\label{tab:model_specs}
\begin{tabular}{lll}
\toprule
\textbf{Component} & \textbf{Specification} & \textbf{Parameters} \\
\midrule
Text/Conditioning Encoder & CLAP (frozen) + projection & 151M (frozen) + 8M \\
HMSE: Macro Transformer & 8 layers, $d=512$, 8 heads & 42M \\
HMSE: Meso Transformer & 12 layers, $d=768$, 12 heads & 108M \\
HMSE: Micro Transformer & 8 layers, $d=512$, 8 heads & 42M \\
MIOM: Per-Instrument Decoder & 16 layers, $d=1024$, 16 heads & 680M \\
MIOM: Cross-Instrument Attn & Every 4th layer & (included above) \\
Shared Latent Encoder & 4 layers, $d=1024$ & 67M \\
Audio Stream: U-Net Denoiser & 4 scales, FiLM conditioning & 310M \\
HiFi-GAN Vocoder & Fine-tuned on music & 14M \\
\midrule
\textbf{Total (trainable)} & & \textbf{1.27B} \\
Max instruments & 16 concurrent & \\
Max duration & 5 minutes (symbolic), 3 min (audio) & \\
Audio output & 24 kHz stereo, 128 mel bins & \\
\bottomrule
\end{tabular}
\end{table}

% =============================================================================
\section{Training}
% =============================================================================

\subsection{Training Data}

\begin{table}[H]
\centering
\caption{Training corpus statistics.}
\label{tab:training_data}
\begin{tabular}{lrrrl}
\toprule
\textbf{Source} & \textbf{Tracks} & \textbf{Hours} & \textbf{Genres} & \textbf{Type} \\
\midrule
Lakh MIDI Dataset~\citep{raffel2016learning} & 176,000 & --- & 38 & Symbolic \\
MusicNet~\citep{thickstun2017learning} & 330 & 34 & 4 & Symbolic + Audio \\
MusicCaps~\citep{agostinelli2023musiclm} & 5,500 & 28 & 42 & Audio + Text \\
Free Music Archive (FMA)~\citep{defferrard2017fma} & 106,000 & 8,800 & 42 & Audio \\
Licensed Commercial Corpus & 920,000 & 74,000 & 42 & Symbolic + Audio \\
\midrule
\textbf{Total} & \textbf{1,207,830} & \textbf{82,862} & 42 & Mixed \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Processing Pipeline.}
Raw MIDI files are tokenized into compound tokens; audio files are converted to mel
spectrograms. Audio-only data (FMA) is annotated via MT3~\citep{gardner2022mt3}
automatic transcription. Tracks without text are captioned using a music tagging model
followed by LLM tag-to-sentence conversion (averaging 32 words). Quality filtering
retains tracks with SNR $> 20$ dB, AMT note F1 $> 0.7$, chord confidence $> 0.6$,
and genre-consistent section boundaries. After filtering, 1.2M tracks remain.

\subsection{Training Protocol}

Training proceeds in four stages:

\paragraph{Stage 1: HMSE Pre-training (7 days, 64 A100 GPUs).}
The three HMSE transformers are trained jointly on structural annotations extracted
from the full corpus. The macro transformer is trained to predict section sequences
from text; the meso transformer predicts chord progressions given sections; the micro
transformer predicts onset patterns given phrases and chords.

Loss:
\begin{equation}
    \mathcal{L}_\text{HMSE} = \mathcal{L}_\text{macro}^\text{CE} +
    \mathcal{L}_\text{meso}^\text{CE} + \mathcal{L}_\text{micro}^\text{BCE}
\end{equation}

Batch size 256, learning rate $3 \times 10^{-4}$, AdamW with cosine decay.

\paragraph{Stage 2: MIOM Training (21 days, 128 A100 GPUs).}
The per-instrument decoders and cross-instrument attention are trained on compound
token sequences from the symbolic data (Lakh MIDI + Licensed Commercial Corpus),
conditioned on HMSE structural plans.

Loss (per-instrument cross-entropy):
\begin{equation}
    \mathcal{L}_\text{MIOM} = \sum_{i=1}^I \sum_{n=1}^{N_i}
    -\log p_\theta(\mathbf{t}_n^{(i)} | \mathbf{t}_{<n}^{(i)},
    \mathbf{H}^{(\setminus i)}, \mathbf{S}, \mathbf{C})
\end{equation}

where $\mathbf{H}^{(\setminus i)}$ denotes the cross-instrument context from all
other instruments. Batch size 128, learning rate $1 \times 10^{-4}$.

\paragraph{Stage 3: Audio Stream Training (14 days, 128 A100 GPUs).}
The spectrogram diffusion U-Net is trained on paired symbolic-audio data from the
Licensed Commercial Corpus and MusicNet. The HMSE and MIOM are frozen.

Loss:
\begin{equation}
    \mathcal{L}_\text{audio} = \mathbb{E}_{\mathbf{M}_0, t, \bm{\epsilon}}
    \left[\| \bm{\epsilon} - g_\psi(\mathbf{M}_t, t, \mathbf{Z}, \mathbf{C}) \|_2^2\right]
    + \lambda_c \mathcal{L}_\text{consist}
\end{equation}

with $\lambda_c = 0.1$. Batch size 64, learning rate $5 \times 10^{-5}$, $T_D = 1000$
diffusion steps with linear noise schedule.

\paragraph{Stage 4: End-to-End Fine-tuning (5 days, 128 A100 GPUs).}
All components are jointly fine-tuned on paired text-symbolic-audio data with a
combined loss:
\begin{equation}
    \mathcal{L}_\text{total} = \mathcal{L}_\text{HMSE} + \mathcal{L}_\text{MIOM}
    + \mathcal{L}_\text{audio} + \lambda_c \mathcal{L}_\text{consist}
\end{equation}

Learning rate $2 \times 10^{-5}$, batch size 32. End-to-end fine-tuning improves
the alignment between structural plan, symbolic output, and audio output.

\subsection{Classifier-Free Guidance}

Text conditioning is dropped with probability 10\%, melody conditioning with 20\%, and
structural specification with 30\% during training. At inference, we use guidance scales:
\begin{equation}
    \hat{\mathbf{o}} = \mathbf{o}_\text{uncond} + w_\text{text}(\mathbf{o}_\text{text} -
    \mathbf{o}_\text{uncond}) + w_\text{melody}(\mathbf{o}_\text{melody} -
    \mathbf{o}_\text{uncond})
\end{equation}
with default values $w_\text{text} = 3.5$ and $w_\text{melody} = 2.0$.

\subsection{Inference Algorithm}

\begin{algorithm}[H]
\caption{Zen-Musician Inference}
\label{alg:inference}
\begin{algorithmic}[1]
\State \textbf{Input}: Text prompt $\mathbf{c}_\text{text}$, optional melody $\mathbf{c}_\text{mel}$,
       optional structure spec, instrument list
\State \textbf{Input}: Audio diffusion steps $S=50$
\State Encode conditioning: $\mathbf{C} = \text{CondEnc}(\mathbf{c}_\text{text},
       \mathbf{c}_\text{mel}, \ldots)$
\State \textbf{--- Stage 1: Plan Structure ---}
\State $\mathbf{S}_\text{macro} = \text{HMSE.macro}(\mathbf{C})$
\State $\mathbf{S}_\text{meso} = \text{HMSE.meso}(\mathbf{S}_\text{macro}, \mathbf{C})$
\State $\mathbf{S}_\text{micro} = \text{HMSE.micro}(\mathbf{S}_\text{meso}, \mathbf{C})$
\State \textbf{--- Stage 2: Generate Per-Instrument Tokens ---}
\For{each instrument $i$ \textbf{in parallel}}
    \State $\mathbf{T}^{(i)} = \text{MIOM.generate}^{(i)}(\mathbf{S}_*, \mathbf{C})$
          \Comment{with cross-instrument attention}
\EndFor
\State \textbf{--- Stage 3: Output Symbolic ---}
\State $\text{MIDI} = \text{Decode}_\text{sym}(\{\mathbf{T}^{(i)}\})$
\State \textbf{--- Stage 4: Generate Audio ---}
\State $\mathbf{Z} = \text{SharedEnc}(\text{Interleave}(\mathbf{T}^{(1)}, \ldots, \mathbf{T}^{(I)}))$
\State $\mathbf{M}_S \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$
\For{$s = S$ \textbf{down to} $1$}
    \State $\hat{\bm{\epsilon}} = g_\psi(\mathbf{M}_s, s, \mathbf{Z}, \mathbf{C})$ with CFG
    \State $\mathbf{M}_{s-1} = \text{DDIM\_step}(\mathbf{M}_s, \hat{\bm{\epsilon}}, s)$
\EndFor
\State $\hat{\mathbf{a}} = \text{HiFi-GAN}(\mathbf{M}_0)$
\State \Return MIDI, $\hat{\mathbf{a}}$
\end{algorithmic}
\end{algorithm}

% =============================================================================
\section{Evaluation}
% =============================================================================

We evaluate Zen-Musician on three dimensions: text-to-audio generation quality,
symbolic music generation quality, and multi-instrument orchestration quality.

\subsection{Evaluation Metrics}

\begin{itemize}
    \item \textbf{FAD} (Fr\'echet Audio Distance)~\citep{kilgour2019frechet}: Distributional
          distance in VGGish embedding space; lower is better.
    \item \textbf{CLAP Score}: Cosine similarity between CLAP embeddings of text and generated
          audio; higher is better.
    \item \textbf{KL Divergence}: KL divergence between generated and real audio event class
          distributions.
    \item \textbf{NDB} (Number of statistically Different Bins)~\citep{richardson2018gans}:
          Measures coverage of the real data distribution by the generated distribution in
          symbolic token space; lower is better.
    \item \textbf{Note F1}: Precision and recall of generated notes compared to ground truth
          MIDI (onset tolerance 50 ms, pitch tolerance 0 semitones, duration tolerance 50 ms).
    \item \textbf{Orchestration Score}: A composite metric for multi-instrument quality
          measuring harmonic consistency, register separation, and rhythmic complementarity.
\end{itemize}

\subsection{Text-to-Audio Generation (MusicCaps)}

\begin{table}[H]
\centering
\caption{Text-to-audio evaluation on MusicCaps test set ($n=400$). Best results in bold.}
\label{tab:musiccaps}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{FAD} $\downarrow$ & \textbf{CLAP Score} $\uparrow$ & \textbf{KL Div.} $\downarrow$ \\
\midrule
MusicLM~\citep{agostinelli2023musiclm} & 4.72 & 0.318 & 1.82 \\
MusicGen-Large~\citep{copet2024musicgen} & 3.41 & 0.356 & 1.43 \\
Stable Audio 2.0~\citep{evans2024stableaudio} & 2.87 & 0.381 & 1.21 \\
JEN-1~\citep{li2024jen1} & 3.18 & 0.342 & 1.54 \\
\midrule
Zen-Musician (audio only) & 2.42 & 0.398 & 1.04 \\
Zen-Musician (full pipeline) & \textbf{2.14} & \textbf{0.412} & \textbf{0.91} \\
\bottomrule
\end{tabular}
\end{table}

Zen-Musician achieves an FAD of 2.14, improving over Stable Audio 2.0 by 25\%. The
full pipeline (including symbolic generation as an intermediate step) outperforms the
audio-only variant by 0.28 FAD, confirming that the symbolic intermediate representation
provides useful structural guidance to the audio decoder.

\subsection{MusicCaps Genre Breakdown}

\begin{table}[H]
\centering
\caption{FAD by genre on MusicCaps (selected genres with $n \geq 20$ test samples).}
\label{tab:genre_breakdown}
\begin{tabular}{lcccc}
\toprule
\textbf{Genre} & \textbf{MusicGen} & \textbf{Stable Audio} & \textbf{Zen-Musician} & \textbf{Improv.} \\
\midrule
Pop/Rock & 3.12 & 2.64 & \textbf{1.98} & $-25.0\%$ \\
Classical & 4.87 & 3.71 & \textbf{2.41} & $-35.0\%$ \\
Jazz & 4.21 & 3.28 & \textbf{2.18} & $-33.5\%$ \\
Electronic & 2.81 & 2.33 & \textbf{1.82} & $-21.9\%$ \\
Hip-Hop/R\&B & 3.58 & 2.91 & \textbf{2.34} & $-19.6\%$ \\
World/Folk & 4.94 & 3.84 & \textbf{2.87} & $-25.3\%$ \\
\midrule
\textbf{Overall} & 3.41 & 2.87 & \textbf{2.14} & $-25.4\%$ \\
\bottomrule
\end{tabular}
\end{table}

Improvements are largest for Classical ($-35.0\%$) and Jazz ($-33.5\%$), where complex
orchestration makes MIOM and HMSE most impactful. Electronic music shows the smallest gain.

\subsection{Symbolic Music Generation (Lakh MIDI)}

\begin{table}[H]
\centering
\caption{Symbolic music generation evaluation on Lakh MIDI test set ($n=2{,}000$).}
\label{tab:lakh}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{NDB} $\downarrow$ & \textbf{Note F1} $\uparrow$ & \textbf{Pitch Entropy} & \textbf{Duration Var.} \\
\midrule
Music Transformer~\citep{huang2019music} & 1.28 & 0.71 & 3.42 & 0.84 \\
Compound Word~\citep{hsiao2021compound} & 1.14 & 0.76 & 3.51 & 0.89 \\
MuseNet~\citep{payne2019musenet} & 1.02 & 0.79 & 3.64 & 0.91 \\
\midrule
Zen-Musician (symbolic only) & \textbf{0.82} & \textbf{0.87} & 3.71 & 0.94 \\
\bottomrule
\end{tabular}
\end{table}

Zen-Musician achieves an NDB of 0.82 (lower is better), indicating that the generated
distribution covers 82\% of the real data's statistical bins without over-concentrating.
Note F1 of 0.87 indicates high accuracy in reproducing the pitch, timing, and duration
patterns of the target genre. Pitch entropy (3.71 vs.\ 3.75 for real data) and duration
variability (0.94 vs.\ 0.97 for real data) are close to ground truth, indicating
natural diversity without mode collapse.

\subsection{Multi-Instrument Orchestration (OrchBench)}

\begin{table}[H]
\centering
\caption{OrchBench evaluation ($n=500$ scored orchestral excerpts with 4--12 instruments).}
\label{tab:orchbench}
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{Harm.} & \textbf{Register} & \textbf{Rhythm} & \textbf{Orch.} & \textbf{Timbral} \\
 & \textbf{Consist.} $\uparrow$ & \textbf{Sep.} $\uparrow$ & \textbf{Comp.} $\uparrow$ & \textbf{Score} $\uparrow$ & \textbf{Diversity} $\uparrow$ \\
\midrule
MuseNet & 0.61 & 0.54 & 0.48 & 0.54 & 0.58 \\
MusicGen (mono) & --- & --- & --- & --- & 0.42 \\
\midrule
Zen-Musician & \textbf{0.81} & \textbf{0.72} & \textbf{0.68} & \textbf{0.74} & \textbf{0.71} \\
\bottomrule
\end{tabular}
\end{table}

The orchestration score (composite of harmonic consistency, register separation, and
rhythmic complementarity) of 0.74 substantially exceeds MuseNet's 0.54, demonstrating
the effectiveness of the cross-instrument attention mechanism. Register separation (0.72
vs.\ 0.54) is particularly improved, confirming that the MIOM's mutual awareness prevents
instruments from crowding the same pitch range.

\subsection{Human Evaluation}

We conducted a listening study with 28 professional musicians (mean 14 years of training,
including 8 orchestral musicians, 6 jazz musicians, 8 pop/electronic producers, and 6
composers). Participants rated 40 randomly assigned compositions on a 5-point Likert scale
across four dimensions and provided an overall publishability judgment.

\begin{table}[H]
\centering
\caption{Human evaluation by professional musicians ($n=28$, 1{,}120 ratings total).}
\label{tab:human_eval}
\begin{tabular}{lcccc}
\toprule
\textbf{Dimension} & \textbf{MusicGen} & \textbf{Stable Audio} & \textbf{Zen-Musician} & \textbf{Real Music} \\
\midrule
Melodic Quality (1--5) & 2.84 & 3.12 & \textbf{3.87} & 4.52 \\
Harmonic Quality (1--5) & 2.71 & 3.04 & \textbf{3.94} & 4.61 \\
Rhythmic Quality (1--5) & 3.21 & 3.38 & \textbf{3.82} & 4.44 \\
Orchestration (1--5) & 2.48 & 2.81 & \textbf{3.91} & 4.58 \\
Overall Quality (1--5) & 2.74 & 3.08 & \textbf{3.88} & 4.53 \\
\midrule
``Publishable (minor edit)'' & 14.2\% & 22.8\% & \textbf{67.1\%} & 94.3\% \\
``Needs major revision'' & 41.4\% & 34.1\% & \textbf{18.7\%} & 3.2\% \\
``Unacceptable'' & 44.4\% & 43.1\% & \textbf{14.2\%} & 2.5\% \\
\bottomrule
\end{tabular}
\end{table}

Zen-Musician's overall quality of 3.88/5.00 is within 0.65 of real music. The orchestration
dimension shows the largest baseline gap ($+1.10$ over Stable Audio). Publishability
rate of 67.1\% nearly triples that of Stable Audio (22.8\%).

\subsection{Structural Coherence Analysis}

\begin{table}[H]
\centering
\caption{Structural coherence measured by section boundary detection accuracy and
harmonic progression quality on 3-minute generations.}
\label{tab:structure}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Section F1} $\uparrow$ & \textbf{Chord Acc.} $\uparrow$ & \textbf{Key Stability} $\uparrow$ \\
\midrule
MusicGen-Large & 0.31 & 0.42 & 0.68 \\
Stable Audio 2.0 & 0.38 & 0.48 & 0.72 \\
Zen-Musician (no HMSE) & 0.44 & 0.57 & 0.78 \\
Zen-Musician (full) & \textbf{0.71} & \textbf{0.73} & \textbf{0.91} \\
\bottomrule
\end{tabular}
\end{table}

The HMSE improves section boundary F1 from 0.44 to 0.71 and chord accuracy from 0.57
to 0.73, confirming that explicit hierarchical planning substantially improves structural
coherence. Key stability (the fraction of time the generated music remains in a
perceptually stable tonal center) increases from 0.78 to 0.91.

% =============================================================================
\section{Ablation Studies}
% =============================================================================

\begin{table}[H]
\centering
\caption{Ablation study on MusicCaps test set (FAD, CLAP) and Lakh MIDI (NDB, Note F1).}
\label{tab:ablation}
\begin{tabular}{lcccc}
\toprule
\textbf{Configuration} & \textbf{FAD} $\downarrow$ & \textbf{CLAP} $\uparrow$ & \textbf{NDB} $\downarrow$ & \textbf{Note F1} $\uparrow$ \\
\midrule
Full model & \textbf{2.14} & \textbf{0.412} & \textbf{0.82} & \textbf{0.87} \\
\midrule
$-$ HMSE (flat generation) & 3.01 & 0.371 & 1.04 & 0.78 \\
$-$ Cross-instrument attention & 2.68 & 0.391 & 0.91 & 0.82 \\
$-$ Audio stream (symbolic only) & --- & --- & 0.82 & 0.87 \\
$-$ Symbolic stream (audio only) & 2.42 & 0.398 & --- & --- \\
$-$ Consistency loss & 2.31 & 0.404 & 0.84 & 0.86 \\
$-$ Instrument embeddings & 2.52 & 0.394 & 0.89 & 0.84 \\
$-$ Orchestration constraints & 2.38 & 0.407 & 0.86 & 0.85 \\
$-$ End-to-end fine-tuning & 2.47 & 0.396 & 0.88 & 0.85 \\
$-$ Licensed data (public only) & 3.24 & 0.358 & 0.98 & 0.81 \\
\midrule
Macro only (no meso/micro) & 2.78 & 0.384 & 0.96 & 0.81 \\
Macro + meso (no micro) & 2.41 & 0.401 & 0.88 & 0.85 \\
\bottomrule
\end{tabular}
\end{table}

Key observations: (1) \textbf{HMSE} is the most impactful component ($-0.87$ FAD, $-0.22$ NDB),
with each level contributing incrementally (macro $\to$ meso: $-0.37$; meso $\to$ micro: $-0.27$).
(2) \textbf{Cross-instrument attention} provides $-0.54$ FAD, with the largest effect on
Classical ($-0.91$) and Jazz ($-0.72$).
(3) The \textbf{symbolic intermediate representation} improves audio quality by $-0.28$ FAD
over audio-only generation.
(4) \textbf{Licensed data} is critical ($-1.10$ FAD without it).
(5) \textbf{End-to-end fine-tuning} adds a modest $-0.33$ FAD by aligning component interfaces.

\subsection{Guidance Scale Analysis}

\begin{table}[H]
\centering
\caption{Effect of text guidance scale on MusicCaps evaluation.}
\label{tab:guidance}
\begin{tabular}{cccc}
\toprule
$w_\text{text}$ & \textbf{FAD} $\downarrow$ & \textbf{CLAP} $\uparrow$ & \textbf{Diversity} $\uparrow$ \\
\midrule
1.0 (no guidance) & 3.41 & 0.312 & \textbf{0.94} \\
2.0 & 2.58 & 0.378 & 0.88 \\
3.5 (default) & \textbf{2.14} & \textbf{0.412} & 0.81 \\
5.0 & 2.21 & 0.408 & 0.72 \\
7.0 & 2.54 & 0.401 & 0.61 \\
\bottomrule
\end{tabular}
\end{table}

The optimal text guidance scale of 3.5 balances text adherence (CLAP score) with
diversity and audio quality (FAD). Higher guidance scales improve text alignment but
reduce diversity and eventually degrade audio quality due to over-concentration.

\subsection{Scaling Study}

Quality improves log-linearly with MIOM parameter count: Zen-Musician-S (170M MIOM):
FAD 3.21, NDB 1.08; -M (340M): FAD 2.72, NDB 0.94; -L (680M, reported): FAD 2.14,
NDB 0.82; -XL (1.4B, unreleased): FAD 1.98, NDB 0.78. Further scaling is expected
to yield continued improvements.

% =============================================================================
\section{Computational Analysis}
% =============================================================================

\begin{table}[H]
\centering
\caption{Inference latency for generating a 3-minute composition on a single A100-80GB.}
\label{tab:compute}
\begin{tabular}{lrr}
\toprule
\textbf{Stage} & \textbf{Time (s)} & \textbf{Memory (GB)} \\
\midrule
Conditioning encoding & 0.3 & 1.2 \\
HMSE (3 stages) & 2.1 & 3.4 \\
MIOM (8 instruments, parallel) & 18.7 & 24.1 \\
Shared latent encoding & 1.4 & 4.8 \\
Audio diffusion (50 steps) & 42.3 & 31.2 \\
HiFi-GAN vocoding & 1.8 & 2.1 \\
\midrule
\textbf{Total (symbolic only)} & \textbf{22.5} & 24.1 \\
\textbf{Total (symbolic + audio)} & \textbf{66.6} & 31.2 \\
\bottomrule
\end{tabular}
\end{table}

Symbolic-only generation (MIDI output) takes 22.5 seconds for a 3-minute composition,
approximately $8\times$ faster than real-time. With audio generation, the total is
66.6 seconds ($2.7\times$ faster than real-time). The audio diffusion stage dominates
latency; reducing diffusion steps from 50 to 20 decreases total time to 34.1 seconds
with only 0.18 FAD degradation.

% =============================================================================
\section{Applications}
% =============================================================================

Zen-Musician's dual-stream output supports several application domains:

\begin{itemize}
    \item \textbf{Music production}: Producers generate compositions from text or melody
          seeds, edit the MIDI in a DAW (Ableton, Logic Pro), and re-render through the
          audio stream or alternative synthesizers.

    \item \textbf{Film and game scoring}: The structural specification input enables
          composers to specify exact section timings, instrumentation changes, and mood
          transitions aligned with narrative structure.

    \item \textbf{Music education}: The symbolic output with HMSE structural annotations
          makes compositional ``reasoning'' transparent, enabling students to analyze form,
          harmony, and orchestration decisions.

    \item \textbf{Accessibility}: Natural language descriptions enable individuals without
          formal training to create multi-instrument compositions.
\end{itemize}

% =============================================================================
\section{Limitations}
% =============================================================================

\begin{itemize}
    \item \textbf{Expressive performance}: The model approximates but does not fully
          capture rubato, dynamic shaping, and idiosyncratic performer articulation.

    \item \textbf{Audio fidelity}: 24 kHz output (12 kHz bandwidth) is adequate for most
          content but insufficient for high-frequency detail in cymbals and electronic
          synthesis. Upgrading to 48 kHz is planned.

    \item \textbf{Lyrics and vocals}: Instrumental music only; vocal synthesis via
          Zen-Scribe is not yet integrated.

    \item \textbf{Copyright}: The model may generate passages resembling copyrighted
          compositions. Post-generation plagiarism checking is recommended for commercial use.

    \item \textbf{Genre imbalance}: Western tonal music is overrepresented; non-Western
          genres show noticeably lower quality.

    \item \textbf{Inference cost}: 66.6 seconds for 3 minutes of music is acceptable for
          offline production but too slow for real-time use.

    \item \textbf{Duration}: Reliable up to 5 minutes (symbolic) and 3 minutes (audio);
          quality degrades beyond 5 minutes.
\end{itemize}

% =============================================================================
\section{Ethical Considerations}
% =============================================================================

Music generation raises ethical concerns around musician displacement, attribution,
training data compensation, and cultural sensitivity. We position Zen-Musician as a
tool for augmenting human creativity: the editable MIDI output facilitates human-AI
collaboration rather than replacement. Our Licensed Commercial Corpus includes licensing
fees paid to rights holders. We recommend clear AI-generated content labeling and flag
culturally specific genres in the user interface to encourage informed use. Non-Western
musical traditions deserve respectful treatment; generation in these styles without
cultural understanding risks trivialization.

% =============================================================================
\section{Conclusion}
% =============================================================================

We have presented Zen-Musician, a 1.27B-parameter dual-stream transformer for generating
multi-instrument music in both symbolic (MIDI) and audio domains from text descriptions,
melody prompts, and structural specifications. The HMSE, MIOM, and Dual-Stream Decoder
address long-range structural coherence, multi-voice coordination, and cross-domain output.

Zen-Musician achieves state-of-the-art results on MusicCaps (FAD 2.14), Lakh MIDI
(NDB 0.82, Note F1 0.87), and OrchBench (orchestration score 0.74). Human evaluation
rates 67\% of outputs as publishable with minor editing. The hierarchical planning
approach is the most impactful contribution ($-0.87$ FAD), confirming that music benefits
from explicit structural planning before detailed generation.

Future work will address vocal synthesis integration, 48 kHz audio, non-Western music
traditions, real-time interactive generation, and scaling to longer compositions.

% =============================================================================
\section*{Acknowledgments}
% =============================================================================

We thank the professional musicians who participated in the evaluation study, the music
licensing partners, and the dataset maintainers. This work was supported by the Zoo Labs
Foundation's Decentralized AI Research Initiative.

% =============================================================================
% REFERENCES
% =============================================================================

\begin{thebibliography}{99}

\bibitem[Agostinelli et~al.(2023)]{agostinelli2023musiclm}
Agostinelli, A., Denk, T.I., Borsos, Z., Engel, J., Verzetti, M., Caber, A., \dots{} Frank, C.
\newblock MusicLM: Generating music from text.
\newblock \emph{arXiv preprint arXiv:2301.11325}, 2023.

\bibitem[B{\"o}ck et~al.(2016)]{bock2016madmom}
B{\"o}ck, S., Korzeniowski, F., Schl{\"u}ter, J., Krebs, F., \& Widmer, G.
\newblock madmom: A new Python audio and music signal processing library.
\newblock \emph{Proceedings of ACM Multimedia}, 2016.

\bibitem[Borsos et~al.(2023)]{borsos2023audiolm}
Borsos, Z., Marinier, R., Vincent, D., Kharitonov, E., Pietquin, O., Sharifi, M., \dots{} Tagliasacchi, A.
\newblock AudioLM: A language modeling approach to audio generation.
\newblock \emph{IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 31:2523--2536, 2023.

\bibitem[Cella et~al.(2020)]{cella2020orchideasol}
Cella, C.E., Music\`{o}, D., Music\`{o}, A., \& Music\`{o}, R.
\newblock OrchideaSOL: A dataset of extended instrumental techniques for computer-assisted orchestration.
\newblock \emph{Proceedings of ISMIR}, 2020.

\bibitem[Copet et~al.(2024)]{copet2024musicgen}
Copet, J., Kreuk, F., Gat, I., Remez, T., Kant, D., Synnaeve, G., Adi, Y., \& D\'{e}fossez, A.
\newblock Simple and controllable music generation.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Crestel \& Esling(2018)]{crestel2018learning}
Crestel, L. \& Esling, P.
\newblock Live orchestral piano, a system for real-time orchestral music generation.
\newblock \emph{Proceedings of the Sound and Music Computing Conference}, 2018.

\bibitem[Defferrard et~al.(2017)]{defferrard2017fma}
Defferrard, M., Benzi, K., Vandergheynst, P., \& Bresson, X.
\newblock FMA: A dataset for music analysis.
\newblock \emph{Proceedings of ISMIR}, 2017.

\bibitem[D{\'e}fossez et~al.(2022)]{defossez2022encodec}
D{\'e}fossez, A., Copet, J., Synnaeve, G., \& Adi, Y.
\newblock High fidelity neural audio compression.
\newblock \emph{arXiv preprint arXiv:2210.13438}, 2022.

\bibitem[Donahue et~al.(2023)]{donahue2023singsong}
Donahue, C., Caillon, A., Roberts, A., Manilow, E., Esling, P., Agostinelli, A.,
Verzetti, M., \& Engel, J.
\newblock SingSong: Generating musical accompaniments from singing.
\newblock \emph{arXiv preprint arXiv:2301.12662}, 2023.

\bibitem[Engel et~al.(2017)]{engel2017neural}
Engel, J., Resnick, C., Roberts, A., Dieleman, S., Norouzi, M., Eck, D., \& Simonyan, K.
\newblock Neural audio synthesis of musical notes with WaveNet autoencoders.
\newblock \emph{Proceedings of ICML}, 2017.

\bibitem[Evans et~al.(2024)]{evans2024stableaudio}
Evans, Z., Parker, J.D., Carr, C.J., Zukowski, Z., Taylor, J., \& Pons, J.
\newblock Stable Audio Open.
\newblock \emph{arXiv preprint arXiv:2407.14358}, 2024.

\bibitem[Gardner et~al.(2022)]{gardner2022mt3}
Gardner, J., Simon, I., Manilow, E., Hawthorne, C., \& Engel, J.
\newblock MT3: Multi-task multitrack music transcription.
\newblock \emph{Proceedings of ICLR}, 2022.

\bibitem[Hsiao et~al.(2021)]{hsiao2021compound}
Hsiao, W.Y., Liu, J.Y., Yeh, Y.C., \& Yang, Y.H.
\newblock Compound word transformer: Learning to compose full-song music over dynamic directed hypergraphs.
\newblock \emph{Proceedings of AAAI}, 2021.

\bibitem[Huang \& Yang(2020)]{huang2019music}
Huang, C.Z.A., Vaswani, A., Uszkoreit, J., Shazeer, N., Simon, I., Hawthorne, C.,
Dai, A.M., Hoffman, M.D., Dinculescu, M., \& Eck, D.
\newblock Music Transformer: Generating music with long-term structure.
\newblock \emph{Proceedings of ICLR}, 2019.

\bibitem[Huang et~al.(2022)]{huang2022mulan}
Huang, Q., Jansen, A., Lee, J., Ganti, R., Li, J.Y., \& Ellis, D.P.W.
\newblock MuLan: A joint embedding of music audio and natural language.
\newblock \emph{Proceedings of ISMIR}, 2022.

\bibitem[Kilgour et~al.(2019)]{kilgour2019frechet}
Kilgour, K., Zuluaga, M., Roblek, D., \& Sharifi, M.
\newblock Fr{\'e}chet audio distance: A reference-free metric for evaluating music
enhancement algorithms.
\newblock \emph{Proceedings of Interspeech}, 2019.

\bibitem[Kim et~al.(2018)]{kim2018crepe}
Kim, J.W., Salamon, J., Li, P., \& Bello, J.P.
\newblock CREPE: A convolutional representation for pitch estimation.
\newblock \emph{Proceedings of ICASSP}, 2018.

\bibitem[Kong et~al.(2020)]{kong2020hifi}
Kong, J., Kim, J., \& Bae, J.
\newblock HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Lerdahl \& Jackendoff(1983)]{lerdahl1983generative}
Lerdahl, F. \& Jackendoff, R.
\newblock \emph{A Generative Theory of Tonal Music}.
\newblock MIT Press, 1983.

\bibitem[Li et~al.(2024)]{li2024jen1}
Li, P., Chen, B., Yao, Y., Wang, Y., Wang, A., \& Wang, A.
\newblock JEN-1: Text-guided universal music generation with omnidirectional diffusion models.
\newblock \emph{arXiv preprint arXiv:2308.04729}, 2024.

\bibitem[Mauch \& Dixon(2010)]{mauch2010approximate}
Mauch, M. \& Dixon, S.
\newblock Approximate note transcription for the improved identification of difficult chords.
\newblock \emph{Proceedings of ISMIR}, 2010.

\bibitem[McFee \& Ellis(2017)]{mcfee2017structured}
McFee, B. \& Ellis, D.P.W.
\newblock Structured training for large-vocabulary chord recognition.
\newblock \emph{Proceedings of ISMIR}, 2017.

\bibitem[Nieto et~al.(2020)]{nieto2020audio}
Nieto, O., McFee, B., Peeters, G., Bello, J.P., \& Peeters, G.
\newblock Audio-based music structure analysis: Current trends, open challenges, and applications.
\newblock \emph{Transactions of the International Society for Music Information Retrieval}, 3(1), 2020.

\bibitem[Payne(2019)]{payne2019musenet}
Payne, C.
\newblock MuseNet.
\newblock OpenAI Blog, 2019. \url{https://openai.com/blog/musenet}

\bibitem[Raffel(2016)]{raffel2016learning}
Raffel, C.
\newblock \emph{Learning-Based Methods for Comparing Sequences, with Applications to
Audio-to-MIDI Alignment and Matching}.
\newblock PhD thesis, Columbia University, 2016.

\bibitem[Ren et~al.(2020)]{ren2020popmag}
Ren, Y., He, J., Tan, X., Qin, T., Zhao, Z., \& Liu, T.Y.
\newblock PopMAG: Pop music accompaniment generation.
\newblock \emph{Proceedings of ACM Multimedia}, 2020.

\bibitem[Richardson \& Weiss(2018)]{richardson2018gans}
Richardson, E. \& Weiss, Y.
\newblock On GANs and GMMs.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Thickstun et~al.(2017)]{thickstun2017learning}
Thickstun, J., Harchaoui, Z., \& Kakade, S.
\newblock Learning features of music from scratch.
\newblock \emph{Proceedings of ICLR}, 2017.

\bibitem[Todd(1989)]{todd1989connectionist}
Todd, P.M.
\newblock A connectionist approach to algorithmic composition.
\newblock \emph{Computer Music Journal}, 13(4):27--43, 1989.

\bibitem[Wu et~al.(2022)]{wu2022midi}
Wu, Y., Manilow, E., Deng, Y., Swavely, R., Kastner, K., Cooijmans, T.,
Courville, A., Huang, C.Z.A., \& Engel, J.
\newblock MIDI-DDSP: Detailed control of musical performance via hierarchical modeling.
\newblock \emph{Proceedings of ICLR}, 2022.

\bibitem[Wu et~al.(2023)]{wu2023clap}
Wu, Y., Chen, K., Zhang, T., Hui, Y., Taylor, B., Harber, C., \dots{} Plumbley, M.D.
\newblock Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation.
\newblock \emph{Proceedings of ICASSP}, 2023.

\bibitem[Zeghidour et~al.(2021)]{zeghidour2021soundstream}
Zeghidour, N., Luebs, A., Omran, A., Skoglund, J., \& Vishnubhotla, M.
\newblock SoundStream: An end-to-end neural audio codec.
\newblock \emph{IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 30:495--507, 2021.

\end{thebibliography}

\end{document}
