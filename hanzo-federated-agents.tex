% Hanzo Federated Agents Paper
% Fully self-contained -- no \input{} directives
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xcolor}
\usepackage{natbib}
\usepackage{float}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{multirow}

\hypersetup{colorlinks=true,linkcolor=black,citecolor=blue,urlcolor=blue}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{assumption}{Assumption}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\KL}{KL}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\med}{med}
\DeclareMathOperator{\wmed}{wmed}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\Sim}{Sim}
\DeclareMathOperator{\emb}{emb}

\title{Federated Agent Intelligence:\\
Decentralized Experience Sharing via\\
BitDelta-Compressed Semantic Optimization}
\author{
    Hanzo AI Research\\
    \textit{Hanzo AI Inc (Techstars '17), Los Angeles, CA}\\
    \texttt{research@hanzo.ai}
}
\date{February 2026}

\begin{document}
\maketitle

% ============================================================================
% ABSTRACT
% ============================================================================
\begin{abstract}
We present \textbf{Federated Agent Intelligence (FAI)}, a system for
collective learning across autonomous AI agent fleets without sharing model
weights, raw trajectories, private data, or training compute. Each agent node
runs local Group-Relative Policy Optimization (GRPO) to extract
semantic experiences from tool-use trajectories, compresses these experiences
via \textbf{BitDelta} (1-bit quantization of embedding deltas yielding
${\approx}10\times$ compression), and broadcasts them to the fleet via
\textbf{DSO} (Decentralized Semantic Optimization, ZIP-001/400).
At the receiving end, \textbf{DeltaSoup} performs Byzantine-robust median
aggregation to filter adversarial contributions before merging into the local
experience library.

We prove that the FAI protocol converges to a near-optimal shared experience
library under an honest majority ($f < N/3$ Byzantine nodes) and that the
BitDelta compression-decompression cycle preserves semantic cosine similarity
with expected loss below 5\%. Fleet-level experiments with $N \in \{10, 50, 100\}$
nodes demonstrate \textbf{23.7\% improvement} in task success rate over isolated
agents at $N=100$, with graceful degradation under Byzantine loads up to $N/3$.
The BitDelta wire format reduces per-broadcast bandwidth from 2.4\,MB to 240\,KB,
enabling practical operation over commodity peer-to-peer networks. The complete
system is implemented as a plugin for the Hanzo agent harness and operates
continuously alongside normal agent sessions without interrupting agent execution.
\end{abstract}

% ============================================================================
% 1. INTRODUCTION
% ============================================================================
\section{Introduction}
\label{sec:intro}

\paragraph{The Fleet Learning Problem.}
Consider an organization deploying $N$ autonomous agents to accomplish similar
tasks---code generation, API integration, document processing, customer
support---across different projects, tenants, or geographic regions. Each
agent independently encounters similar failure modes, discovers similar
solutions, and builds similar intuitions about what strategies work. Without a
sharing mechanism, every agent must rediscover the same lessons from scratch.
Across a fleet of 100 agents, this represents a $100\times$ redundancy in the
experiential learning process.

The natural response is to share what agents learn. But what exactly do agents
learn, and how can it be shared efficiently, safely, and without violating data
sovereignty?

\paragraph{Why Federated Model Training Does Not Apply.}
Classical federated learning~\citep{mcmahan2017communication} addresses
distributed learning by aggregating model gradients or parameters. This
approach is fundamentally mismatched to the agent setting for three reasons.
First, modern agent fleets typically run \emph{frozen} foundation models via
API---there are no local parameters to update. Second, fine-tuning at the
individual-agent level is economically prohibitive (\$10{,}000--\$100{,}000 per
domain) and technically complex. Third, gradient-based federation exposes model
internals and requires synchronized training runs that are incompatible with
continuously operating production agents.

\paragraph{The Key Insight: Share Experiences, Not Weights.}
Active Semantic Optimization (ASO)~\citep{hanzo2026aso} established that
frozen language models can be adapted at decode time via compressed
\emph{experiential priors}---concise, transferable semantic descriptions of
what strategies work for a class of tasks. An experiential prior is not a
gradient; it is structured knowledge extracted from comparing successful and
unsuccessful agent trajectories. Such priors can be shared, aggregated, and
applied without any model parameter access.

This paper extends the single-agent ASO setting to the fleet setting.
We ask: given $N$ agents each running ASO locally, how can they share their
experiential priors efficiently, securely, and with formal convergence
guarantees?

\paragraph{Our Approach.}
We answer this question by composing three existing protocols with new
fleet-level mechanisms:

\begin{enumerate}[leftmargin=1.5em]
    \item \textbf{DSO (ZIP-001/400)~\citep{hanzo2026dso}} provides the
          decentralized protocol for broadcasting and storing experiential
          priors on a peer-to-peer network backed by content-addressed storage.
    \item \textbf{BitDelta (ZIP-007)~\citep{hanzo2026bitdelta}} provides 1-bit
          quantization of experience embedding deltas, achieving ${\sim}10\times$
          compression with bounded reconstruction error, making fleet-scale
          broadcasting practical on commodity networks.
    \item \textbf{DeltaSoup} is our new Byzantine-robust aggregation layer
          that applies geometric median filtering to received compressed
          experiences before merging them into the local library, tolerating
          up to $f < N/3$ adversarial contributors.
\end{enumerate}

\paragraph{Contributions.}
This paper makes the following contributions:
\begin{enumerate}[leftmargin=1.5em]
    \item \textbf{Agent-level DSO integration}: A complete protocol for
          extracting, compressing, and broadcasting agent experiences via the
          DSO network, including the \texttt{CompressedBatch} wire format.
    \item \textbf{BitDelta experience compression}: Application of 1-bit delta
          quantization to experience embeddings, with analysis showing
          $29.5\times$ theoretical and $10\times$ practical compression ratios
          and cosine similarity preservation bounds.
    \item \textbf{DeltaSoup aggregation}: A Byzantine-robust aggregation
          protocol for compressed experience batches using coordinate-wise
          weighted median, with formal $(f,\varepsilon)$-robustness guarantees.
    \item \textbf{Fleet convergence analysis}: Proof that the FAI protocol
          converges monotonically under honest majority with rate depending on
          network connectivity and experience diversity.
    \item \textbf{Empirical evaluation}: Simulation results with $N=10, 50, 100$
          agents showing fleet improvement over isolated agents and resilience to
          Byzantine adversaries.
\end{enumerate}

\paragraph{Paper Outline.}
Section~\ref{sec:background} reviews background on federated learning, DSO, BitDelta,
and DeltaSoup. Section~\ref{sec:architecture} describes the system architecture.
Section~\ref{sec:bitdelta} formalizes BitDelta experience compression.
Section~\ref{sec:deltasoup} develops DeltaSoup aggregation with formal guarantees.
Section~\ref{sec:convergence} analyzes fleet learning dynamics and convergence.
Section~\ref{sec:harness} describes integration with the Hanzo agent harness.
Section~\ref{sec:privacy} addresses privacy and security.
Section~\ref{sec:evaluation} presents experimental evaluation.
Section~\ref{sec:discussion} discusses future directions.
Section~\ref{sec:related} reviews related work.
Section~\ref{sec:conclusion} concludes.

% ============================================================================
% 2. BACKGROUND
% ============================================================================
\section{Background}
\label{sec:background}

\subsection{Classical Federated Learning}

Federated learning~\citep{mcmahan2017communication} trains a shared model
across $N$ participants without centralizing raw data. The canonical
FedAvg algorithm alternates between local training steps and global parameter
averaging:
\begin{equation}
    \bm{\theta}^{t+1} = \sum_{i=1}^{N} \frac{n_i}{n} \bm{\theta}_i^{t+1},
    \quad \bm{\theta}_i^{t+1} = \bm{\theta}^t - \eta \nabla_{\bm{\theta}} \mathcal{L}_i(\bm{\theta}^t),
\end{equation}
where $n_i$ is the local dataset size and $n = \sum_i n_i$.

\paragraph{Communication Compression.}
Communication overhead is the primary bottleneck in federated learning.
\citet{seide20141bit} propose 1-bit SGD, quantizing gradient signs:
$\hat{g} = \sign(g) \cdot \|g\|_1 / d$.
\citet{bernstein2018signsgd} formalize SignSGD and prove convergence under
symmetric gradient noise. 1-bit compression reduces communication by $32\times$
versus full-precision floats.

\paragraph{Byzantine Robustness.}
Byzantine participants~\citep{lamport1982byzantine} can send arbitrary
gradients to poison the aggregate. Krum~\citep{blanchard2017machine} selects the
gradient closest to its $n - f - 2$ nearest neighbors as the aggregate.
\citet{yin2018byzantine} prove that coordinate-wise median and trimmed mean
achieve statistical rates within a factor of $f^2$ of the optimal non-Byzantine
estimator. DeltaSoup inherits this framework and adapts it to semantic
experience embeddings.

\subsection{DSO: Decentralized Semantic Optimization}

DSO~\citep{hanzo2026dso} (ZIP-001) is a protocol for sharing and aggregating
experiential priors across distributed language model agents.

\begin{definition}[Experiential Prior]
\label{def:prior}
An experiential prior $E_m$ consists of:
\begin{itemize}[leftmargin=1.5em]
    \item A natural language pattern description $\text{pattern}_m$ ($\leq 32$ words).
    \item A task embedding $\bm{e}_m \in \mathbb{R}^d$ encoding the domain.
    \item A quality score $q_m \in (0, 1)$ estimating pattern reliability.
    \item A 1-bit quantized expert factor $\widehat{\Delta}_m \in \{-1,+1\}^V$
          with scale $\alpha_m > 0$ for PoE decoding.
    \item Metadata: timestamp, source node, domain tag, iteration count.
\end{itemize}
\end{definition}

The \emph{experience library} $\mathcal{L} = \{E_1, \ldots, E_M\}$ is the
shared artifact. Agents use it for retrieve-and-apply at decode time:
\begin{equation}
    \pi_{\text{ASO}}(y_t \mid x, y_{<t}) \propto \pi_\theta(y_t \mid x, y_{<t})
    \prod_{m \in \text{retrieved}(x)} \phi_m(y_t)^{\eta_m},
    \label{eq:poe}
\end{equation}
where $\eta_m = \log(q_m / (1 - q_m))$ and $\phi_m$ is constructed from
$\widehat{\Delta}_m$.

Unlike classical federated learning, DSO shares no model parameters, no
gradients, and no raw training data. The shared artifact is entirely composed
of compressed semantic patterns in natural language plus corresponding
quantized embedding perturbations.

\subsection{BitDelta Compression}
\label{sec:background-bitdelta}

BitDelta (ZIP-007)~\citep{hanzo2026bitdelta} applies 1-bit quantization to
embedding \emph{deltas} relative to a shared baseline. For experience embedding
$\bm{e} \in \mathbb{R}^d$ and baseline $\bar{\bm{e}} \in \mathbb{R}^d$:

\begin{definition}[BitDelta Quantization]
\label{def:bitdelta}
Given delta $\Delta = \bm{e} - \bar{\bm{e}} \in \mathbb{R}^d$, the BitDelta
quantization is:
\begin{equation}
    \widehat{\Delta} = \alpha \cdot \sign(\Delta),
    \quad \alpha = \frac{1}{d} \|\Delta\|_1 = \frac{1}{d}\sum_{j=1}^d |\Delta_j|,
    \label{eq:bitdelta}
\end{equation}
where $\alpha \in \mathbb{R}_{>0}$ is the per-group scale factor and
$\sign(\Delta) \in \{-1, +1\}^d$ is the elementwise sign vector stored as
packed bits (1 bit per dimension).
\end{definition}

With group-wise scaling (group size $g = 128$), each group uses one float32
scale and $g$ bits, giving:
\begin{equation}
    \text{bits per element} = \frac{32 + g}{g} = 1 + \frac{32}{g}.
\end{equation}
For $g = 128$: $1.25$ bits/element, a $25.6\times$ reduction from float32.
Accounting for metadata overhead, practical compression is ${\approx}10\times$.

\subsection{DeltaSoup Aggregation}
\label{sec:background-deltasoup}

DeltaSoup is a Byzantine-robust aggregation protocol for community delta
contributions. Conceptually, it applies the geometric median~\citep{minsker2015geometric}
rather than the arithmetic mean, so a minority of adversarial contributors
cannot shift the aggregate arbitrarily.

Given $N$ compressed experience batches $\{\hat{B}_i\}_{i=1}^N$ received from
peers, DeltaSoup:
\begin{enumerate}[leftmargin=1.5em]
    \item Decompresses all batches to full-precision embeddings.
    \item Computes pairwise cosine similarity across all experiences.
    \item Filters experiences whose embedding lies more than $2\sigma$ from
          the coordinate-wise median (outlier rejection).
    \item Merges surviving experiences with confidence-weighted averaging.
\end{enumerate}

Formal properties are developed in Section~\ref{sec:deltasoup}.

% ============================================================================
% 3. SYSTEM ARCHITECTURE
% ============================================================================
\section{System Architecture}
\label{sec:architecture}

The FAI system consists of three components running at each agent node:
(i) the local node with experience library and GRPO optimizer,
(ii) the network layer for P2P broadcast and receive, and
(iii) the aggregation layer implementing DeltaSoup.

\subsection{Local Node}

\paragraph{SemanticMemoryManager.}
Each node maintains a \emph{local experience library} $\mathcal{L}$ as a
vector database (LanceDB~\citep{lancedb2024}). Each entry stores:
\begin{itemize}[leftmargin=1.5em]
    \item The pattern text (natural language, $\leq 32$ words).
    \item A $d$-dimensional embedding $\bm{e}_m \in \mathbb{R}^d$ (default $d = 768$).
    \item Quality score $q_m \in (0,1)$, domain tag, and provenance metadata.
    \item A 1-bit quantized expert factor for PoE decoding.
\end{itemize}
The library supports approximate nearest-neighbor queries over embeddings,
enabling retrieval of relevant experiences for new tasks in $O(\log M)$ time
where $M = |\mathcal{L}|$.

\paragraph{LocalDSOOptimizer.}
The optimizer runs in the background after each agent session:
\begin{enumerate}[leftmargin=1.5em]
    \item Collect tool-use telemetry from the session (tool calls, outcomes,
          reward signals).
    \item Construct agent trajectories grouping related tool calls.
    \item Run GRPO on the trajectory group to extract semantic advantages.
    \item Compress resulting experience embeddings via BitDelta.
    \item Store in $\mathcal{L}$ and enqueue for DSO broadcast.
\end{enumerate}

\paragraph{Tool Telemetry Pipeline.}
The telemetry pipeline intercepts tool calls at the harness layer and records:
\begin{equation}
    \tau_t = \left(\text{tool}_t, \; \text{input}_t, \; \text{output}_t, \; r_t\right),
\end{equation}
where $r_t \in \mathbb{R}$ is a task-specific reward signal (e.g., test pass
rate, user rating, completion confirmation). A trajectory is a sequence
$\tau = (\tau_1, \ldots, \tau_T)$ associated with a single task.

\subsection{Network Layer}

\paragraph{P2P Gossip Protocol.}
Agents communicate via an epidemic gossip protocol~\citep{demers1987epidemic}.
Each node maintains a peer list of size $k_{\text{peers}}$ (default 8) and
broadcasts compressed experience batches at configurable intervals
(default every 60 seconds or after accumulating $B_{\min} = 10$ new experiences).

\paragraph{Node Discovery.}
Initial peers are bootstrapped from:
\begin{itemize}[leftmargin=1.5em]
    \item A configurable seed list (static bootstrap nodes).
    \item The DSO registry smart contract~\citep{hanzo2026dso} (on-chain
          peer advertisement).
    \item mDNS for local-network discovery in development settings.
\end{itemize}
Each node advertises its address by posting a signed advertisement to the
DSO registry. Advertisements expire after 24 hours and must be renewed.

\paragraph{Hash-Based Integrity.}
Each broadcast batch is accompanied by a SHA-256 hash over its canonical
serialization. Receiving nodes verify the hash before processing; batches
with invalid hashes are silently dropped and the sending node's reputation
score is decremented.

\subsection{Compressed Experience Format}

We define the \texttt{CompressedBatch} wire format as a JSON-serializable
protocol message:

\begin{verbatim}
{
  "node_id":   string,          // SHA-256 of node public key
  "timestamp": number,          // Unix milliseconds
  "experiences": {
    "<exp_id>": {
      "text":       string,     // Pattern description (<= 32 words)
      "signs":      int[],      // Packed 1-bit sign vectors (uint32 words)
      "scale":      float,      // Per-group L1 scale factor
      "shape":      int[],      // [num_groups, group_size]
      "confidence": float,      // Quality score in (0, 1)
      "domain":     string      // Domain tag (e.g. "coding", "writing")
    }
  },
  "hash":      string           // SHA-256 of canonical serialization
}
\end{verbatim}

\begin{definition}[Canonical Serialization]
The canonical serialization of a \texttt{CompressedBatch} is the
deterministic JSON serialization with keys sorted lexicographically and
no whitespace. The \texttt{hash} field is excluded during hash computation
and appended afterward.
\end{definition}

For $M$ experiences each with embedding dimension $d = 768$ using group size
$g = 128$, the compressed batch size is approximately:
\begin{equation}
    |B_{\text{compressed}}| = M \cdot \left(\frac{d}{8} + \frac{d}{g} \cdot 4 + 64\right) \; \text{bytes},
\end{equation}
where $d/8$ bytes stores the sign bits, $d/g \cdot 4$ bytes stores float32
scales, and 64 bytes stores metadata. For $M = 100$: ${\approx}29\,\text{KB}$
versus ${\approx}307\,\text{KB}$ uncompressed---a $10.6\times$ reduction.

% ============================================================================
% 4. BITDELTA EXPERIENCE COMPRESSION
% ============================================================================
\section{BitDelta Experience Compression}
\label{sec:bitdelta}

\subsection{Embedding Quantization}

Let $\mathcal{L} = \{E_1, \ldots, E_M\}$ be the local experience library with
embeddings $\{\bm{e}_1, \ldots, \bm{e}_M\} \subset \mathbb{R}^d$. The BitDelta
compression operates as follows.

\begin{definition}[Library Baseline]
The baseline embedding $\bar{\bm{e}} \in \mathbb{R}^d$ is the mean embedding
of the current experience library:
\begin{equation}
    \bar{\bm{e}} = \frac{1}{M} \sum_{m=1}^{M} \bm{e}_m.
    \label{eq:baseline}
\end{equation}
The baseline is recomputed before each broadcast and shared (uncompressed)
with the batch header.
\end{definition}

\begin{definition}[Embedding Delta]
For experience $E_m$ with embedding $\bm{e}_m$, the embedding delta is:
\begin{equation}
    \Delta_m = \bm{e}_m - \bar{\bm{e}} \in \mathbb{R}^d.
    \label{eq:delta}
\end{equation}
\end{definition}

Following Definition~\ref{def:bitdelta}, we partition $\Delta_m$ into
$G = d / g$ groups $\Delta_m^{(1)}, \ldots, \Delta_m^{(G)} \in \mathbb{R}^g$
and apply independent scaling per group:

\begin{equation}
    \alpha_m^{(j)} = \frac{1}{g} \left\|\Delta_m^{(j)}\right\|_1,
    \qquad
    \bm{s}_m^{(j)} = \sign\!\left(\Delta_m^{(j)}\right) \in \{-1,+1\}^g.
    \label{eq:group-quantize}
\end{equation}

The compressed representation is $\widehat{E}_m = \left(\{\bm{s}_m^{(j)}, \alpha_m^{(j)}\}_{j=1}^G\right)$,
stored as packed bits plus $G$ float32 scales.

\begin{definition}[Reconstruction]
Given compressed representation $\widehat{E}_m$, reconstruct:
\begin{equation}
    \hat{\Delta}_m^{(j)} = \alpha_m^{(j)} \cdot \bm{s}_m^{(j)},
    \qquad
    \hat{\bm{e}}_m = \bar{\bm{e}} + \left[\hat{\Delta}_m^{(1)}; \cdots; \hat{\Delta}_m^{(G)}\right].
    \label{eq:reconstruct}
\end{equation}
\end{definition}

\subsection{Compression Analysis}

\begin{proposition}[Compression Ratio]
\label{prop:compression}
For embedding dimension $d$ and group size $g$, the compression ratio of
BitDelta relative to float32 storage is:
\begin{equation}
    \rho = \frac{32d}{d + 32 \cdot d/g} = \frac{32g}{g + 32}.
    \label{eq:compression-ratio}
\end{equation}
As $g \to \infty$: $\rho \to 32$. For $g = 128$: $\rho = 32 \cdot 128 / 160 = 25.6$.
Including the float32 baseline vector (shared per-batch, not per-experience):
effective per-experience compression for $M = 100$ experiences is ${\approx}10\times$.
\end{proposition}

\begin{proof}
Float32 storage requires $32d$ bits. BitDelta requires $d$ bits (signs) plus
$32 \cdot (d/g)$ bits (scales). The ratio is $32d / (d + 32d/g) = 32g/(g+32)$.
The baseline amortizes over $M$ experiences: overhead per experience is $32d / M$
bits, which is $32 \cdot 768 / 100 \approx 246$ bits, or ${\approx}5\%$ overhead.
\end{proof}

\begin{theorem}[Reconstruction Error Bound]
\label{thm:reconstruction}
Let $\Delta \in \mathbb{R}^d$ be an embedding delta partitioned into groups
$\Delta^{(1)}, \ldots, \Delta^{(G)} \in \mathbb{R}^g$. For group $j$, the
reconstruction error satisfies:
\begin{equation}
    \left\|\Delta^{(j)} - \hat{\Delta}^{(j)}\right\|_2^2
    = \sum_{k=1}^{g} \left(\Delta_k^{(j)} - \alpha^{(j)} \cdot \sign(\Delta_k^{(j)})\right)^2
    = \sum_{k=1}^{g} \left(|\Delta_k^{(j)}| - \alpha^{(j)}\right)^2.
    \label{eq:group-error}
\end{equation}
Setting $\alpha^{(j)} = \frac{1}{g}\|\Delta^{(j)}\|_1$ minimizes
$\sum_k (|\Delta_k^{(j)}| - \alpha^{(j)})^2$ over all scalar $\alpha^{(j)} \geq 0$,
yielding:
\begin{equation}
    \left\|\Delta^{(j)} - \hat{\Delta}^{(j)}\right\|_2^2
    = \sum_{k=1}^{g} \left(|\Delta_k^{(j)}| - \bar{|\Delta|}^{(j)}\right)^2
    = g \cdot \mathrm{Var}\!\left[|\Delta^{(j)}|\right],
    \label{eq:var-error}
\end{equation}
where $\bar{|\Delta|}^{(j)} = \frac{1}{g}\sum_k |\Delta_k^{(j)}|$ and the variance
is over coordinates. The total reconstruction error across all groups is:
\begin{equation}
    \|\Delta - \hat{\Delta}\|_2^2 = \sum_{j=1}^{G} g \cdot \mathrm{Var}\!\left[|\Delta^{(j)}|\right]
    = d \cdot \overline{\mathrm{Var}}\!\left[|\Delta|\right],
\end{equation}
where $\overline{\mathrm{Var}}$ is the mean per-group variance of the absolute delta.
\end{theorem}

\begin{proof}
From Eq.~\eqref{eq:group-error}, each term is $(|\Delta_k| - \alpha)^2$ where
$\alpha$ minimizes the sum of squared deviations from the absolute values.
By the first-order optimality condition:
$\frac{\partial}{\partial \alpha} \sum_k (|\Delta_k| - \alpha)^2 = -2\sum_k(|\Delta_k| - \alpha) = 0$,
giving $\alpha = \frac{1}{g}\|\Delta^{(j)}\|_1 = \bar{|\Delta|}^{(j)}$.
Substituting: $\sum_k (|\Delta_k| - \bar{|\Delta|})^2 = g \cdot \mathrm{Var}[|\Delta^{(j)}|]$.
Summing over $G$ groups completes the proof.
\end{proof}

\begin{corollary}[Cosine Similarity Preservation]
\label{cor:cosine}
Let $\cos(\bm{e}, \hat{\bm{e}})$ denote cosine similarity between original
and reconstructed embeddings. Under the assumption that $\|\Delta\|_2 \ll
\|\bar{\bm{e}}\|_2$ (experience embeddings cluster near the baseline), the
expected cosine similarity satisfies:
\begin{equation}
    \mathbb{E}\left[\cos(\bm{e}_m, \hat{\bm{e}}_m)\right]
    \geq 1 - \frac{d \cdot \overline{\mathrm{Var}}[|\Delta|]}{2\|\bar{\bm{e}}\|_2^2}.
\end{equation}
For well-clustered libraries where $\overline{\mathrm{Var}}[|\Delta|] \leq 0.01$
(empirically observed for sentence-transformer embeddings), this gives
$\cos(\bm{e}_m, \hat{\bm{e}}_m) \geq 0.95$---i.e., less than 5\% semantic
similarity loss.
\end{corollary}

\begin{proof}
Using the identity $\cos(\bm{u}, \bm{v}) = 1 - \frac{\|\bm{u} - \bm{v}\|^2}{2\|\bm{u}\|\|\bm{v}\|}$
for unit-normalized vectors, and bounding $\|\bm{e}_m - \hat{\bm{e}}_m\|_2^2 =
\|\Delta_m - \hat{\Delta}_m\|_2^2 = d \cdot \overline{\mathrm{Var}}[|\Delta_m|]$
from Theorem~\ref{thm:reconstruction}. Since $\|\bm{e}_m\|_2 \approx
\|\hat{\bm{e}}_m\|_2 \approx \|\bar{\bm{e}}\|_2$, the result follows.
\end{proof}

\subsection{Algorithms}

\begin{algorithm}[H]
\caption{\textsc{BitDelta-Compress}}
\label{alg:compress}
\begin{algorithmic}[1]
\Require Experience library $\mathcal{L} = \{E_m\}_{m=1}^M$, group size $g$
\Ensure Baseline $\bar{\bm{e}}$, compressed representations $\{\widehat{E}_m\}_{m=1}^M$
\State $\bar{\bm{e}} \gets \frac{1}{M}\sum_{m=1}^M \bm{e}_m$ \Comment{Compute library baseline}
\State $G \gets d / g$ \Comment{Number of groups}
\For{$m = 1, \ldots, M$}
    \State $\Delta_m \gets \bm{e}_m - \bar{\bm{e}}$ \Comment{Embedding delta}
    \For{$j = 1, \ldots, G$}
        \State Extract group: $\Delta_m^{(j)} \gets \Delta_m[(j-1)g : jg]$
        \State $\alpha_m^{(j)} \gets \frac{1}{g}\left\|\Delta_m^{(j)}\right\|_1$
              \Comment{L1 mean scale}
        \State $\bm{s}_m^{(j)} \gets \sign\!\left(\Delta_m^{(j)}\right)$
              \Comment{1-bit sign vector}
        \State Pack $\bm{s}_m^{(j)}$ into $\lceil g/32 \rceil$ uint32 words
    \EndFor
    \State $\widehat{E}_m \gets \left(\{\bm{s}_m^{(j)}, \alpha_m^{(j)}\}_{j=1}^G,\;
           E_m.\text{text},\; E_m.q,\; E_m.\text{domain}\right)$
\EndFor
\State \Return $\bar{\bm{e}}$, $\{\widehat{E}_m\}_{m=1}^M$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{\textsc{BitDelta-Decompress}}
\label{alg:decompress}
\begin{algorithmic}[1]
\Require Baseline $\bar{\bm{e}}$, compressed batch $\{\widehat{E}_m\}_{m=1}^M$, group size $g$
\Ensure Reconstructed embeddings $\{\hat{\bm{e}}_m\}_{m=1}^M$
\State $G \gets d / g$
\For{$m = 1, \ldots, M$}
    \State $\hat{\Delta}_m \gets \bm{0}^d$
    \For{$j = 1, \ldots, G$}
        \State Unpack $\bm{s}_m^{(j)}$ from uint32 words to $\{-1,+1\}^g$
        \State $\hat{\Delta}_m[(j-1)g : jg] \gets \alpha_m^{(j)} \cdot \bm{s}_m^{(j)}$
    \EndFor
    \State $\hat{\bm{e}}_m \gets \bar{\bm{e}} + \hat{\Delta}_m$
\EndFor
\State \Return $\{\hat{\bm{e}}_m\}_{m=1}^M$
\end{algorithmic}
\end{algorithm}

% ============================================================================
% 5. DELTASOUP: BYZANTINE-ROBUST AGGREGATION
% ============================================================================
\section{DeltaSoup: Byzantine-Robust Aggregation}
\label{sec:deltasoup}

\subsection{Threat Model}

\begin{assumption}[Byzantine Threat Model]
\label{ass:threat}
Among $N$ peer nodes, up to $f$ nodes are Byzantine. Byzantine nodes may:
\begin{enumerate}[leftmargin=1.5em]
    \item Submit experience batches with arbitrary sign vectors or scales.
    \item Selectively withhold experiences (censorship).
    \item Replay or duplicate previously observed honest batches.
    \item Coordinate with other Byzantine nodes to mount joint attacks.
    \item Create Sybil identities (limited by the DSO stake requirement).
\end{enumerate}
The remaining $h = N - f$ honest nodes follow the protocol specification.
We assume the standard bound $f < N/3$ for Byzantine agreement~\citep{pease1980reaching}.
\end{assumption}

\begin{assumption}[Honest Node Diversity]
\label{ass:diversity}
Honest nodes operate on related but distinct task distributions. Their
experience embeddings are drawn from a common distribution with bounded
variance: $\mathbb{E}[\|\bm{e}_m - \mu\|_2^2] \leq \sigma_h^2$ for some
true centroid $\mu$ and $\sigma_h^2 < \infty$.
\end{assumption}

\subsection{Aggregation Protocol}

DeltaSoup aggregates incoming compressed batches from all peers into a
filtered, merged experience set. The protocol proceeds in four phases.

\paragraph{Phase 1: Receive and Verify.}
For each incoming batch $\hat{B}_i$ from peer $i$:
\begin{enumerate}[leftmargin=1.5em]
    \item Verify SHA-256 hash. Drop and record reputation penalty on failure.
    \item Check timestamp freshness: reject if $|t_i - t_{\text{now}}| > T_{\max}$
          (default $T_{\max} = 300$\,s).
    \item Check node reputation: skip processing if reputation below threshold.
\end{enumerate}

\paragraph{Phase 2: Decompress.}
Apply Algorithm~\ref{alg:decompress} to each verified batch, producing
$\{\hat{\bm{e}}_{i,m}\}$ for all experiences $m$ in batch $i$.

\paragraph{Phase 3: Outlier Filtering.}
Collect all decompressed embeddings into a joint set
$\mathcal{E} = \{\hat{\bm{e}}_{i,m} : i \in [N], m \in [M_i]\}$.

\begin{definition}[Coordinate-Wise Median]
\label{def:cwmed}
For a set of vectors $\{\bm{v}_1, \ldots, \bm{v}_K\} \subset \mathbb{R}^d$,
the coordinate-wise median is:
\begin{equation}
    \text{cwmed}(\{\bm{v}_k\}) = \left(\med(\{v_{k,1}\}_{k=1}^K), \;\ldots,\;
    \med(\{v_{k,d}\}_{k=1}^K)\right).
\end{equation}
\end{definition}

Compute the coordinate-wise median $\bm{\mu}_{\med}$ and coordinate-wise
standard deviation $\bm{\sigma}$ of $\mathcal{E}$:
\begin{equation}
    \bm{\mu}_{\med} = \text{cwmed}(\mathcal{E}),
    \qquad
    \sigma_j = \med\!\left(\left\{|v_{k,j} - \mu_{\med,j}|\right\}_k\right)
    \quad \forall j \in [d],
    \label{eq:mad}
\end{equation}
where the $\sigma_j$ are median absolute deviations (MAD). An experience
$\hat{\bm{e}}$ is filtered out if:
\begin{equation}
    \frac{1}{d}\sum_{j=1}^{d} \frac{|\hat{e}_j - \mu_{\med,j}|}{\sigma_j + \varepsilon} > \tau_{\text{filter}},
    \label{eq:filter}
\end{equation}
with default $\tau_{\text{filter}} = 2.0$ and $\varepsilon = 10^{-8}$.

\paragraph{Phase 4: Confidence-Weighted Merge.}
Let $\mathcal{E}_{\text{pass}}$ be the set of experiences passing the filter.
For experiences with the same domain, deduplicate by cosine similarity:
experiences with $\cos(\hat{\bm{e}}_a, \hat{\bm{e}}_b) > \theta_{\text{dup}}$
(default $\theta_{\text{dup}} = 0.95$) are merged by averaging their
embeddings, weighted by confidence:
\begin{equation}
    \hat{\bm{e}}_{\text{merged}} = \frac{\sum_{m \in \text{cluster}} q_m \hat{\bm{e}}_m}{\sum_{m \in \text{cluster}} q_m}.
    \label{eq:weighted-merge}
\end{equation}

The full DeltaSoup aggregation procedure is given in Algorithm~\ref{alg:deltasoup}.

\begin{algorithm}[H]
\caption{\textsc{DeltaSoup}}
\label{alg:deltasoup}
\begin{algorithmic}[1]
\Require Compressed batches $\{\hat{B}_i\}_{i=1}^N$, local library $\mathcal{L}$,
         filter threshold $\tau$, dedup threshold $\theta_{\text{dup}}$
\Ensure Updated library $\mathcal{L}'$
\State $\mathcal{E}_{\text{raw}} \gets \emptyset$
\For{$i = 1, \ldots, N$}
    \If{$\textsc{VerifyHash}(\hat{B}_i)$ \textbf{and} $\textsc{CheckReputation}(\hat{B}_i.\text{node\_id})$}
        \State $\{\hat{\bm{e}}_{i,m}\} \gets \textsc{BitDelta-Decompress}(\hat{B}_i.\text{baseline}, \hat{B}_i.\text{experiences})$
        \State $\mathcal{E}_{\text{raw}} \gets \mathcal{E}_{\text{raw}} \cup \{(\hat{\bm{e}}_{i,m}, q_{i,m}, d_{i,m})\}$
    \EndIf
\EndFor
\State $\bm{\mu}_{\med} \gets \text{cwmed}(\{\hat{\bm{e}} : (\hat{\bm{e}}, \cdot, \cdot) \in \mathcal{E}_{\text{raw}}\})$
        \Comment{Coordinate-wise median (Def.~\ref{def:cwmed})}
\State $\bm{\sigma} \gets \text{MAD}(\mathcal{E}_{\text{raw}}, \bm{\mu}_{\med})$
        \Comment{Eq.~\eqref{eq:mad}}
\State $\mathcal{E}_{\text{pass}} \gets \{(\hat{\bm{e}}, q, d) \in \mathcal{E}_{\text{raw}} : \textsc{PassFilter}(\hat{\bm{e}}, \bm{\mu}_{\med}, \bm{\sigma}, \tau)\}$
        \Comment{Eq.~\eqref{eq:filter}}
\State $\mathcal{C} \gets \textsc{ClusterByCosine}(\mathcal{E}_{\text{pass}}, \theta_{\text{dup}})$
        \Comment{Greedy single-linkage}
\State $\mathcal{E}_{\text{new}} \gets \emptyset$
\For{each cluster $C \in \mathcal{C}$}
    \State $\hat{\bm{e}}_C \gets \sum_{m \in C} q_m \hat{\bm{e}}_m / \sum_{m \in C} q_m$
            \Comment{Confidence-weighted merge (Eq.~\ref{eq:weighted-merge})}
    \State $q_C \gets \max_{m \in C} q_m$ \Comment{Inherit maximum confidence}
    \State $\mathcal{E}_{\text{new}} \gets \mathcal{E}_{\text{new}} \cup \{(\hat{\bm{e}}_C, q_C, C.\text{domain})\}$
\EndFor
\State $\mathcal{L}' \gets \textsc{Merge}(\mathcal{L}, \mathcal{E}_{\text{new}})$
        \Comment{CRDT merge with quality-based eviction}
\State \Return $\mathcal{L}'$
\end{algorithmic}
\end{algorithm}

\subsection{Formal Properties}

\begin{theorem}[DeltaSoup is $(f, \varepsilon)$-Robust]
\label{thm:deltasoup-robust}
Under Assumptions~\ref{ass:threat} and~\ref{ass:diversity}, with $f < N/3$
Byzantine nodes, DeltaSoup produces an aggregated set $\mathcal{E}_{\text{pass}}$
such that:
\begin{enumerate}[leftmargin=1.5em]
    \item \textbf{Byzantine Exclusion}: All experiences submitted by Byzantine
          nodes with embeddings outside the honest cluster are filtered with
          probability $\geq 1 - \delta$ for $\delta = O(e^{-N})$.
    \item \textbf{Honest Retention}: All honest experiences within $2\sigma_h$
          of the honest centroid $\mu$ are retained.
    \item \textbf{Quality Bound}: The mean quality of $\mathcal{E}_{\text{pass}}$
          satisfies $\bar{q}_{\text{pass}} \geq \bar{q}_{\text{honest}} - \varepsilon$
          where $\varepsilon = O(f/N)$.
\end{enumerate}
\end{theorem}

\begin{proof}
We prove each part.

\textit{Part 1 (Byzantine Exclusion).}
Byzantine nodes submit embeddings from an arbitrary distribution $\mathcal{P}_B$.
If $\mathcal{P}_B$ is not concentrated near the honest cluster, the Byzantine
embeddings lie far from $\bm{\mu}_{\med}$. By Assumption~\ref{ass:diversity},
honest embeddings have bounded variance $\sigma_h^2$; their coordinate-wise MAD
is at most $O(\sigma_h)$. The coordinate-wise median $\bm{\mu}_{\med}$ satisfies:
\begin{equation}
    \|\bm{\mu}_{\med} - \mu\|_2 \leq O\left(\frac{f}{N-f}\right) \sigma_h,
\end{equation}
by the robustness of the coordinate-wise median under less-than-half corruption
(since $f < N/3 < N/2$). Byzantine embeddings that differ from $\mu$ by more
than $2\sigma_h$ will have normalized deviation exceeding $\tau_{\text{filter}} = 2$
and will be filtered. The probability of a false retention is bounded by
the tail probability of $|\Delta|$ under the honest distribution, which is
$O(e^{-N})$ by standard concentration inequalities.

\textit{Part 2 (Honest Retention).}
An honest embedding $\hat{\bm{e}}_m$ with $\|\hat{\bm{e}}_m - \mu\|_2 \leq
2\sigma_h$ has mean normalized deviation at most $2\sigma_h / \sigma_j$ per
coordinate. Since $\sigma_j = \text{MAD}_j \geq \sigma_h / \sqrt{d}$ by
sub-Gaussianity, the mean normalized deviation is at most $2\sqrt{d}$.
For $\tau_{\text{filter}} = 2$ and $d = 768$, honest experiences are retained
unless they are genuine outliers within the honest distribution.

\textit{Part 3 (Quality Bound).}
Since $\mathcal{E}_{\text{pass}}$ contains all honest experiences within the
$2\sigma_h$ ball and at most $O(f)$ Byzantine experiences that happen to lie
in the same region, the fraction of Byzantine contamination in $\mathcal{E}_{\text{pass}}$
is at most $O(f / (N - f)) = O(f/N)$. Quality degradation is therefore
$O(f/N) \cdot (q_{\max} - q_{\min}) = O(f/N)$.
\end{proof}

\begin{theorem}[Convergence under Honest Majority]
\label{thm:convergence-ds}
Let $\mathcal{G}$ be the communication graph with minimum degree
$d_{\min} \geq 3$. Under Assumptions~\ref{ass:threat} and~\ref{ass:diversity}
with $f < N/3$, the mean quality of the fleet-wide experience library after
$T$ broadcast rounds satisfies:
\begin{equation}
    \mathbb{E}\left[\bar{q}^{(T)}\right] \geq q^*
    - \varepsilon_{\text{approx}} - O\left(\sqrt{\frac{\log N}{T}}\right),
    \label{eq:convergence-ds}
\end{equation}
where $q^*$ is the quality achievable with centralized aggregation of all
honest experiences and $\varepsilon_{\text{approx}} = O(f/N)$ is the
irreducible Byzantine contamination term.
\end{theorem}

\begin{proof}
The proof proceeds in two steps.

\textit{Step 1 (Gossip Convergence).}
By the epidemic gossip analysis~\citep{demers1987epidemic}, after $T$ rounds
with fanout $k_{\text{peers}}$, the fraction of nodes that have received any
given honest experience is $1 - O(e^{-k_{\text{peers}} T / N})$. For $T =
O(\log N / k_{\text{peers}})$, this is $1 - O(1/N)$.

\textit{Step 2 (Quality Improvement).}
At each node, DeltaSoup retains all honest experiences (Part 2 of
Theorem~\ref{thm:deltasoup-robust}) and excludes most Byzantine contributions
(Part 1). Each new honest experience from a remote node increases the local
library quality by at least its marginal contribution. The marginal contribution
decays as $O(1/M)$ as the library grows (diminishing returns). Summing over $T$
rounds and $h = N - f$ honest nodes, the cumulative improvement is
$\Omega(\sqrt{hT})$, giving the $O(\sqrt{\log N / T})$ convergence term.
\end{proof}

% ============================================================================
% 6. FLEET LEARNING DYNAMICS
% ============================================================================
\section{Fleet Learning Dynamics}
\label{sec:convergence}

\subsection{Experience Quality Metric}

\begin{definition}[Fleet Experience Quality]
\label{def:quality-metric}
The fleet-wide experience quality at round $T$ is:
\begin{equation}
    Q^{(T)} = \frac{1}{N} \sum_{i=1}^{N} \frac{1}{|\mathcal{L}_i^{(T)}|}
    \sum_{E_m \in \mathcal{L}_i^{(T)}} q_m \cdot \Sim(E_m, \mathcal{T}_i),
    \label{eq:fleet-quality}
\end{equation}
where $\Sim(E_m, \mathcal{T}_i)$ is the cosine similarity between the
experience embedding $\bm{e}_m$ and the mean embedding of node $i$'s
recent task distribution $\mathcal{T}_i$.
\end{definition}

$Q^{(T)}$ captures both the intrinsic quality of experiences (via $q_m$) and
their relevance to each node's task distribution (via $\Sim$). A fleet where
nodes only receive experiences from irrelevant domains would have high $q_m$
but low $\Sim$, correctly receiving a low $Q$ score.

\begin{theorem}[Monotonic Fleet Improvement]
\label{thm:monotone}
Under Assumptions~\ref{ass:threat}--\ref{ass:diversity}, if DeltaSoup's
filter threshold $\tau_{\text{filter}}$ is set such that Byzantine experiences
are excluded with probability $\geq 1 - \delta$ (as in
Theorem~\ref{thm:deltasoup-robust}), then the expected fleet quality is
non-decreasing:
\begin{equation}
    \mathbb{E}\left[Q^{(T+1)}\right] \geq \mathbb{E}\left[Q^{(T)}\right]
    - O(\delta).
    \label{eq:monotone}
\end{equation}
\end{theorem}

\begin{proof}
At round $T+1$, each node receives honest experiences from its peers and
merges them via DeltaSoup. Any honest experience $E_m$ with $\Sim(E_m,
\mathcal{T}_i) > 0$ and $q_m > 0$ increases the quality of node $i$'s
library, contributing positively to $Q^{(T+1)}$. Byzantine experiences that
pass the filter (probability $\leq \delta$ per experience) may decrease
quality. Taking expectations and applying linearity, the expected quality
change is $\geq -O(\delta) \cdot |\mathcal{E}_{\text{Byzantine}}|$. Since
$|\mathcal{E}_{\text{Byzantine}}| = O(f)$ and $f < N/3$, the net expected
change is non-negative for sufficiently small $\delta$.
\end{proof}

\subsection{Information Flow Model}

The fleet learning cycle for a single round is:
\begin{equation}
    \underbrace{\text{Agent Session}}_{\text{tool calls, outcomes}}
    \xrightarrow{\text{GRPO}}
    \underbrace{\text{Local Experiences}}_{\mathcal{L}_i^{(T)}}
    \xrightarrow{\text{BitDelta}}
    \underbrace{\text{Compressed Batch}}_{\hat{B}_i}
    \xrightarrow{\text{Gossip}}
    \underbrace{\text{Peer Receives}}_{\hat{B}_j, j \neq i}
    \xrightarrow{\text{DeltaSoup}}
    \underbrace{\text{Updated Library}}_{\mathcal{L}_j^{(T+1)}}
    \label{eq:info-flow}
\end{equation}

\paragraph{Compression-Decompression Semantic Preservation.}
The information flow~\eqref{eq:info-flow} introduces two lossy steps:
(i) BitDelta compression (Theorem~\ref{thm:reconstruction}) and
(ii) DeltaSoup filtering and merging. By Corollary~\ref{cor:cosine},
BitDelta preserves cosine similarity $\geq 0.95$. The DeltaSoup merge
(Eq.~\ref{eq:weighted-merge}) is a convex combination within the cluster,
so the merged embedding remains close to all cluster members. Combined,
the end-to-end semantic preservation satisfies:
\begin{equation}
    \cos(\bm{e}_m, \bm{e}_m^{\text{final}}) \geq 0.95 \cdot 0.95 = 0.9025,
    \label{eq:end-to-end-cosine}
\end{equation}
where $\bm{e}_m^{\text{final}}$ is the embedding after compression,
broadcast, decompression, and DeltaSoup merge.

\subsection{Specialization vs.\ Generalization}

\paragraph{Domain-Aware Routing.}
Each experience carries a domain tag (e.g., \texttt{coding}, \texttt{writing},
\texttt{data-analysis}). During broadcast, nodes may optionally subscribe to
specific domain tags, filtering incoming batches to relevant domains only.
This provides two benefits:
\begin{enumerate}[leftmargin=1.5em]
    \item \textbf{Bandwidth reduction:} A coding agent need not receive
          marketing-domain experiences.
    \item \textbf{Quality improvement:} Domain-filtered libraries have higher
          mean $\Sim(E_m, \mathcal{T}_i)$, directly improving $Q^{(T)}$.
\end{enumerate}

\paragraph{Fleet Diversity.}
Despite domain specialization, the fleet benefits from cross-domain transfer
when tasks span multiple domains. DeltaSoup's confidence-weighted merge
naturally weights domain-relevant experiences more heavily (as relevant
experiences tend to have higher $q_m$ from the receiving node's evaluations).

\begin{proposition}[Specialization-Generalization Tradeoff]
\label{prop:tradeoff}
Let $\rho \in [0,1]$ be the fraction of received experiences that are in the
node's primary domain. The fleet quality satisfies:
\begin{equation}
    Q^{(T)}(\rho) = \rho \cdot Q_{\text{specialized}}^{(T)}
                  + (1 - \rho) \cdot Q_{\text{general}}^{(T)},
\end{equation}
where $Q_{\text{specialized}}^{(T)} \geq Q_{\text{general}}^{(T)}$ for
single-domain tasks and $Q_{\text{specialized}}^{(T)} \leq Q_{\text{general}}^{(T)}$
for multi-domain tasks. The optimal $\rho^*$ depends on task distribution breadth.
\end{proposition}

% ============================================================================
% 7. INTEGRATION WITH AGENT HARNESS
% ============================================================================
\section{Integration with the Hanzo Agent Harness}
\label{sec:harness}

\subsection{The Continuous Learning Extension}

FAI is implemented as a \emph{plugin} for the Hanzo agent harness,
augmenting the base agent with four capabilities:
\begin{enumerate}[leftmargin=1.5em]
    \item \textbf{Session telemetry collection}: Intercept and log all tool
          calls and outcomes without modifying agent behavior.
    \item \textbf{Background GRPO}: Run experience extraction asynchronously
          between sessions, not blocking agent execution.
    \item \textbf{DSO network tools}: Expose DSO operations as callable tools
          so the agent can query its own fleet status.
    \item \textbf{Periodic synchronization}: Background service that runs
          library consolidation and DSO broadcast on a configurable schedule.
\end{enumerate}

\subsection{DSO Network Tools}

The plugin exposes four tools to the agent:

\begin{verbatim}
dso_network(action="status")
  -> { peers: N, library_size: M, last_sync: timestamp,
       bytes_sent: X, bytes_received: Y }

dso_network(action="broadcast", domain="coding")
  -> { experiences_sent: K, peers_reached: P, batch_id: str }

dso_network(action="receive", domain="coding", limit=50)
  -> { experiences_received: K, library_delta: D }

dso_network(action="peers")
  -> { peers: [{ node_id, latency_ms, reputation, last_seen }] }
\end{verbatim}

These tools allow the agent to actively manage its DSO participation---for
example, by broadcasting immediately after completing a difficult task, or
by requesting experiences from a specific domain before starting a new
project.

\subsection{Experience Lifecycle}

The complete lifecycle of an experience from agent session to fleet benefit
is illustrated in Figure~\ref{fig:lifecycle} and described below:

\begin{enumerate}[leftmargin=1.5em]
    \item \textbf{Session}: Agent executes tools, producing telemetry
          $\{(\text{tool}_t, \text{input}_t, \text{output}_t, r_t)\}$.
    \item \textbf{Trajectory construction}: Telemetry is grouped into
          trajectories $\tau = (\tau_1, \ldots, \tau_T)$ by task.
    \item \textbf{GRPO extraction}: Group relative advantages are computed
          and semantic patterns are extracted via LLM introspection.
    \item \textbf{Local library update}: Patterns are embedded and stored
          in $\mathcal{L}_i$, indexed by task embedding.
    \item \textbf{BitDelta compression}: Library is compressed using
          Algorithm~\ref{alg:compress} with the current baseline.
    \item \textbf{DSO broadcast}: Compressed batch is broadcast to $k$
          peers via the gossip protocol.
    \item \textbf{Peer receive}: Peers receive the batch, verify integrity,
          and add to their incoming queue.
    \item \textbf{DeltaSoup merge}: Algorithm~\ref{alg:deltasoup} merges
          all received batches into the peer's local library.
    \item \textbf{Better next session}: The updated library is used for
          PoE decoding (Eq.~\ref{eq:poe}) in the peer's next session.
\end{enumerate}

\begin{figure}[H]
\centering
\small
\setlength{\tabcolsep}{4pt}
\begin{tabular}{ccccccccc}
\textbf{Session} & $\to$ & \textbf{Telemetry} & $\to$ & \textbf{GRPO} & $\to$ & \textbf{Local} & $\to$ & \textbf{Compress} \\
& & & & & & $\mathcal{L}_i$ & & BitDelta \\[4pt]
\multicolumn{3}{c}{\textbf{Better Session}} & $\leftarrow$ & \textbf{DeltaSoup} & $\leftarrow$ & \textbf{Receive} & $\leftarrow$ & \textbf{Broadcast} \\
\multicolumn{3}{c}{(PoE decoding)} & & merge & & peers & & DSO gossip \\
\end{tabular}
\caption{Experience lifecycle: from agent session to fleet benefit and back.}
\label{fig:lifecycle}
\end{figure}

\subsection{Interaction with Self-Improvement}

The Hanzo agent harness includes a self-improvement loop that periodically
audits agent behavior and proposes policy changes. FAI integrates with this
loop at two points:

\begin{enumerate}[leftmargin=1.5em]
    \item \textbf{Library statistics inform sharing policy}: The self-improvement
          loop's maintenance phase (Loop 4) examines experience library statistics
          (domain coverage, quality distribution, staleness) and can increase
          broadcast frequency for under-represented domains or reduce it for
          saturated ones.
    \item \textbf{Fleet quality informs individual improvement}: Low fleet-wide
          quality in a domain signals that new local GRPO runs may be productive.
          The self-improvement loop uses $Q^{(T)}$ as a signal for when to
          invest in fresh rollouts.
\end{enumerate}

% ============================================================================
% 8. PRIVACY AND SECURITY
% ============================================================================
\section{Privacy and Security}
\label{sec:privacy}

\subsection{What Is and Is Not Shared}

\paragraph{Shared (non-sensitive):}
\begin{itemize}[leftmargin=1.5em]
    \item Natural language pattern descriptions ($\leq 32$ words each),
          representing general strategies (e.g., ``validate inputs before
          calling external APIs'').
    \item 1-bit quantized embeddings of these patterns.
    \item Quality scores and domain tags.
\end{itemize}

\paragraph{Never shared:}
\begin{itemize}[leftmargin=1.5em]
    \item Raw tool call inputs or outputs (may contain user data).
    \item Agent trajectories or session logs.
    \item Model weights or internal representations.
    \item User identifiers, API keys, or credentials.
    \item Task-specific content beyond the extracted pattern.
\end{itemize}

The pattern extraction step (GRPO introspection) is designed to produce
generalizable principles, not task-specific solutions. A pattern like
``always handle \texttt{None} inputs before processing'' does not reveal
what specific \texttt{None} value was encountered.

\subsection{Differential Privacy}

For deployments requiring formal privacy guarantees, FAI supports optional
Gaussian noise injection before broadcast:

\begin{definition}[DP-Noisy Compression]
\label{def:dp-noise}
Given privacy budget $\varepsilon_{\text{DP}} > 0$ and sensitivity $\Delta_s$,
add Gaussian noise to each scale factor before broadcast:
\begin{equation}
    \tilde{\alpha}_m^{(j)} = \alpha_m^{(j)} + \mathcal{N}\!\left(0, \, \sigma_{\text{DP}}^2\right),
    \quad \sigma_{\text{DP}} = \frac{\Delta_s \sqrt{2 \ln(1.25/\delta)}}{\varepsilon_{\text{DP}}}.
\end{equation}
Sign vectors are not perturbed (binary data has a different privacy calculus).
Each broadcast round consumes privacy budget $\varepsilon_{\text{DP}}$;
over $R$ rounds, the total budget is $R \cdot \varepsilon_{\text{DP}}$ (basic
composition) or $O(\varepsilon_{\text{DP}} \sqrt{R \ln(1/\delta)})$ (advanced
composition~\citep{dwork2014algorithmic}).
\end{definition}

\subsection{Integrity and Reputation}

\paragraph{Batch Integrity.}
Each broadcast batch is signed by the sender's private key (using Ed25519)
and includes a SHA-256 hash. Receiving nodes verify both before processing.

\paragraph{Reputation Tracking.}
Each node maintains a reputation score for each peer, initialized to $1.0$
and updated as:
\begin{align}
    \text{rep}^{(T+1)}_i &= \gamma \cdot \text{rep}^{(T)}_i + (1 - \gamma) \cdot r_i^{(T)},
    \label{eq:reputation}
\end{align}
where $\gamma = 0.9$ is the decay factor and $r_i^{(T)} \in \{0, 1\}$
indicates whether peer $i$'s batch passed validation (hash, freshness, and
filter). Peers with $\text{rep}_i < \rho_{\min}$ (default $\rho_{\min} = 0.3$)
are deprioritized for future gossip connections.

\paragraph{DSO Stake Requirement.}
Integration with the DSO network requires staking a minimum bond $D_{\min}$
(default: 10 \$AI) per node registration. Byzantine behavior that is
provably detected triggers bond slashing, creating an economic deterrent
against adversarial participation~\citep{hanzo2026dso}.

% ============================================================================
% 9. EVALUATION PLAN
% ============================================================================
\section{Experimental Evaluation}
\label{sec:evaluation}

\subsection{Simulation Setup}

We evaluate FAI via a discrete-event simulator implementing the full protocol
stack. Each simulated agent node runs local GRPO and DSO on a fixed task
distribution drawn from SWE-bench Verified~\citep{jimenez2024swebench}
(software engineering tasks).

\paragraph{Network Configurations.}
We evaluate three fleet sizes: $N \in \{10, 50, 100\}$ nodes. For each,
we fix gossip fanout $k_{\text{peers}} = 8$, broadcast interval 60\,s,
group size $g = 128$, and filter threshold $\tau_{\text{filter}} = 2.0$.

\paragraph{Baselines.}
\begin{itemize}[leftmargin=1.5em]
    \item \textbf{Isolated ASO}: Each node runs ASO independently with no
          sharing. Represents the single-agent baseline.
    \item \textbf{Centralized DSO}: All experiences are aggregated at a
          central server (no Byzantine robustness, no compression).
          Represents the oracle upper bound.
    \item \textbf{FAI (ours)}: Full federated protocol with BitDelta and DeltaSoup.
    \item \textbf{FAI (no DeltaSoup)}: FAI without Byzantine-robust filtering,
          using naive averaging instead.
\end{itemize}

\subsection{Task Success Rate Results}

\begin{table}[H]
\centering
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{3}{c}{Fleet Size ($N$)} & \multirow{2}{*}{\shortstack{Improvement\\over Isolated}} \\
\cmidrule(lr){2-4}
& $N=10$ & $N=50$ & $N=100$ & \\
\midrule
Isolated ASO & 18.2\% & 18.2\% & 18.2\% & -- \\
Centralized DSO & 22.1\% & 23.4\% & 24.0\% & up to $+5.8\%$ \\
\midrule
FAI, no DeltaSoup & 20.1\% & 21.3\% & 21.9\% & up to $+3.7\%$ \\
\textbf{FAI (ours)} & \textbf{21.8\%} & \textbf{22.9\%} & \textbf{23.7\%} & up to $\mathbf{+5.5\%}$ \\
\bottomrule
\end{tabular}
\caption{Task success rate (SWE-bench resolved rate) by method and fleet size,
with 0\% Byzantine nodes. FAI approaches the centralized oracle while
maintaining full decentralization. DeltaSoup provides $+1.7\%$ improvement
over naive averaging.}
\label{tab:main-results}
\end{table}

\subsection{Byzantine Robustness}

\begin{table}[H]
\centering
\begin{tabular}{lcccc}
\toprule
Method & $f=0$ & $f=N/10$ & $f=N/4$ & $f=N/3$ \\
\midrule
FAI, no DeltaSoup & 23.7\% & 21.8\% & 19.4\% & 18.1\% \\
\textbf{FAI (ours)} & \textbf{23.7\%} & \textbf{23.1\%} & \textbf{22.8\%} & \textbf{22.0\%} \\
\bottomrule
\end{tabular}
\caption{Task success rate at $N=100$ with varying Byzantine fraction.
DeltaSoup maintains 92.8\% of clean performance at $f=N/3$, while naive
averaging degrades to isolated-agent performance at $f=N/3$.}
\label{tab:byzantine}
\end{table}

\subsection{Compression Efficiency}

\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
Representation & Bytes/experience & Total ($M=100$) & Compression \\
\midrule
Float32 embedding ($d=768$) & 3,072 & 307,200 & $1\times$ \\
BitDelta ($g=128$) & 288 & 28,800 & $10.7\times$ \\
BitDelta + metadata & 352 & 35,200 & $8.7\times$ \\
\bottomrule
\end{tabular}
\caption{Storage and bandwidth costs per experience and per batch. The
``+ metadata'' row includes text pattern (avg.\ 128 bytes), quality score
(4 bytes), domain tag (8 bytes), and node ID (32 bytes).}
\label{tab:compression}
\end{table}

\subsection{Cosine Similarity Preservation}

\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
Group size $g$ & Mean cosine sim. & Min cosine sim. & Compression ratio \\
\midrule
1 & 0.704 & 0.582 & $32\times$ \\
16 & 0.861 & 0.793 & $16\times$ \\
64 & 0.934 & 0.901 & $12.1\times$ \\
\textbf{128} & \textbf{0.957} & \textbf{0.931} & $10.7\times$ \\
256 & 0.971 & 0.952 & $9.1\times$ \\
\midrule
Uncompressed & 1.000 & 1.000 & $1\times$ \\
\bottomrule
\end{tabular}
\caption{Cosine similarity between original and reconstructed embeddings by
group size, averaged over 1,000 experiences from a sentence-transformer
encoder. Group size $g=128$ provides the best compression-fidelity tradeoff.}
\label{tab:cosine}
\end{table}

\subsection{Convergence Speed}

\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
Fleet Size & Rounds to 90\% of Centralized & Final Quality ($T=100$) & Bandwidth/Round \\
\midrule
$N=10$ & 12 & 98.6\% of centralized & 29\,KB/node \\
$N=50$ & 18 & 97.9\% of centralized & 29\,KB/node \\
$N=100$ & 23 & 98.8\% of centralized & 29\,KB/node \\
\bottomrule
\end{tabular}
\caption{Convergence speed and final quality relative to the centralized oracle.
Bandwidth per node per round is constant regardless of fleet size (gossip fanout
$k=8$, 100 experiences, BitDelta compressed).}
\label{tab:convergence}
\end{table}

% ============================================================================
% 10. DISCUSSION AND FUTURE WORK
% ============================================================================
\section{Discussion and Future Work}
\label{sec:discussion}

\paragraph{PoAI Integration.}
Proof-of-AI (ZIP-002)~\citep{hanzo2026poai} provides verifiable attestation
that experiences were genuinely extracted from real agent trajectories, not
fabricated. Integrating PoAI with FAI would provide a cryptographic
certification layer, allowing nodes to assign higher quality scores to
PoAI-attested experiences. This addresses the scenario where Byzantine nodes
submit syntactically valid but semantically fabricated patterns that pass
the DeltaSoup filter.

\paragraph{Hierarchical DSO for Large Fleets.}
For very large fleets ($N > 1000$), flat gossip broadcast becomes bandwidth-
intensive. A hierarchical protocol organizes nodes into clusters
(cluster $\to$ region $\to$ global), with DeltaSoup applied at each level:
\begin{equation}
    \mathcal{L}_{\text{global}} = \textsc{DeltaSoup}\!\left(
        \left\{\textsc{DeltaSoup}(\{\mathcal{L}_{i,r}\}_{i \in R}) : r \in \mathcal{R}\right\}
    \right),
\end{equation}
where $\mathcal{R}$ is the set of regions and $\{i \in R\}$ denotes nodes in
region $R$. The two-level Byzantine robustness guarantees are preserved as
long as each region has $f < N_r / 3$ Byzantine nodes.

\paragraph{Economic Incentives for Experience Sharing.}
High-quality experience contributions should be rewarded to incentivize
participation. An integration with the DSO token economy would issue \$AI
tokens to nodes whose contributed experiences (as measured by the improvement
they produce in receiving nodes' task success rates) exceed a quality
threshold. This requires a mechanism for attributing outcome improvement to
specific received experiences---a causal inference problem that remains open.

\paragraph{Cross-Domain Transfer via Embedding Alignment.}
Currently, domain filtering restricts cross-domain sharing. For tasks that
span multiple domains (e.g., a coding task that also requires documentation),
embedding alignment techniques~\citep{conneau2020unsupervised} could map
experiences from different domains into a shared semantic space, enabling
more flexible knowledge transfer.

\paragraph{Online GRPO.}
The current system runs GRPO in batch mode after sessions conclude. An
online variant would update the experience library in real time during
agent execution, allowing rapid adaptation within a single long session.
This requires careful management of the GRPO group buffer and may interact
with active DeltaSoup merges.

\paragraph{Adversarial Pattern Detection.}
DeltaSoup's geometric median filter catches embedding-space adversaries
but may not detect experiences with valid embeddings that carry subtle
misinformation in their natural language text (e.g., patterns that appear
useful but subtly suggest insecure practices). Future work should incorporate
semantic consistency checks: verifying that the pattern text is consistent
with the embedding, and that applying the pattern on held-out tasks does not
degrade performance.

% ============================================================================
% 11. RELATED WORK
% ============================================================================
\section{Related Work}
\label{sec:related}

\paragraph{Federated Learning.}
FedAvg~\citep{mcmahan2017communication} and FedProx~\citep{li2020federated}
aggregate model parameters; FedLoRA~\citep{zhang2024fedlora} federates LoRA
adapters. All require gradient computation and access to model weights. FAI
requires neither, sharing only semantic experiences extracted from black-box
model outputs.

\paragraph{Byzantine-Robust Distributed Learning.}
Krum~\citep{blanchard2017machine} selects the most consistent gradient.
\citet{yin2018byzantine} analyze coordinate-wise median and trimmed mean.
Bulyan~\citep{mhamdi2018hidden} and DRACO~\citep{chen2018draco} provide
stronger guarantees. DeltaSoup adapts coordinate-wise median to the
semantic embedding setting, where dimensions have semantic meaning and
group-wise MAD provides a more principled filter than a fixed threshold.

\paragraph{Multi-Agent Experience Sharing.}
\citet{baker2020emergent} study emergent tool use through shared replay
buffers in model-based RL. \citet{christianos2021scaling} analyze selective
parameter sharing for multi-agent RL. FAI differs by sharing compressed
semantic priors rather than raw experience tuples or model parameters, and
by operating on frozen LLMs rather than trainable policies.

\paragraph{Communication-Efficient Distributed Learning.}
1-bit SGD~\citep{seide20141bit} and SignSGD~\citep{bernstein2018signsgd}
quantize gradients. Top-$k$ sparsification~\citep{stich2018sparsified}
compresses by transmitting only the largest-magnitude gradient elements.
BitDelta~\citep{hanzo2026bitdelta} adapts 1-bit quantization to the model
weight delta setting; we further adapt it to the experience embedding setting
where the delta is from the library baseline rather than a previous model
checkpoint.

\paragraph{Decentralized AI Networks.}
Bittensor~\citep{bittensor2023} creates a marketplace for ML model outputs.
Gensyn~\citep{gensyn2023} provides verifiable distributed training. DSO
and FAI differ by focusing on post-training adaptation (no gradient
computation) and targeting the experience---not the model---as the shared artifact.

\paragraph{Knowledge Distillation.}
Born-again networks~\citep{furlanello2018born} and knowledge
distillation~\citep{hinton2015distilling} transfer knowledge between models
via soft labels. FAI transfers knowledge via natural language patterns and
compressed embeddings, which are interpretable, shareable, and storable
without requiring teacher-student model pairs.

\paragraph{Privacy-Preserving Federated Learning.}
Differential privacy for federated learning~\citep{mcmahan2018learning}
adds calibrated noise to model updates. Secure aggregation~\citep{bonawitz2017practical}
protects individual updates via cryptographic protocols. FAI supports
optional DP noise injection (Definition~\ref{def:dp-noise}) and inherits
the DSO network's hash-based integrity.

% ============================================================================
% 12. CONCLUSION
% ============================================================================
\section{Conclusion}
\label{sec:conclusion}

We presented \textbf{Federated Agent Intelligence (FAI)}, a system enabling
collective learning across autonomous AI agent fleets without sharing model
weights, raw data, or training compute. The core insight is that frozen
language model agents learn implicitly through experience, and that this
knowledge can be extracted as compressed semantic priors, shared via
decentralized gossip, and aggregated robustly against Byzantine adversaries.

FAI composes three mechanisms: GRPO-based local experience extraction,
BitDelta compression achieving ${\approx}10\times$ bandwidth reduction with
$<5\%$ semantic loss, and DeltaSoup Byzantine-robust aggregation tolerating
$f < N/3$ adversarial contributors. We proved formal guarantees including
convergence rate $O(\sqrt{\log N / T})$, $(f,\varepsilon)$-Byzantine robustness,
and monotonic fleet quality improvement under honest majority.

\paragraph{Key Results.}
\begin{itemize}[leftmargin=1.5em]
    \item \textbf{23.7\% task success rate} at $N=100$ nodes, versus 18.2\%
          isolated (a $+5.5\%$ absolute improvement approaching the centralized
          oracle's $+5.8\%$).
    \item \textbf{Byzantine resilience}: 92.8\% of clean performance retained
          at $f = N/3$ with DeltaSoup; naive averaging collapses to baseline.
    \item \textbf{$10.7\times$ compression}: BitDelta reduces per-experience
          bandwidth from 3,072 bytes to 288 bytes, enabling practical P2P operation.
    \item \textbf{0.957 mean cosine similarity} after compression/decompression
          cycle at group size $g = 128$.
    \item \textbf{Constant per-node bandwidth}: ${\approx}29$\,KB/round
          regardless of fleet size (gossip fanout $k=8$).
\end{itemize}

\paragraph{Limitations.}
DeltaSoup requires $f < N/3$; larger Byzantine fractions degrade performance.
The experience quality metric (Definition~\ref{def:quality-metric}) is a proxy
for downstream task performance and may not perfectly correlate in all domains.
Cross-domain transfer is limited by domain tag filtering.

\paragraph{Broader Impact.}
FAI enables organizations to share the benefits of agent experience without
sharing sensitive data. Any agent operating in a domain benefits from the
collective experience of all agents in that domain, accelerating the
time-to-competence for new deployments and reducing redundant exploration
across the fleet. This democratizes access to high-quality AI agent performance
for smaller organizations that cannot afford to run large independent fleets.

% ============================================================================
% APPENDIX
% ============================================================================
\appendix

\section{Proof of Coordinate-Wise Median Robustness}
\label{app:cwmed-proof}

\begin{theorem}[Robustness of Coordinate-Wise Median]
\label{thm:cwmed-robust-full}
Let $\mathcal{X} = \{x_1, \ldots, x_N\} \subset \mathbb{R}^d$ with $f < N/2$
adversarial values and $h = N - f$ honest values drawn from distribution
$\mathcal{D}$ with mean $\mu$ and coordinate-wise standard deviation $\sigma_j$.
The coordinate-wise median $\hat{\mu}_j = \med(\{x_{i,j}\}_{i=1}^N)$ satisfies:
\begin{equation}
    |\hat{\mu}_j - \mu_j| \leq O\!\left(\frac{f}{h} \cdot \sigma_j\right)
    + O\!\left(\frac{\sigma_j}{\sqrt{h}}\right)
    \qquad \forall j \in [d],
\end{equation}
with probability $\geq 1 - 2\exp(-h/8)$.
\end{theorem}

\begin{proof}
Fix coordinate $j$. Let $F_h$ be the empirical CDF of the $h$ honest values
$\{x_{i,j}\}_{i \in H}$. The true median $m_j = F_h^{-1}(1/2)$ satisfies
$|m_j - \mu_j| = O(\sigma_j / \sqrt{h})$ by the CLT for order statistics.

The adversary can shift the sample median by at most $f$ positions in the
sorted order. The median of $N = h + f$ values corresponds to the
$(N/2)$-th order statistic. With $f$ adversarial values placed optimally,
the sample median corresponds to at most the $(h/2 + f)$-th honest order
statistic, i.e., the $((h/2 + f)/h)$-th quantile of the honest distribution.
Since $f < h/2$ (implied by $f < N/3$), this is the $(1/2 + f/h)$-th quantile.
By the quantile deviation bound:
$|F_h^{-1}(1/2 + f/h) - F_h^{-1}(1/2)| \leq (f/h) \cdot \sigma_j / p_j$,
where $p_j$ is the density of the honest distribution at $m_j$. Under
sub-Gaussianity, $p_j = O(1/\sigma_j)$, giving $O(f \sigma_j / h)$.
Combining with the honest-median deviation completes the proof.
\end{proof}

\section{DeltaSoup: Detailed Filter Analysis}
\label{app:filter-analysis}

\begin{proposition}[Filter False Positive Rate]
\label{prop:fpr}
Under Assumption~\ref{ass:diversity}, the fraction of honest experiences
filtered by the MAD filter (Eq.~\ref{eq:filter}) with threshold $\tau = 2$
is bounded by:
\begin{equation}
    P(\text{honest filtered}) \leq \frac{d \cdot \mathrm{Var}[|\Delta|]}{4 \sigma_h^2}.
\end{equation}
For well-clustered libraries with $\mathrm{Var}[|\Delta|] \ll \sigma_h^2$,
this is negligible.
\end{proposition}

\begin{proof}
An honest experience $\hat{\bm{e}}_m$ is filtered when its mean normalized
deviation exceeds $\tau = 2$. By Markov's inequality:
$P(\bar{D}_m > 2) \leq \bar{D}_m / 2 \leq d \cdot \mathrm{Var}[|\Delta|] / (4 \sigma_h^2)$,
where we used $\mathbb{E}[\bar{D}_m] = d \cdot \mathrm{Var}[|\Delta|] / \sigma_h^2$
(the mean deviation of honest values from the coordinate-wise median is
proportional to the variance of $|\Delta|$ relative to $\sigma_h$).
\end{proof}

\section{Bandwidth Analysis}
\label{app:bandwidth}

The per-node bandwidth consumption per round is:

\paragraph{Outgoing bandwidth:}
\begin{equation}
    B_{\text{out}} = k_{\text{peers}} \cdot |B_{\text{compressed}}|
    = k_{\text{peers}} \cdot M \cdot \left(\frac{d}{8} + \frac{d}{g} \cdot 4 + 64\right) \; \text{bytes}.
\end{equation}
For $k_{\text{peers}} = 8$, $M = 100$, $d = 768$, $g = 128$:
$B_{\text{out}} = 8 \times 29{,}472 \approx 236\,\text{KB/round}$.

\paragraph{Incoming bandwidth:}
Each node receives batches from $k_{\text{peers}}$ peers, so
$B_{\text{in}} \approx B_{\text{out}}$.

\paragraph{Gossip propagation:}
A single experience reaches all $N$ nodes within
$T_{\text{prop}} = O(\log N / \log k_{\text{peers}})$ rounds~\citep{demers1987epidemic}.
For $N = 100$, $k = 8$: $T_{\text{prop}} \approx 2.2$ rounds.

\section{Reproducibility Details}
\label{app:reproducibility}

\begin{itemize}[leftmargin=1.5em]
    \item \textbf{Environment:} Python 3.11, LanceDB 0.3.0, sentence-transformers
          1.2.0 (all-MiniLM-L6-v2, $d=384$; we use $d=768$ in the paper via
          all-mpnet-base-v2).
    \item \textbf{Network simulation:} Discrete-event simulator with configurable
          latency (uniform 10--100\,ms), packet loss ($p=0.01$), and node failures.
    \item \textbf{Byzantine behavior:} Random noise ($\bm{s}_{i,m}^{(j)} \sim
          \text{Unif}(\{-1,+1\}^g)$, $\alpha_{i,m}^{(j)} \sim \text{Unif}(0, 1)$).
    \item \textbf{GRPO setup:} Group size $G = 4$, max iterations $K = 3$,
          reward weights $\alpha = 0.7$, $\beta = 0.3$.
    \item \textbf{Seeds:} Fixed seeds 42, 123, 456 for three independent runs;
          all tables report mean $\pm$ std ($\leq 0.4\%$ in all cases).
    \item \textbf{Hardware:} Simulated on a single 16-core x86 machine;
          no GPU required for simulation.
\end{itemize}

% ============================================================================
% REFERENCES
% ============================================================================
\bibliographystyle{plainnat}
\begin{thebibliography}{40}

\bibitem[Baker et~al.(2020)]{baker2020emergent}
Baker, B., Kanitscheider, I., Marber, T., Wu, Y., Powell, G., McGrew, B.,
  and Mordatch, I.
\newblock Emergent tool use from multi-agent autocurricula.
\newblock \emph{ICLR}, 2020.

\bibitem[Bernstein et~al.(2018)]{bernstein2018signsgd}
Bernstein, J., Wang, Y.-X., Azizzadenesheli, K., and Anandkumar, A.
\newblock signSGD: Compressed optimisation for non-convex problems.
\newblock \emph{ICML}, 2018.

\bibitem[Bittensor(2023)]{bittensor2023}
Bittensor.
\newblock Bittensor: A peer-to-peer intelligence market.
\newblock Whitepaper, 2023.

\bibitem[Blanchard et~al.(2017)]{blanchard2017machine}
Blanchard, P., El~Mhamdi, E.~M., Guerraoui, R., and Stainer, J.
\newblock Machine learning with adversaries: Byzantine tolerant gradient
  descent.
\newblock \emph{NeurIPS}, 2017.

\bibitem[Bonawitz et~al.(2017)]{bonawitz2017practical}
Bonawitz, K., Ivanov, V., Kreuter, B., Marcedone, A., McMahan, H.~B.,
  Patel, S., Ramage, D., Segal, A., and Seth, K.
\newblock Practical secure aggregation for privacy-preserving machine learning.
\newblock \emph{ACM CCS}, 2017.

\bibitem[Chen et~al.(2018)]{chen2018draco}
Chen, L., Wang, H., Charles, Z., and Papailiopoulos, D.
\newblock DRACO: Byzantine-resilient distributed training via redundant
  gradients.
\newblock \emph{ICML}, 2018.

\bibitem[Christianos et~al.(2021)]{christianos2021scaling}
Christianos, F., Papoudakis, G., Rahman, M.~A., and Albrecht, S.~V.
\newblock Scaling multi-agent reinforcement learning with selective parameter
  sharing.
\newblock \emph{ICML}, 2021.

\bibitem[Conneau et~al.(2020)]{conneau2020unsupervised}
Conneau, A., Rinott, R., Lample, G., Williams, A., Bowman, S., Schwenk, H.,
  and Stoyanov, V.
\newblock XNLI: Evaluating cross-lingual sentence representations.
\newblock \emph{EMNLP}, 2020.

\bibitem[Demers et~al.(1987)]{demers1987epidemic}
Demers, A., Greene, D., Hauser, C., Irish, W., Larson, J., Shenker, S.,
  Sturgis, H., Swinehart, D., and Terry, D.
\newblock Epidemic algorithms for replicated database maintenance.
\newblock \emph{PODC}, 1987.

\bibitem[Dwork \& Roth(2014)]{dwork2014algorithmic}
Dwork, C. and Roth, A.
\newblock The algorithmic foundations of differential privacy.
\newblock \emph{Foundations and Trends in Theoretical Computer Science},
  9(3--4):211--407, 2014.

\bibitem[Furlanello et~al.(2018)]{furlanello2018born}
Furlanello, T., Lipton, Z.~C., Tschannen, M., Itti, L., and Anandkumar, A.
\newblock Born again neural networks.
\newblock \emph{ICML}, 2018.

\bibitem[Gensyn(2023)]{gensyn2023}
Gensyn.
\newblock Gensyn: Protocol for decentralized ML computation.
\newblock Whitepaper, 2023.

\bibitem[Hanzo AI Research(2026a)]{hanzo2026aso}
Hanzo AI Research.
\newblock Active Semantic Optimization: Training-free adaptation via Bayesian
  Product-of-Experts decoding.
\newblock Technical report, Hanzo AI, 2026.

\bibitem[Hanzo AI Research(2026b)]{hanzo2026dso}
Hanzo AI Research.
\newblock Decentralized Semantic Optimization: Byzantine-robust prior
  aggregation for collective model adaptation.
\newblock Technical report, Hanzo AI (ZIP-001/400), 2026.

\bibitem[Hanzo AI Research(2026c)]{hanzo2026bitdelta}
Hanzo AI Research.
\newblock BitDelta: 1-bit quantization of experience embedding deltas for
  efficient fleet communication.
\newblock Technical report, Hanzo AI (ZIP-007), 2026.

\bibitem[Hanzo AI Research(2026d)]{hanzo2026poai}
Hanzo AI Research.
\newblock Proof of AI: Verifiable attestation for agent trajectory extraction.
\newblock Technical report, Hanzo AI (ZIP-002), 2026.

\bibitem[Hinton(2002)]{hinton2002training}
Hinton, G.~E.
\newblock Training products of experts by minimizing contrastive divergence.
\newblock \emph{Neural Computation}, 14(8):1771--1800, 2002.

\bibitem[Hinton et~al.(2015)]{hinton2015distilling}
Hinton, G., Vinyals, O., and Dean, J.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{NeurIPS Workshop}, 2015.

\bibitem[Jimenez et~al.(2024)]{jimenez2024swebench}
Jim{\'e}nez, C.~E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O.,
  and Narasimhan, K.
\newblock SWE-bench: Can language models resolve real-world {GitHub} issues?
\newblock \emph{ICLR}, 2024.

\bibitem[Koloskova et~al.(2020)]{koloskova2020unified}
Koloskova, A., Stich, S.~U., and Jaggi, M.
\newblock Decentralized stochastic optimization and gossip algorithms with
  compressed communication.
\newblock \emph{ICML}, 2020.

\bibitem[Lamport et~al.(1982)]{lamport1982byzantine}
Lamport, L., Shostak, R., and Pease, M.
\newblock The Byzantine generals problem.
\newblock \emph{ACM TOPLAS}, 4(3):382--401, 1982.

\bibitem[LanceDB(2024)]{lancedb2024}
LanceDB.
\newblock LanceDB: Serverless vector database for AI applications.
\newblock \url{https://lancedb.com}, 2024.

\bibitem[Li et~al.(2020)]{li2020federated}
Li, T., Sahu, A.~K., Zaheer, M., Sanjabi, M., Talwalkar, A., and
  Smith, V.
\newblock Federated optimization in heterogeneous networks.
\newblock \emph{MLSys}, 2020.

\bibitem[Liu et~al.(2024)]{liu2024bitdelta}
Liu, J., Kulikov, R., Fox, Z., Nikdan, A., Kim, Y., Papailiopoulos, D.,
  and De~Sa, C.
\newblock BitDelta: Your fine-tune may only be worth one bit.
\newblock \emph{arXiv preprint arXiv:2402.10193}, 2024.

\bibitem[McMahan et~al.(2017)]{mcmahan2017communication}
McMahan, H.~B., Moore, E., Ramage, D., Hampson, S., and
  Ag{\"u}era~y~Arcas, B.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock \emph{AISTATS}, 2017.

\bibitem[McMahan et~al.(2018)]{mcmahan2018learning}
McMahan, H.~B., Ramage, D., Talwar, K., and Zhang, L.
\newblock Learning differentially private recurrent language models.
\newblock \emph{ICLR}, 2018.

\bibitem[El~Mhamdi et~al.(2018)]{mhamdi2018hidden}
El~Mhamdi, E.~M., Guerraoui, R., and Rouault, S.
\newblock The hidden vulnerability of distributed learning in Byzantium.
\newblock \emph{ICML}, 2018.

\bibitem[Minsker(2015)]{minsker2015geometric}
Minsker, S.
\newblock Geometric median and robust estimation in Banach spaces.
\newblock \emph{Bernoulli}, 21(4):2308--2335, 2015.

\bibitem[Pease et~al.(1980)]{pease1980reaching}
Pease, M., Shostak, R., and Lamport, L.
\newblock Reaching agreement in the presence of faults.
\newblock \emph{Journal of the ACM}, 27(2):228--234, 1980.

\bibitem[Seide et~al.(2014)]{seide20141bit}
Seide, F., Fu, H., Droppo, J., Li, G., and Yu, D.
\newblock 1-bit stochastic gradient descent and its application to
  data-parallel distributed training of speech DNNs.
\newblock \emph{Interspeech}, 2014.

\bibitem[Shao et~al.(2024)]{shao2024deepseekmath}
Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M.,
  Li, Y.~K., Wu, Y., and Guo, D.
\newblock DeepSeekMath: Pushing the limits of mathematical reasoning in open
  language models.
\newblock \emph{arXiv preprint arXiv:2402.03300}, 2024.

\bibitem[Stich et~al.(2018)]{stich2018sparsified}
Stich, S.~U., Cordonnier, J.-B., and Jaggi, M.
\newblock Sparsified SGD with memory.
\newblock \emph{NeurIPS}, 2018.

\bibitem[Warnat-Herresthal et~al.(2021)]{warnat2021swarm}
Warnat-Herresthal, S., Schultze, H., Shastry, K.~L., et~al.
\newblock Swarm learning for decentralized and confidential clinical machine
  learning.
\newblock \emph{Nature}, 594:265--270, 2021.

\bibitem[Yin et~al.(2018)]{yin2018byzantine}
Yin, D., Chen, Y., Kannan, R., and Bartlett, P.
\newblock Byzantine-robust distributed learning: Towards optimal statistical
  rates.
\newblock \emph{ICML}, 2018.

\bibitem[Zhang et~al.(2024)]{zhang2024fedlora}
Zhang, J., Hua, Y., Wang, H., Song, T., Xue, Z., Ma, R., and Guan, H.
\newblock FedLoRA: When personalized federated learning meets low-rank
  adaptation.
\newblock \emph{arXiv preprint arXiv:2307.06768}, 2024.

\end{thebibliography}

\end{document}
