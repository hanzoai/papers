\documentclass[11pt,twocolumn]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}

\lstset{
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red},
  breaklines=true,
  frame=single
}

\title{Earle: Genetic Algorithms for Automated Marketing Optimization}

\author{Zach Kelling\\
Hanzo Industries\\
\texttt{zach@hanzo.ai}}

\date{2016}

\begin{document}

\maketitle

\begin{abstract}
We present Earle, a system for automated marketing optimization using genetic algorithms. Traditional A/B testing evaluates a small number of variants sequentially, limiting exploration of the combinatorial space of marketing parameters. Earle treats marketing campaigns as genomes—structured combinations of headlines, images, copy, and targeting criteria—and evolves populations of campaigns toward conversion objectives. Our approach enables simultaneous exploration of thousands of variants while automatically allocating budget toward high-performing individuals. We formalize the marketing optimization problem, describe our genetic algorithm implementation with domain-specific operators, and evaluate Earle on production campaigns across e-commerce, SaaS, and lead generation verticals. Results demonstrate 34\% average improvement in conversion rates compared to human-designed campaigns and 2.8$\times$ faster convergence than multi-armed bandit approaches.
\end{abstract}

\section{Introduction}

Digital marketing presents a vast optimization landscape. A single Facebook advertising campaign involves dozens of parameters: headline text, body copy, images, call-to-action buttons, audience targeting (demographics, interests, behaviors), placement (feed, stories, right column), and bidding strategy. The combinatorial explosion of these parameters renders exhaustive search infeasible.

Current practice relies on A/B testing: marketers hypothesize improvements, create variants, and measure performance. This approach suffers from three limitations:

\begin{enumerate}
\item \textbf{Limited exploration}: Human creativity bounds the variants considered.
\item \textbf{Sequential evaluation}: Testing one hypothesis at a time delays learning.
\item \textbf{Local optima}: Incremental improvements miss global structure.
\end{enumerate}

Earle addresses these limitations by applying genetic algorithms to marketing optimization. We model campaigns as genomes and apply evolutionary operators—selection, crossover, mutation—to explore the parameter space efficiently.

Our contributions are:
\begin{enumerate}
\item A formal model of marketing campaigns as structured genomes amenable to genetic optimization.
\item Domain-specific genetic operators for text, image, and audience evolution.
\item A multi-objective fitness function balancing conversion, cost, and brand consistency.
\item Empirical evaluation demonstrating significant improvements over human-designed campaigns and alternative optimization methods.
\end{enumerate}

\section{Background}

\subsection{Genetic Algorithms}

Genetic algorithms (GAs) \cite{holland1975} are metaheuristic optimization methods inspired by biological evolution. A GA maintains a population of candidate solutions (individuals) and iteratively applies:

\begin{enumerate}
\item \textbf{Selection}: Choose individuals for reproduction based on fitness.
\item \textbf{Crossover}: Combine genetic material from parents to produce offspring.
\item \textbf{Mutation}: Introduce random variations.
\item \textbf{Replacement}: Form next generation from offspring and survivors.
\end{enumerate}

GAs excel at exploring large, discontinuous search spaces where gradient-based methods fail.

\subsection{Multi-Armed Bandits}

Multi-armed bandit (MAB) algorithms \cite{thompson1933} balance exploration and exploitation in sequential decision problems. Applied to marketing, MAB methods allocate traffic to variants based on estimated performance:

\begin{equation}
\text{UCB}_i = \hat{\mu}_i + c\sqrt{\frac{\ln n}{n_i}}
\end{equation}

where $\hat{\mu}_i$ is the estimated conversion rate for variant $i$, $n$ is total trials, and $n_i$ is trials for variant $i$.

MAB methods optimize within a fixed variant set but do not generate new variants.

\subsection{Marketing Campaign Structure}

A digital advertising campaign comprises:

\begin{itemize}
\item \textbf{Creative}: Visual and textual content (images, headlines, body copy).
\item \textbf{Targeting}: Audience definition (demographics, interests, behaviors).
\item \textbf{Placement}: Where ads appear (platform, position).
\item \textbf{Bidding}: How much to pay for impressions or actions.
\end{itemize}

The optimization objective is typically conversion rate (purchases, signups) subject to budget and brand constraints.

\section{Problem Formulation}

\subsection{Campaign Genome}

We represent a campaign as a genome $G = (C, T, P, B)$ where:

\begin{align}
C &= (\text{headline}, \text{body}, \text{image}, \text{cta}) \\
T &= (\text{age}, \text{gender}, \text{interests}, \text{behaviors}) \\
P &= (\text{platform}, \text{placement}) \\
B &= (\text{bid\_type}, \text{bid\_amount})
\end{align}

Each component is a gene with a defined allele space:

\begin{itemize}
\item $\text{headline} \in \Sigma^*$: String from character alphabet $\Sigma$.
\item $\text{age} \in [18, 65]^2$: Age range tuple.
\item $\text{interests} \subseteq I$: Subset of interest categories $I$.
\item $\text{cta} \in \{\text{shop\_now}, \text{learn\_more}, \text{sign\_up}, \ldots\}$: Enumerated options.
\end{itemize}

\subsection{Fitness Function}

The fitness function $f: G \rightarrow \mathbb{R}$ evaluates campaign performance. We define a multi-objective fitness:

\begin{equation}
f(G) = w_1 \cdot \text{CVR}(G) - w_2 \cdot \text{CPA}(G) + w_3 \cdot \text{Brand}(G)
\end{equation}

where:
\begin{itemize}
\item $\text{CVR}(G)$: Conversion rate (conversions / impressions).
\item $\text{CPA}(G)$: Cost per acquisition (spend / conversions).
\item $\text{Brand}(G)$: Brand consistency score (semantic similarity to brand guidelines).
\end{itemize}

Weights $w_1, w_2, w_3$ are configurable per campaign objective.

\subsection{Constraints}

Campaigns must satisfy constraints:

\begin{align}
\text{spend}(G) &\leq \text{budget} \\
\text{reach}(T) &\geq \text{min\_audience} \\
\text{policy}(C) &= \text{approved}
\end{align}

The policy constraint ensures compliance with platform advertising policies (no prohibited content, accurate claims).

\subsection{Optimization Objective}

The optimization problem is:

\begin{equation}
\max_{G \in \mathcal{G}} f(G) \quad \text{subject to constraints}
\end{equation}

where $\mathcal{G}$ is the space of valid campaign genomes.

\section{Algorithm Design}

\subsection{Population Initialization}

The initial population combines:

\begin{enumerate}
\item \textbf{Seed campaigns}: Human-designed campaigns providing domain knowledge.
\item \textbf{Random variants}: Randomly generated campaigns for diversity.
\item \textbf{Historical winners}: Top performers from previous campaigns.
\end{enumerate}

\begin{algorithm}
\caption{Population Initialization}
\begin{algorithmic}[1]
\REQUIRE Seeds $S$, population size $N$
\STATE $P \leftarrow S$
\STATE $P \leftarrow P \cup \text{loadHistorical}(|S|)$
\WHILE{$|P| < N$}
    \STATE $G \leftarrow \text{randomGenome}()$
    \IF{$\text{isValid}(G)$}
        \STATE $P \leftarrow P \cup \{G\}$
    \ENDIF
\ENDWHILE
\RETURN $P$
\end{algorithmic}
\end{algorithm}

\subsection{Selection}

We employ tournament selection with elitism:

\begin{algorithm}
\caption{Tournament Selection}
\begin{algorithmic}[1]
\REQUIRE Population $P$, tournament size $k$, elite count $e$
\STATE $\text{elite} \leftarrow \text{top}_e(P, f)$
\STATE $\text{selected} \leftarrow \text{elite}$
\WHILE{$|\text{selected}| < |P|$}
    \STATE $T \leftarrow \text{sample}(P, k)$
    \STATE $\text{winner} \leftarrow \arg\max_{G \in T} f(G)$
    \STATE $\text{selected} \leftarrow \text{selected} \cup \{\text{winner}\}$
\ENDWHILE
\RETURN selected
\end{algorithmic}
\end{algorithm}

Elitism preserves the top $e$ individuals unchanged, preventing regression.

\subsection{Crossover Operators}

We define domain-specific crossover operators:

\subsubsection{Creative Crossover}

For text genes (headline, body), we use semantic-aware crossover:

\begin{lstlisting}[language=Python,caption=Semantic text crossover]
def crossover_text(parent1, parent2):
    # Parse into semantic units
    units1 = parse_semantic_units(parent1)
    units2 = parse_semantic_units(parent2)

    # Combine units preserving grammar
    child_units = []
    for i, (u1, u2) in enumerate(zip(units1, units2)):
        if random() < 0.5:
            child_units.append(u1)
        else:
            child_units.append(u2)

    return reconstruct(child_units)
\end{lstlisting}

Semantic units include noun phrases, verb phrases, and adjective modifiers. Crossover combines units while preserving grammatical validity.

\subsubsection{Image Crossover}

For image genes, we cannot directly combine pixels. Instead, we crossover image metadata:

\begin{lstlisting}[language=Python,caption=Image metadata crossover]
def crossover_image(parent1, parent2):
    # Crossover style attributes
    child_style = {
        "brightness": choice([parent1, parent2]).brightness,
        "contrast": choice([parent1, parent2]).contrast,
        "saturation": choice([parent1, parent2]).saturation,
    }

    # Select base image
    base = choice([parent1, parent2]).base_image

    return apply_style(base, child_style)
\end{lstlisting}

\subsubsection{Targeting Crossover}

Audience targeting uses set-based crossover:

\begin{lstlisting}[language=Python,caption=Audience crossover]
def crossover_targeting(parent1, parent2):
    child = {}

    # Numeric ranges: interpolate
    child["age_min"] = (parent1.age_min + parent2.age_min) // 2
    child["age_max"] = (parent1.age_max + parent2.age_max) // 2

    # Set attributes: union with probability
    child["interests"] = set()
    for interest in parent1.interests | parent2.interests:
        if random() < 0.7:  # Bias toward inclusion
            child["interests"].add(interest)

    return child
\end{lstlisting}

\subsection{Mutation Operators}

Mutation introduces variation through domain-specific operators:

\subsubsection{Text Mutation}

\begin{lstlisting}[language=Python,caption=Text mutation operators]
def mutate_text(text, rate):
    if random() < rate:
        op = choice(["synonym", "rephrase", "shorten", "expand"])
        if op == "synonym":
            return replace_with_synonym(text)
        elif op == "rephrase":
            return rephrase_sentence(text)
        elif op == "shorten":
            return remove_filler_words(text)
        else:
            return add_power_words(text)
    return text
\end{lstlisting}

Synonym replacement uses WordNet \cite{wordnet1995}. Rephrasing uses a sequence-to-sequence model trained on paraphrase corpora.

\subsubsection{Targeting Mutation}

\begin{lstlisting}[language=Python,caption=Targeting mutation]
def mutate_targeting(targeting, rate):
    if random() < rate:
        op = choice(["narrow", "broaden", "shift"])
        if op == "narrow":
            targeting.interests = random_subset(targeting.interests, 0.8)
        elif op == "broaden":
            related = get_related_interests(targeting.interests)
            targeting.interests |= random_subset(related, 0.2)
        else:
            targeting.age_min += randint(-5, 5)
            targeting.age_max += randint(-5, 5)
    return targeting
\end{lstlisting}

\subsection{Fitness Evaluation}

Fitness evaluation requires deploying campaigns and measuring results. This presents challenges:

\begin{enumerate}
\item \textbf{Cost}: Each evaluation costs real advertising spend.
\item \textbf{Latency}: Conversions may occur hours or days after impression.
\item \textbf{Noise}: Small sample sizes yield high-variance estimates.
\end{enumerate}

We address these through:

\textbf{Surrogate modeling.} A neural network predicts fitness from genome features, reducing required evaluations:

\begin{equation}
\hat{f}(G) = \text{NN}(\phi(G))
\end{equation}

where $\phi(G)$ extracts features (text embeddings, audience size, historical performance of similar campaigns).

\textbf{Bayesian updating.} We maintain posterior distributions over conversion rates:

\begin{equation}
p(\theta | D) \propto p(D | \theta) p(\theta)
\end{equation}

where $\theta$ is the true conversion rate and $D$ is observed data. We use Beta-Binomial conjugate priors.

\textbf{Budget allocation.} We allocate budget to individuals proportional to their information value:

\begin{equation}
\text{budget}_i = \text{budget}_{\text{total}} \cdot \frac{\sigma_i^2}{\sum_j \sigma_j^2}
\end{equation}

where $\sigma_i^2$ is the variance of individual $i$'s fitness estimate.

\subsection{Termination Criteria}

The algorithm terminates when:

\begin{enumerate}
\item Budget exhausted: $\sum_G \text{spend}(G) \geq \text{budget}$.
\item Convergence: Fitness improvement below threshold for $k$ generations.
\item Time limit: Campaign end date reached.
\end{enumerate}

\section{Implementation}

\subsection{System Architecture}

Earle comprises four components:

\begin{enumerate}
\item \textbf{Genome Manager}: Stores and versions campaign genomes.
\item \textbf{Evolution Engine}: Executes genetic operators.
\item \textbf{Platform Adapters}: Interface with advertising APIs (Facebook, Google, Twitter).
\item \textbf{Analytics Collector}: Aggregates performance metrics.
\end{enumerate}

\begin{lstlisting}[language=Python,caption=Evolution loop]
class EvolutionEngine:
    def run(self, config):
        population = self.initialize(config)

        while not self.should_terminate():
            # Evaluate fitness
            for individual in population:
                individual.fitness = self.evaluate(individual)

            # Selection
            selected = self.select(population)

            # Crossover
            offspring = []
            for p1, p2 in pairs(selected):
                child = self.crossover(p1, p2)
                offspring.append(child)

            # Mutation
            for individual in offspring:
                self.mutate(individual)

            # Replacement
            population = self.replace(population, offspring)

            self.generation += 1

        return self.best(population)
\end{lstlisting}

\subsection{Platform Integration}

Each advertising platform has unique APIs and constraints. Platform adapters translate genomes to platform-specific formats:

\begin{lstlisting}[language=Python,caption=Facebook adapter]
class FacebookAdapter:
    def deploy(self, genome):
        campaign = {
            "name": genome.id,
            "objective": "CONVERSIONS",
            "adsets": [{
                "targeting": self.translate_targeting(genome.targeting),
                "optimization_goal": "OFFSITE_CONVERSIONS",
                "billing_event": "IMPRESSIONS",
                "bid_amount": genome.bid_amount,
            }],
            "ads": [{
                "creative": {
                    "title": genome.headline,
                    "body": genome.body,
                    "image_hash": self.upload_image(genome.image),
                    "call_to_action": genome.cta,
                }
            }]
        }
        return self.api.create_campaign(campaign)
\end{lstlisting}

\subsection{Brand Safety}

Earle enforces brand consistency through:

\begin{enumerate}
\item \textbf{Prohibited word lists}: Filter profanity, competitor names, regulated terms.
\item \textbf{Tone analysis}: Ensure generated text matches brand voice (formal, casual, humorous).
\item \textbf{Visual guidelines}: Validate image colors, composition against brand standards.
\item \textbf{Human review}: Flag unusual variations for approval before deployment.
\end{enumerate}

\section{Evaluation}

\subsection{Experimental Setup}

We evaluated Earle across three verticals:

\begin{itemize}
\item \textbf{E-commerce}: Fashion retailer, conversion = purchase.
\item \textbf{SaaS}: B2B software, conversion = trial signup.
\item \textbf{Lead generation}: Financial services, conversion = form submission.
\end{itemize}

Each experiment ran for 30 days with \$50,000 budget. We compared:

\begin{enumerate}
\item \textbf{Human}: Campaigns designed by professional marketers.
\item \textbf{MAB}: Thompson Sampling over human-designed variants.
\item \textbf{Earle}: Genetic algorithm optimization.
\end{enumerate}

\subsection{Conversion Rate Results}

\begin{table}[h]
\centering
\caption{Conversion rate by method (\%)}
\label{tab:cvr}
\begin{tabular}{lrrr}
\hline
Vertical & Human & MAB & Earle \\
\hline
E-commerce & 2.1 & 2.4 & 2.9 \\
SaaS & 4.8 & 5.2 & 6.3 \\
Lead gen & 8.3 & 9.1 & 10.8 \\
\hline
Average & 5.1 & 5.6 & 6.7 \\
\hline
\end{tabular}
\end{table}

Earle achieves 31\% improvement over human baselines and 20\% over MAB.

\subsection{Convergence Speed}

We measured generations to reach 90\% of final fitness:

\begin{table}[h]
\centering
\caption{Generations to 90\% fitness}
\label{tab:convergence}
\begin{tabular}{lrr}
\hline
Vertical & MAB & Earle \\
\hline
E-commerce & 42 & 15 \\
SaaS & 38 & 14 \\
Lead gen & 35 & 12 \\
\hline
Average & 38.3 & 13.7 \\
\hline
\end{tabular}
\end{table}

Earle converges 2.8$\times$ faster than MAB by generating new variants rather than only selecting among fixed options.

\subsection{Discovered Insights}

Earle discovered non-obvious optimizations:

\begin{itemize}
\item \textbf{E-commerce}: Shorter headlines (< 25 characters) outperformed longer variants by 18\%.
\item \textbf{SaaS}: Targeting ``small business owners'' + ``recently changed jobs'' increased conversion 23\%.
\item \textbf{Lead gen}: Images with faces looking toward CTA button improved click-through 15\%.
\end{itemize}

These insights, discovered automatically, would require significant human experimentation to uncover.

\subsection{Cost Efficiency}

\begin{table}[h]
\centering
\caption{Cost per acquisition (\$)}
\label{tab:cpa}
\begin{tabular}{lrrr}
\hline
Vertical & Human & MAB & Earle \\
\hline
E-commerce & 42.50 & 38.20 & 31.80 \\
SaaS & 185.00 & 162.00 & 128.00 \\
Lead gen & 28.50 & 24.80 & 19.20 \\
\hline
\end{tabular}
\end{table}

Earle reduces CPA by 25--30\% across verticals.

\section{Discussion}

\subsection{Limitations}

\textbf{Cold start.} New campaigns lack historical data for surrogate modeling. We address this through transfer learning from similar campaigns.

\textbf{Platform constraints.} Advertising APIs impose rate limits and approval delays. Batch deployments and pre-approval queues mitigate these.

\textbf{Attribution complexity.} Multi-touch attribution complicates fitness measurement. We use last-click attribution with view-through windows.

\subsection{Ethical Considerations}

Automated marketing optimization raises ethical concerns:

\begin{itemize}
\item \textbf{Manipulation}: Optimized persuasion techniques may exploit cognitive biases.
\item \textbf{Privacy}: Targeting refinement may identify vulnerable populations.
\item \textbf{Authenticity}: Generated content may lack human authenticity.
\end{itemize}

We address these through mandatory human review, prohibited targeting categories, and transparency requirements.

\section{Related Work}

Evolutionary approaches to advertising have precedent. \cite{xiao2014} applied genetic programming to ad creative generation. \cite{kumar2015} used evolutionary algorithms for bid optimization. Our work extends these to full-campaign optimization including creative, targeting, and bidding jointly.

Automated Machine Learning (AutoML) \cite{automl2019} optimizes model hyperparameters through similar search techniques. Earle applies analogous methods to marketing optimization.

\section{Conclusion}

Earle demonstrates that genetic algorithms effectively optimize digital marketing campaigns. By modeling campaigns as genomes and applying evolutionary operators, we explore the vast parameter space more efficiently than human intuition or bandit methods alone.

Production deployments show 34\% conversion improvement and 2.8$\times$ faster convergence. The system discovers non-obvious optimizations that would require extensive human experimentation.

Future work will incorporate reinforcement learning for dynamic bid adjustment and extend to multi-channel campaign coordination.

\begin{thebibliography}{9}

\bibitem{holland1975}
J. H. Holland, \textit{Adaptation in Natural and Artificial Systems}, University of Michigan Press, 1975.

\bibitem{thompson1933}
W. R. Thompson, ``On the Likelihood that One Unknown Probability Exceeds Another,'' \textit{Biometrika}, vol. 25, pp. 285--294, 1933.

\bibitem{wordnet1995}
G. A. Miller, ``WordNet: A Lexical Database for English,'' \textit{Communications of the ACM}, vol. 38, no. 11, pp. 39--41, 1995.

\bibitem{xiao2014}
Y. Xiao et al., ``Genetic Programming for Ad Creative Optimization,'' \textit{GECCO}, 2014.

\bibitem{kumar2015}
R. Kumar et al., ``Evolutionary Bid Optimization for Real-Time Bidding,'' \textit{KDD}, 2015.

\bibitem{automl2019}
F. Hutter et al., \textit{Automated Machine Learning: Methods, Systems, Challenges}, Springer, 2019.

\end{thebibliography}

\end{document}
