% Product-of-Experts Decoding section
\section{Product-of-Experts (PoE) Decoding}
\subsection{Decode-Time PoE Formulation}
For a base model with conditional \(p_\theta(y\mid x)\) and a set of token-local expert factors \(\{\phi_i(y\mid x)\}\), we define the combined distribution via:
\begin{equation}\label{eq:poe-main}
\log \pi_*(y\mid x) = \eta_0 \log \pi_0(y\mid x) + \sum_{i=1}^K \eta_i \log \phi_i(y\mid x) - \log Z,
\end{equation}
where \(\pi_0 = p_\theta\) is the base model, \(\{\phi_i\}\) are expert beliefs from experiential priors, \(\{\eta_i\}\) are precision weights, and \(Z\) is the partition function (computed locally per token).

\subsection{Token-Local Boosts}
Expert factors \(\phi_i\) provide token-level boosts based on:
\begin{itemize}[leftmargin=1.1em]
  \item \textbf{Unit-test feedback:} reward signals from test execution
  \item \textbf{Semantic advantages:} distilled from TF-GRPO rollouts
  \item \textbf{Historical experiences:} aggregated from distributed nodes
\end{itemize}

The final logits become:
\begin{equation}
\bm z = \log p_\theta(\cdot\mid x) + \sum_{i=1}^K \eta_i \log \phi_i(\cdot\mid x).
\end{equation}

\subsection{Bayesian Interpretation}
The PoE formulation corresponds to multiplying probability densities, equivalent to combining independent observations under a shared prior. This provides a principled framework for integrating diverse knowledge sources without parameter updates.
