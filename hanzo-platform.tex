% Hanzo Platform: PaaS for AI-Native Application Deployment
\documentclass[11pt,twocolumn]{article}
\usepackage[margin=0.85in]{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xcolor}
\usepackage{float}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{listings}
\usepackage{natbib}
\hypersetup{colorlinks=true,linkcolor=black,citecolor=blue,urlcolor=blue}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{proposition}[theorem]{Proposition}

\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  numbers=left,
  numberstyle=\tiny,
  xleftmargin=2em,
}

\title{Hanzo Platform: PaaS for AI-Native Application Deployment}
\author{
    Marcus Chen \quad
    David Wei \quad
    Zach Kelling \\
    \textit{Hanzo AI Research} \\
    \texttt{research@hanzo.ai}
}
\date{February 2026}

\begin{document}
\maketitle

\begin{abstract}
We present \textbf{Hanzo Platform}, a Platform-as-a-Service (PaaS) designed specifically for deploying AI-native applications---systems that incorporate LLM inference, vector databases, model serving, and GPU compute as first-class primitives. Traditional PaaS offerings (Heroku, Vercel, Railway) treat AI workloads as an afterthought, requiring manual configuration of GPU instances, model serving infrastructure, and inference APIs. Hanzo Platform introduces three innovations: (i) an \emph{AI-aware build system} that automatically detects model dependencies, optimizes container images for inference workloads, and configures GPU scheduling, reducing deployment time for AI applications from hours to minutes; (ii) a \emph{unified resource model} that treats LLM API access, vector database provisioning, GPU allocation, and traditional compute/storage as a single resource plane with declarative configuration; and (iii) a \emph{cost-optimized autoscaler} that dynamically scales inference endpoints based on token throughput, request latency, and cost budgets, achieving 38\% lower inference cost compared to static provisioning. We evaluate Hanzo Platform against five competing PaaS offerings on deployment time, operational cost, scaling responsiveness, and developer experience across 15 representative AI application architectures. Results demonstrate 4.7x faster deployment, 38\% lower cost, and 2.1x better scaling responsiveness. Production deployment data from 14 months of operation with 2,800+ applications and 47M inference requests validates the approach.
\end{abstract}

\section{Introduction}
\label{sec:intro}

The architecture of modern applications is undergoing a fundamental shift. AI-native applications---those that incorporate LLM inference, embedding computation, retrieval-augmented generation, and model serving as core components---have distinct infrastructure requirements that existing PaaS offerings do not address.

\subsection{The AI Infrastructure Gap}

Traditional PaaS platforms were designed for request-response web applications with predictable resource consumption. AI applications differ in several critical ways:

\begin{enumerate}[leftmargin=1.4em]
    \item \textbf{Heterogeneous compute}: AI workloads require GPUs for inference, CPUs for pre/post-processing, and specialized hardware (TPUs, Inferentia) for cost optimization. Traditional PaaS provides only CPU-based containers.
    \item \textbf{Model lifecycle}: Models must be downloaded (often multi-GB), loaded into GPU memory, warmed up, and version-managed. Traditional PaaS treats all application artifacts as stateless code bundles.
    \item \textbf{Token-based economics}: LLM inference is priced per-token rather than per-request, requiring fundamentally different cost modeling and optimization strategies.
    \item \textbf{Streaming responses}: LLM applications typically stream responses via Server-Sent Events or WebSockets, requiring persistent connections that conflict with traditional HTTP load balancers.
    \item \textbf{Vector storage}: RAG applications require vector databases co-located with inference for low-latency retrieval, a primitive not provided by traditional PaaS.
\end{enumerate}

\subsection{Contributions}

\begin{enumerate}[leftmargin=1.4em]
    \item An AI-aware build system with automatic model dependency detection and GPU-optimized container images (\S\ref{sec:build}).
    \item A unified resource model treating AI primitives as first-class platform resources (\S\ref{sec:resources}).
    \item A cost-optimized autoscaler for inference workloads (\S\ref{sec:autoscaler}).
    \item Comprehensive evaluation against competing platforms (\S\ref{sec:evaluation}).
    \item Production deployment analysis from 14 months of operation (\S\ref{sec:production}).
\end{enumerate}

\section{Architecture}
\label{sec:architecture}

\subsection{System Overview}

Hanzo Platform is built on Kubernetes with custom operators for AI workload management:

\begin{enumerate}[leftmargin=1.4em]
    \item \textbf{Control Plane}: API server, scheduler, build system, and resource manager.
    \item \textbf{Compute Plane}: Kubernetes clusters with GPU and CPU node pools.
    \item \textbf{Data Plane}: Managed PostgreSQL, Redis, vector databases, and object storage.
    \item \textbf{Network Plane}: Ingress controllers with WebSocket support, CDN, and DNS management.
    \item \textbf{Observability Plane}: Metrics, logging, tracing, and cost analytics.
\end{enumerate}

\subsection{Application Model}

\begin{definition}[Hanzo Application]
An application is a tuple $A = (S, R, C, E)$ where:
\begin{itemize}[leftmargin=1.1em]
    \item $S$: Source specification (Git repository, Docker image, or Nixpack).
    \item $R$: Resource requirements (compute, storage, AI services).
    \item $C$: Configuration (environment variables, secrets, domains).
    \item $E$: Scaling policy (min/max replicas, autoscaler parameters).
\end{itemize}
\end{definition}

Applications are defined declaratively in a \texttt{hanzo.yaml} file:

\begin{lstlisting}[language=yaml,caption={Example hanzo.yaml for an AI application.}]
name: my-rag-app
runtime: python-3.12

services:
  web:
    build: .
    port: 8000
    gpu: a10g
    replicas: {min: 1, max: 8}
    scaling:
      metric: tokens_per_second
      target: 1000

resources:
  llm:
    provider: hanzo-gateway
    models: [claude-3.5-sonnet, gpt-4o]
    budget: $500/month
  vector:
    engine: pgvector
    dimensions: 1024
    storage: 50GB
  db:
    engine: postgres
    storage: 20GB
  cache:
    engine: redis
    memory: 2GB
\end{lstlisting}

\section{AI-Aware Build System}
\label{sec:build}

\subsection{Model Dependency Detection}

The build system automatically scans application code for model dependencies:

\begin{algorithm}[H]
\caption{Model Dependency Detection}
\label{alg:detect}
\begin{algorithmic}[1]
\Require Source code $S$, known model registries $\mathcal{R}$
\State $\text{deps} \gets \text{ParseRequirements}(S)$ \Comment{pip, npm, cargo}
\State $\text{models} \gets \emptyset$
\State \Comment{Pattern 1: HuggingFace model references}
\State $\text{models} \gets \text{models} \cup \text{FindHFModels}(S)$
\State \Comment{Pattern 2: Model download URLs}
\State $\text{models} \gets \text{models} \cup \text{FindModelURLs}(S)$
\State \Comment{Pattern 3: Framework-specific configs}
\State $\text{models} \gets \text{models} \cup \text{FindFrameworkModels}(S)$
\State \Comment{Pattern 4: API client configurations}
\State $\text{apis} \gets \text{FindLLMAPIs}(S)$
\State \Comment{Determine GPU requirements}
\State $\text{gpu} \gets \text{EstimateGPU}(\text{models})$
\State \Return $(\text{models}, \text{apis}, \text{gpu})$
\end{algorithmic}
\end{algorithm}

\subsection{GPU-Optimized Container Images}

For applications requiring local model inference, the build system generates optimized container images:

\begin{enumerate}[leftmargin=1.4em]
    \item \textbf{Base image selection}: Choose from pre-built base images with CUDA, cuDNN, and framework-specific optimizations (PyTorch, TensorFlow, ONNX Runtime, vLLM).
    \item \textbf{Model caching}: Models are stored in a shared cache layer, avoiding redundant downloads across deployments. Cache hit rate: 87\%.
    \item \textbf{Quantization}: Automatically apply GPTQ or AWQ quantization for models that exceed available GPU memory, with quality verification.
    \item \textbf{Multi-stage builds}: Separate build dependencies from runtime, reducing final image size by 40--60\%.
\end{enumerate}

\begin{table}[H]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{App Type} & \textbf{Traditional} & \textbf{Hanzo} & \textbf{Speedup} \\
\midrule
API-only (LLM gateway) & 3.2 min & 1.1 min & 2.9x \\
RAG application & 8.7 min & 2.3 min & 3.8x \\
Local model serving & 42.1 min & 6.8 min & 6.2x \\
Full-stack AI app & 15.4 min & 3.2 min & 4.8x \\
Fine-tuning pipeline & 67.3 min & 12.1 min & 5.6x \\
\bottomrule
\end{tabular}
\caption{Build and deploy times: traditional PaaS vs. Hanzo Platform.}
\label{tab:build_times}
\end{table}

\subsection{Incremental Deployment}

Hanzo Platform supports zero-downtime deployments with AI-specific optimizations:

\begin{algorithm}[H]
\caption{AI-Aware Rolling Deployment}
\label{alg:deploy}
\begin{algorithmic}[1]
\Require New image $I'$, current pods $\mathcal{P}$, model warmup time $T_w$
\State \Comment{Phase 1: Pre-warm new pods}
\State $\mathcal{P}' \gets \text{StartPods}(I', |\mathcal{P}|)$
\For{each $p \in \mathcal{P}'$}
    \State Wait for model loading
    \State Run warmup inference requests
    \State Verify latency $\le$ SLA threshold
\EndFor
\State \Comment{Phase 2: Gradual traffic shift}
\For{$w = 0.1, 0.25, 0.5, 0.75, 1.0$}
    \State Route $w$ fraction of traffic to $\mathcal{P}'$
    \State Monitor error rate and latency for 60s
    \If{error rate $> 1\%$ or P99 latency $> 2\times$ baseline}
        \State Rollback: route all traffic to $\mathcal{P}$
        \State \Return failure
    \EndIf
\EndFor
\State \Comment{Phase 3: Cleanup}
\State Terminate old pods $\mathcal{P}$
\State \Return success
\end{algorithmic}
\end{algorithm}

The model pre-warming phase is critical: without it, cold starts for GPU inference can take 30--120 seconds, causing unacceptable latency spikes during deployment.

\section{Unified Resource Model}
\label{sec:resources}

\subsection{Resource Types}

Hanzo Platform provides seven first-class resource types:

\begin{table}[H]
\centering
\small
\begin{tabular}{llc}
\toprule
\textbf{Resource} & \textbf{Description} & \textbf{Provision Time} \\
\midrule
Compute (CPU) & Container instances & $<$ 30s \\
Compute (GPU) & GPU-accelerated instances & $<$ 120s \\
PostgreSQL & Managed relational DB & $<$ 60s \\
Redis & Managed cache/queue & $<$ 30s \\
Vector DB & pgvector or Qdrant & $<$ 60s \\
Object Storage & S3-compatible & $<$ 10s \\
LLM Gateway & Multi-provider LLM access & Instant \\
\bottomrule
\end{tabular}
\caption{Platform resource types and provisioning times.}
\label{tab:resources}
\end{table}

\subsection{Declarative Resource Binding}

Resources are bound to applications via environment variables and service mesh connections. The platform automatically injects connection strings, API keys, and service discovery information:

\begin{algorithm}[H]
\caption{Resource Binding}
\label{alg:binding}
\begin{algorithmic}[1]
\Require Application $A$, resource declarations $R$
\For{each resource $r \in R$}
    \State $\text{instance} \gets \text{Provision}(r.\text{type}, r.\text{config})$
    \State $\text{env} \gets \text{GenerateEnvVars}(\text{instance})$
    \State Inject $\text{env}$ into application containers
    \State Configure network policy: $A \leftrightarrow \text{instance}$
\EndFor
\State \Return bound application
\end{algorithmic}
\end{algorithm}

For example, declaring a \texttt{vector} resource automatically:
\begin{enumerate}[leftmargin=1.4em]
    \item Provisions a pgvector instance with the specified dimensions.
    \item Injects \texttt{VECTOR\_DB\_URL} into the application environment.
    \item Creates a Kubernetes NetworkPolicy allowing direct access.
    \item Configures backup and monitoring.
\end{enumerate}

\subsection{LLM Gateway Integration}

The LLM Gateway resource provides unified access to 100+ models via a single API endpoint:

\begin{itemize}[leftmargin=1.1em]
    \item \textbf{Automatic routing}: Requests are routed to the optimal provider based on model, cost, and latency.
    \item \textbf{Cost budgets}: Per-application monthly cost limits with alerts at 75\% and 90\%.
    \item \textbf{Key management}: API keys are stored in encrypted vaults and injected at runtime. The application never sees raw provider keys.
    \item \textbf{Usage analytics}: Per-model, per-request cost tracking with real-time dashboards.
\end{itemize}

\section{Cost-Optimized Autoscaler}
\label{sec:autoscaler}

\subsection{Problem Formulation}

Traditional autoscalers use CPU/memory utilization as scaling signals, which poorly correlate with AI workload performance. We formulate autoscaling as a constrained optimization problem:

\begin{align}
\min_{k(t)} \quad & \int_0^T c(k(t)) \, dt \label{eq:autoscale} \\
\text{s.t.} \quad & L_{P99}(k(t), \lambda(t)) \le L_{\max}, \nonumber \\
& \text{TPS}(k(t), \lambda(t)) \ge \text{TPS}_{\min}, \nonumber \\
& k_{\min} \le k(t) \le k_{\max}, \nonumber
\end{align}

where $k(t)$ is the number of replicas at time $t$, $c(k)$ is the cost function, $\lambda(t)$ is the request arrival rate, $L_{P99}$ is the P99 latency, $\text{TPS}$ is tokens per second throughput, and $L_{\max}$, $\text{TPS}_{\min}$ are SLA thresholds.

\subsection{AI-Specific Scaling Signals}

The autoscaler uses four AI-specific signals:

\begin{enumerate}[leftmargin=1.4em]
    \item \textbf{Token throughput}: Tokens generated per second across all replicas.
    \item \textbf{Queue depth}: Number of requests waiting for inference.
    \item \textbf{GPU utilization}: Percentage of GPU compute capacity in use.
    \item \textbf{KV cache pressure}: Fraction of KV cache memory occupied (for transformer inference).
\end{enumerate}

\begin{equation}
\text{Scale signal} = \alpha_1 \cdot \frac{\text{TPS}}{\text{TPS}_{\text{target}}} + \alpha_2 \cdot \frac{\text{Queue}}{\text{Queue}_{\max}} + \alpha_3 \cdot \text{GPU\%} + \alpha_4 \cdot \text{KV\%},
\end{equation}

where $\alpha_i$ are learned weights optimized to minimize cost subject to SLA constraints.

\subsection{Predictive Scaling}

We augment reactive scaling with a time-series forecasting model that predicts load 15 minutes ahead:

\begin{algorithm}[H]
\caption{Predictive Autoscaler}
\label{alg:autoscaler}
\begin{algorithmic}[1]
\Require Current replicas $k$, load history $\lambda_{t-W:t}$, SLA thresholds
\State \Comment{Reactive component}
\State $s_{\text{react}} \gets \text{ScaleSignal}(\text{current metrics})$
\State \Comment{Predictive component}
\State $\hat{\lambda}_{t+\Delta} \gets \text{LSTM.Predict}(\lambda_{t-W:t})$
\State $\hat{k}_{\text{pred}} \gets \text{CapacityModel}(\hat{\lambda}_{t+\Delta}, \text{SLA})$
\State \Comment{Combine with safety margin}
\State $k' \gets \max(k \cdot s_{\text{react}}, \hat{k}_{\text{pred}})$
\State $k' \gets \text{clamp}(k', k_{\min}, k_{\max})$
\State \Comment{Cost gate: prevent over-scaling}
\If{$\text{ProjectedCost}(k') > 1.5 \times \text{Budget}$}
    \State $k' \gets \text{CostConstrained}(k', \text{Budget})$
\EndIf
\State \Return $k'$
\end{algorithmic}
\end{algorithm}

\subsection{Spot Instance Integration}

For GPU workloads, the autoscaler integrates with spot/preemptible instance markets:

\begin{itemize}[leftmargin=1.1em]
    \item \textbf{Price monitoring}: Continuously monitor spot prices across regions and instance types.
    \item \textbf{Hybrid scaling}: Maintain a base of on-demand instances for SLA guarantees; scale burst capacity with spot instances.
    \item \textbf{Graceful preemption}: When spot instances are reclaimed, drain requests and redirect to on-demand capacity.
\end{itemize}

\begin{table}[H]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Strategy} & \textbf{Avg. Cost} & \textbf{P99 Lat.} & \textbf{Avail.} \\
\midrule
On-demand only & \$1.00 & 850ms & 99.99\% \\
Spot only & \$0.35 & 920ms & 97.2\% \\
Hybrid (Hanzo) & \$0.62 & 870ms & 99.95\% \\
\bottomrule
\end{tabular}
\caption{Cost-availability trade-off with spot instance integration.}
\label{tab:spot}
\end{table}

\subsection{Scaling Performance}

Comparison of autoscaler responsiveness:

\begin{table}[H]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Platform} & \textbf{Scale-up} & \textbf{Scale-down} & \textbf{Overprov.} \\
\midrule
Vercel & 12s & 300s & 41\% \\
Railway & 45s & 180s & 28\% \\
Render & 120s & 600s & 52\% \\
Fly.io & 8s & 120s & 23\% \\
\textbf{Hanzo} & \textbf{6s} (CPU) & \textbf{90s} & \textbf{14\%} \\
 & \textbf{45s} (GPU) & & \\
\bottomrule
\end{tabular}
\caption{Autoscaler responsiveness. Overprov. = average over-provisioning.}
\label{tab:scaling}
\end{table}

\section{Developer Experience}
\label{sec:dx}

\subsection{CLI Interface}

The Hanzo CLI provides a streamlined deployment workflow:

\begin{lstlisting}[language=bash,caption={Hanzo CLI deployment workflow.}]
# Initialize project
hanzo init

# Deploy (auto-detects framework)
hanzo deploy

# View logs
hanzo logs --service web --follow

# Scale manually
hanzo scale web --replicas 4

# View cost breakdown
hanzo cost --period 30d

# Manage secrets
hanzo secrets set API_KEY=sk-...

# Open dashboard
hanzo dashboard
\end{lstlisting}

\subsection{Git-Based Deployment}

Pushing to a connected Git repository triggers automatic deployment:

\begin{enumerate}[leftmargin=1.4em]
    \item Push triggers webhook.
    \item Build system detects changes, constructs optimized image.
    \item Preview deployment created for pull requests.
    \item Production deployment on merge to main branch.
    \item Automatic rollback if health checks fail within 5 minutes.
\end{enumerate}

\subsection{Preview Environments}

Every pull request automatically receives a preview environment with:
\begin{itemize}[leftmargin=1.1em]
    \item Unique URL (e.g., \texttt{pr-42.my-app.hanzo.dev}).
    \item Isolated database snapshot (copy-on-write from production).
    \item Shared LLM gateway access with sandboxed cost tracking.
    \item Automatic teardown on PR close.
\end{itemize}

\section{Security and Compliance}
\label{sec:security}

\subsection{Secret Management}

Secrets are managed via integration with Hanzo KMS (based on Infisical):

\begin{itemize}[leftmargin=1.1em]
    \item Secrets are encrypted at rest (AES-256-GCM) and in transit (TLS 1.3).
    \item Access is scoped per-application and per-environment (dev/staging/prod).
    \item Audit logging for all secret access.
    \item Automatic rotation for database credentials (every 30 days).
\end{itemize}

\subsection{Network Isolation}

Each application runs in a dedicated Kubernetes namespace with:
\begin{itemize}[leftmargin=1.1em]
    \item Network policies restricting inter-application communication.
    \item Egress filtering with allowlists for external API access.
    \item Service mesh (Istio) providing mTLS between services.
    \item WAF (Web Application Firewall) at the ingress layer.
\end{itemize}

\subsection{SOC 2 Compliance}

Hanzo Platform maintains SOC 2 Type II compliance through:
\begin{enumerate}[leftmargin=1.4em]
    \item Comprehensive audit logging (CloudTrail-equivalent).
    \item Automated vulnerability scanning (Trivy) on every build.
    \item Annual penetration testing by third-party auditors.
    \item Incident response procedures with 15-minute acknowledgment SLA.
\end{enumerate}

\section{Evaluation}
\label{sec:evaluation}

\subsection{Benchmark Applications}

We evaluate Hanzo Platform using 15 representative AI application architectures:

\begin{table}[H]
\centering
\small
\begin{tabular}{llc}
\toprule
\textbf{App Type} & \textbf{Components} & \textbf{GPU?} \\
\midrule
Chat API & LLM Gateway, Redis & No \\
RAG App & LLM, Vector DB, Web & No \\
Code Assistant & LLM, Git, Sandbox & No \\
Image Gen API & Diffusion model, S3 & Yes \\
Voice Agent & STT, LLM, TTS & Yes \\
Search Engine & Embedding, Index, LLM & Opt. \\
Recommendation & ML model, DB, API & Opt. \\
Content Moderation & Classifier, Queue & Yes \\
Document Processor & OCR, LLM, Storage & Opt. \\
Fine-tuning Service & Training, Eval, API & Yes \\
\bottomrule
\end{tabular}
\caption{Subset of 10/15 benchmark AI applications.}
\label{tab:apps}
\end{table}

\subsection{Deployment Time Comparison}

Time from \texttt{git push} to serving first request:

\begin{table}[H]
\centering
\small
\begin{tabular}{lccccc}
\toprule
\textbf{App Type} & \textbf{Hanzo} & \textbf{Vercel} & \textbf{Rly.} & \textbf{Fly} & \textbf{Render} \\
\midrule
Chat API & 1.1m & 0.8m & 2.4m & 1.9m & 3.2m \\
RAG App & 2.3m & 4.1m & 5.7m & 4.8m & 7.1m \\
Image Gen & 6.8m & N/A & 28m & 15m & 35m \\
Voice Agent & 5.2m & N/A & 22m & 12m & 29m \\
Fine-tuning & 12.1m & N/A & N/A & 41m & N/A \\
\midrule
\textbf{Average} & \textbf{5.5m} & --- & 14.5m & 14.9m & 18.6m \\
\bottomrule
\end{tabular}
\caption{Deployment times. N/A = platform does not support GPU workloads.}
\label{tab:deploy_time}
\end{table}

\subsection{Cost Comparison}

Monthly cost for running a medium-traffic RAG application (1000 req/hour):

\begin{table}[H]
\centering
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Component} & \textbf{Hanzo} & \textbf{AWS} & \textbf{Fly} & \textbf{Rly.} \\
\midrule
Compute & \$47 & \$89 & \$62 & \$58 \\
Database & \$15 & \$42 & \$20 & \$25 \\
Vector DB & \$12 & \$67 & \$35 & N/A \\
LLM API & \$180 & \$180 & \$180 & \$180 \\
Storage & \$5 & \$12 & \$8 & \$7 \\
\midrule
\textbf{Total} & \textbf{\$259} & \$390 & \$305 & \$270+ \\
\bottomrule
\end{tabular}
\caption{Monthly cost comparison for a medium-traffic RAG application.}
\label{tab:cost}
\end{table}

\subsection{Developer Experience Survey}

We surveyed 120 developers who deployed AI applications on multiple platforms:

\begin{table}[H]
\centering
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Dimension} & \textbf{Hanzo} & \textbf{Vercel} & \textbf{Fly} & \textbf{AWS} \\
\midrule
Setup ease & 4.6/5 & 4.7/5 & 3.8/5 & 2.4/5 \\
AI support & 4.8/5 & 2.9/5 & 3.2/5 & 3.8/5 \\
Cost clarity & 4.5/5 & 3.4/5 & 3.9/5 & 2.1/5 \\
Debugging & 4.3/5 & 3.8/5 & 3.5/5 & 3.2/5 \\
Scaling & 4.4/5 & 4.2/5 & 4.0/5 & 4.5/5 \\
Overall & \textbf{4.5/5} & 3.8/5 & 3.7/5 & 3.2/5 \\
\bottomrule
\end{tabular}
\caption{Developer experience survey results (120 respondents).}
\label{tab:dx_survey}
\end{table}

\section{Production Deployment}
\label{sec:production}

\subsection{Infrastructure}

Hanzo Platform runs on two DOKS (DigitalOcean Kubernetes Service) clusters:

\begin{itemize}[leftmargin=1.1em]
    \item \textbf{hanzo-k8s} (24.199.76.156): Control plane, databases, core services.
    \item \textbf{GPU pool}: 8x A100 GPU nodes for inference workloads, expandable to 32.
    \item \textbf{Edge nodes}: 12 PoPs globally for CDN and edge compute.
\end{itemize}

\subsection{Usage Statistics (14 Months)}

\begin{table}[H]
\centering
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Applications deployed & 2,847 \\
Total deployments & 41,293 \\
Active organizations & 891 \\
Inference requests served & 47.2M \\
Tokens processed & 18.7B \\
Total LLM cost facilitated & \$2.1M \\
Avg. deployment time & 3.4 min \\
Platform uptime & 99.97\% \\
Avg. cost savings vs. AWS & 34\% \\
\bottomrule
\end{tabular}
\caption{Production statistics (Dec 2024 -- Feb 2026).}
\label{tab:prod_stats}
\end{table}

\subsection{Application Distribution}

\begin{table}[H]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{App Category} & \textbf{Count} & \textbf{Avg. Monthly Cost} \\
\midrule
Chat/conversational & 34.2\% & \$127 \\
RAG/search & 21.7\% & \$234 \\
API services & 18.3\% & \$89 \\
Model serving & 12.1\% & \$412 \\
Data pipelines & 8.4\% & \$187 \\
Other & 5.3\% & \$156 \\
\bottomrule
\end{tabular}
\caption{Application category distribution in production.}
\label{tab:app_dist}
\end{table}

\section{Related Work}
\label{sec:related}

\subsection{General-Purpose PaaS}

Heroku~\cite{heroku2007} pioneered the PaaS model with git-push deployment. Vercel~\cite{vercel2020} specializes in frontend and serverless deployments. Railway~\cite{railway2021} provides database-friendly PaaS. Fly.io~\cite{flyio2020} offers edge computing with container support. Render~\cite{render2019} provides managed infrastructure. None of these treat AI workloads as first-class citizens.

\subsection{AI-Specific Infrastructure}

Modal~\cite{modal2023} provides serverless GPU compute for AI workloads. Replicate~\cite{replicate2022} offers model hosting with API generation. Baseten~\cite{baseten2022} specializes in ML model deployment. Together AI~\cite{together2023} provides inference API with custom model support. Hanzo Platform differs by providing a full application platform (not just model serving) with integrated databases, secrets, and frontend hosting.

\subsection{Kubernetes-Based PaaS}

Dokku~\cite{dokku2013} provides Heroku-like deployment on single servers. KubeSphere~\cite{kubesphere2020} offers Kubernetes-based application management. Dokploy~\cite{dokploy2024} provides self-hosted PaaS on Kubernetes. Hanzo Platform builds on Dokploy's foundation with AI-specific extensions for GPU management, model caching, and inference autoscaling.

\subsection{ML Deployment Platforms}

MLflow~\cite{mlflow2018} provides ML lifecycle management. Seldon Core~\cite{seldon2019} offers ML model serving on Kubernetes. KServe~\cite{kserve2021} provides serverless inference. BentoML~\cite{bentoml2022} packages ML models for deployment. These tools focus on model serving; Hanzo Platform encompasses the full application stack.

\section{Discussion}
\label{sec:discussion}

\subsection{Vendor Lock-in Mitigation}

Hanzo Platform uses standard Kubernetes primitives and OCI containers, enabling applications to be extracted and deployed on any Kubernetes cluster. The \texttt{hanzo eject} command generates standard Kubernetes manifests, Dockerfiles, and Helm charts for migration.

\subsection{Limitations}

\begin{enumerate}[leftmargin=1.4em]
    \item \textbf{GPU availability}: GPU capacity is finite; burst scaling may be delayed during high-demand periods.
    \item \textbf{Cold starts}: GPU inference cold starts (30--120s for model loading) remain a challenge despite pre-warming.
    \item \textbf{Regional coverage}: Currently deployed in 3 regions (US East, US West, EU West); Asia-Pacific coverage is planned.
    \item \textbf{Custom hardware}: Only NVIDIA GPUs are currently supported; AMD and custom accelerators are on the roadmap.
\end{enumerate}

\subsection{Future Work}

\begin{itemize}[leftmargin=1.1em]
    \item \textbf{Edge inference}: Deploy small models at CDN edge nodes for ultra-low-latency inference.
    \item \textbf{Multi-cloud}: Support AWS, GCP, and Azure as compute backends alongside DigitalOcean.
    \item \textbf{Fine-tuning as a service}: Managed fine-tuning with automatic deployment of resulting models.
    \item \textbf{AI-assisted operations}: Use LLMs to diagnose deployment failures and suggest fixes.
\end{itemize}

\section{Conclusion}
\label{sec:conclusion}

We have presented Hanzo Platform, a PaaS designed for AI-native applications that treats LLM inference, vector databases, GPU compute, and model serving as first-class platform primitives. The AI-aware build system reduces deployment time by 4.7x, the unified resource model simplifies infrastructure management, and the cost-optimized autoscaler reduces inference costs by 38\%. Evaluation against five competing platforms demonstrates superior performance across deployment time, cost, scaling, and developer experience. Production deployment with 2,800+ applications and 47M inference requests over 14 months validates the practical viability of the approach. Hanzo Platform is available at \texttt{platform.hanzo.ai}.

\bibliographystyle{plain}
\begin{thebibliography}{28}

\bibitem{baseten2022}
Baseten.
\newblock Baseten: The fastest way to deploy {ML} models.
\newblock \emph{Baseten Documentation}, 2022.

\bibitem{bentoml2022}
BentoML.
\newblock {BentoML}: The unified model serving framework.
\newblock \emph{GitHub Repository}, 2022.

\bibitem{dokku2013}
Dokku.
\newblock Dokku: The smallest {PaaS} implementation you've ever seen.
\newblock \emph{GitHub Repository}, 2013.

\bibitem{dokploy2024}
Dokploy.
\newblock Dokploy: Self-hosted platform as a service.
\newblock \emph{GitHub Repository}, 2024.

\bibitem{flyio2020}
Fly.io.
\newblock Fly.io: Run your full stack apps close to your users.
\newblock \emph{Fly.io Documentation}, 2020.

\bibitem{heroku2007}
Heroku.
\newblock Heroku: Cloud application platform.
\newblock \emph{Heroku Documentation}, 2007.

\bibitem{kserve2021}
KServe.
\newblock {KServe}: Highly scalable and standards based model inference platform.
\newblock \emph{KServe Documentation}, 2021.

\bibitem{kubesphere2020}
KubeSphere.
\newblock {KubeSphere}: The container platform tailored for {Kubernetes}.
\newblock \emph{KubeSphere Documentation}, 2020.

\bibitem{mlflow2018}
M.~Zaharia, A.~Chen, A.~Davidson, et~al.
\newblock Accelerating the machine learning lifecycle with {MLflow}.
\newblock \emph{IEEE Data Engineering Bulletin}, 41(4):39--45, 2018.

\bibitem{modal2023}
Modal.
\newblock Modal: End the struggle with cloud infrastructure.
\newblock \emph{Modal Documentation}, 2023.

\bibitem{railway2021}
Railway.
\newblock Railway: Instant deployments, effortless scale.
\newblock \emph{Railway Documentation}, 2021.

\bibitem{render2019}
Render.
\newblock Render: Cloud application hosting.
\newblock \emph{Render Documentation}, 2019.

\bibitem{replicate2022}
Replicate.
\newblock Replicate: Run and fine-tune open-source models.
\newblock \emph{Replicate Documentation}, 2022.

\bibitem{seldon2019}
Seldon.
\newblock Seldon core: An open source platform to deploy machine learning models.
\newblock \emph{Seldon Documentation}, 2019.

\bibitem{together2023}
Together~AI.
\newblock Together: Fast inference and fine-tuning.
\newblock \emph{Together Documentation}, 2023.

\bibitem{vercel2020}
Vercel.
\newblock Vercel: Develop. Preview. Ship.
\newblock \emph{Vercel Documentation}, 2020.

\bibitem{burns2016borg}
B.~Burns, B.~Grant, D.~Oppenheimer, E.~Brewer, and J.~Wilkes.
\newblock Borg, omega, and {Kubernetes}.
\newblock \emph{ACM Queue}, 14(1):70--93, 2016.

\bibitem{hindman2011mesos}
B.~Hindman, A.~Konwinski, M.~Zaharia, et~al.
\newblock Mesos: A platform for fine-grained resource sharing in the data center.
\newblock In \emph{NSDI}, 2011.

\bibitem{kwon2023vllm}
W.~Kwon, Z.~Li, S.~Zhuang, et~al.
\newblock Efficient memory management for large language model serving with {PagedAttention}.
\newblock In \emph{SOSP}, 2023.

\bibitem{zheng2024sglang}
L.~Zheng, L.~Yin, Z.~Xie, et~al.
\newblock {SGLang}: Efficient execution of structured language model programs.
\newblock \emph{arXiv preprint arXiv:2312.07104}, 2024.

\bibitem{sheng2023flexgen}
Y.~Sheng, L.~Zheng, B.~Yuan, et~al.
\newblock {FlexGen}: High-throughput generative inference of large language models with a single {GPU}.
\newblock In \emph{ICML}, 2023.

\bibitem{pope2023efficiently}
R.~Pope, S.~Douglas, A.~Chowdhery, et~al.
\newblock Efficiently scaling transformer inference.
\newblock In \emph{MLSys}, 2023.

\bibitem{aminabadi2022deepspeed}
R.~Y. Aminabadi, S.~Rajbhandari, M.~Zhang, et~al.
\newblock {DeepSpeed} inference: Enabling efficient inference of transformer models at unprecedented scale.
\newblock In \emph{SC22}, 2022.

\bibitem{patel2024splitwise}
P.~Patel, E.~Choukse, C.~Zhang, et~al.
\newblock Splitwise: Efficient generative {LLM} inference using phase splitting.
\newblock In \emph{ISCA}, 2024.

\bibitem{agrawal2024taming}
A.~Agrawal, N.~Kedia, A.~Panwar, et~al.
\newblock Taming throughput-latency tradeoff in {LLM} inference with {Sarathi-Serve}.
\newblock In \emph{OSDI}, 2024.

\end{thebibliography}

\end{document}
