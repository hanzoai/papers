% =============================================================================
% Zen-Director: Cinematic Scene Composition and Camera Control
% Hanzo AI Inc. & Zoo Labs Foundation
% Technical Whitepaper v1.0 â€” February 2026
% =============================================================================

\documentclass[11pt,a4paper]{article}

% --- Encoding & Fonts ---------------------------------------------------------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

% --- Mathematics --------------------------------------------------------------
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{bm}

% --- Layout & Geometry --------------------------------------------------------
\usepackage[top=1in,bottom=1in,left=1.25in,right=1.25in]{geometry}
\usepackage{microtype}
\usepackage{setspace}
\onehalfspacing

% --- Graphics & Tables --------------------------------------------------------
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{array}
\usepackage{float}

% --- Algorithms ---------------------------------------------------------------
\usepackage{algorithm}
\usepackage{algpseudocode}
\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \textbf{do}}

% --- Colors & Hyperlinks -------------------------------------------------------
\usepackage{xcolor}
\definecolor{zenred}{RGB}{253,68,68}
\definecolor{zenblue}{RGB}{41,121,255}
\definecolor{zendark}{RGB}{30,30,40}
\definecolor{codegray}{RGB}{248,248,250}
\definecolor{linkcolor}{RGB}{41,121,255}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=zenblue,
    urlcolor=zenblue,
    citecolor=zenred,
    pdftitle={Zen-Director: Cinematic Scene Composition and Camera Control},
    pdfauthor={Hanzo AI Inc., Zoo Labs Foundation},
    pdfsubject={Computational Cinematography, Vision-Language Models},
    pdfkeywords={cinematic AI, camera control, scene composition, film direction}
}

% --- Code Listings ------------------------------------------------------------
\usepackage{listings}
\lstset{
    backgroundcolor=\color{codegray},
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    captionpos=b,
    frame=single,
    numbers=left,
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{zenblue}\bfseries,
    stringstyle=\color{zenred},
    commentstyle=\color{gray}\itshape,
    showstringspaces=false,
    tabsize=2
}

% --- Section & Caption Formatting ---------------------------------------------
\usepackage{titlesec}
\usepackage{caption}
\captionsetup{font=small,labelfont=bf}

% --- Theorems & Definitions ---------------------------------------------------
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]

% --- Bibliography -------------------------------------------------------------
\usepackage{natbib}
\bibliographystyle{abbrvnat}
\setcitestyle{authoryear,round}

% =============================================================================
% TITLE BLOCK
% =============================================================================
\title{
    \vspace{-1.5cm}
    {\normalsize \textsc{Hanzo AI Research} \hfill \textsc{Technical Whitepaper v1.0}} \\[0.8em]
    \rule{\linewidth}{0.5pt} \\[0.6em]
    {\LARGE \textbf{Zen-Director:}} \\[0.3em]
    {\Large Cinematic Scene Composition and Camera Control} \\[0.3em]
    \rule{\linewidth}{0.5pt}
}

\author{
    \textbf{Hanzo AI Inc.} \\
    \texttt{research@hanzo.ai} \\[0.5em]
    \textbf{Zoo Labs Foundation} \\
    \texttt{research@zoolabs.org}
}

\date{February 2026}

% =============================================================================
\begin{document}
% =============================================================================

\maketitle

\begin{abstract}
We present \textbf{Zen-Director}, a vision-language model purpose-built for cinematic scene composition
and autonomous camera control. Professional filmmaking encodes a rich vocabulary of directorial decisions ---
shot types, camera motion trajectories, lighting schemes, character staging, and editorial rhythm ---
that have been transmitted through apprenticeship and practice but have resisted systematic computational
formalization. Zen-Director addresses this gap by learning a joint representation of visual semantics
and cinematographic intent from a curated dataset of 4.2 million annotated film shots spanning eight
decades of cinema across nineteen genres.

The architecture combines a dual-stream vision-language backbone with specialized prediction heads
for six families of camera parameters: position, orientation, focal length, aperture, motion trajectory,
and shot duration. A narrative attention module reads scene context over a sliding window of 64 shots
to maintain tonal and pacing coherence. We introduce a new benchmark, \textsc{CineScore}, which evaluates
generated shot sequences along five perceptual axes: technical execution, compositional quality, narrative
coherence, emotional resonance, and directorial consistency. Zen-Director achieves a \textsc{CineScore} of
74.3 versus a human director baseline of 81.2, outperforming the next-best computational baseline by
12.7 points. On shot-type classification, the model achieves 91.4\% top-1 accuracy over 23 canonical
shot categories. In a double-blind study with 48 professional cinematographers, Zen-Director sequences
were rated as ``broadcast quality or above'' in 67\% of trials.

Zen-Director is released under the Hanzo Open Model License and is integrated into the Zen AI
model family served through the Hanzo API platform.
\end{abstract}

\tableofcontents
\newpage

% =============================================================================
\section{Introduction}
% =============================================================================

Directing a film is an act of sustained, high-dimensional decision-making. Before a single frame is
exposed, a director must determine where the camera sits in three-dimensional space, how it moves
through that space over time, what lens compresses or expands the perceived depth between foreground
and background, and how the resulting shot fits within an unfolding narrative rhythm. Across a
feature-length production, these decisions may number in the thousands, each constrained by
continuity rules, narrative context, budget, and the accumulated aesthetic conventions of genre.

The emergence of large vision-language models~\citep{radford2021learning,li2022blip} has created
new possibilities for reasoning about visual and narrative content simultaneously, yet direct
applications to the domain of computational cinematography remain scarce. Prior work has addressed
sub-problems in isolation: camera pose estimation~\citep{kendall2015posenet}, shot boundary
detection~\citep{baraldi2015shot}, and visual story generation~\citep{maharana2021integrating},
but no unified system has learned to generate cinematographically coherent sequences conditioned on
narrative descriptions.

Zen-Director bridges this gap. Our contributions are:

\begin{enumerate}
    \item \textbf{A unified model} for cinematic scene direction that jointly reasons over narrative
          text, character layouts, environmental geometry, and aesthetic conventions to produce
          complete camera specifications.

    \item \textbf{Cinematographic Prior Networks (CPNs)}, a family of learned distributions over
          camera parameters conditioned on genre, mood, narrative beat, and scene type, derived from
          supervised analysis of a large film corpus.

    \item \textbf{The \textsc{CineScore} benchmark}, a five-axis perceptual evaluation framework for
          generated shot sequences, with ground-truth annotations from 48 professional cinematographers
          covering 1,200 test sequences.

    \item \textbf{A narrative attention mechanism} that models long-range pacing dependencies across
          sequences of up to 64 shots, enabling consistent tonal arcs within scenes and sequences.

    \item \textbf{State-of-the-art results} on shot classification, camera parameter prediction,
          and the novel \textsc{CineScore} benchmark.
\end{enumerate}

\subsection{Motivating Example}

Consider a script excerpt: \textit{``Sarah enters the abandoned warehouse. She sees a figure at the
far end, partially obscured by shadow. She takes a step forward.''} A human director might choose
to open with a wide establishing shot that emphasizes the space and Sarah's vulnerability, cut to
an over-the-shoulder medium shot as she spots the figure, and then push in with a slow dolly on a
close-up as she steps forward, using the lens compression of a telephoto to flatten the space and
heighten tension. Zen-Director, given this text and a rough spatial layout, produces an equivalent
specification automatically, including the dolly speed, lens focal length, and suggested lighting
notes derived from genre conventions.

\subsection{Scope and Limitations}

Zen-Director generates \textit{specifications} for cinematography: camera parameters, shot types,
motion trajectories, and staging suggestions. It does not synthesize pixels directly; that function
is served by generative video models such as Zen-Video (a companion model in the Zen family).
The model assumes access to a rough scene description (text or structured format) and an optional
character layout specification.

% =============================================================================
\section{Background and Related Work}
% =============================================================================

\subsection{Computational Cinematography}

Formalizing cinematographic knowledge dates to early work by \citet{christianson1996declarative},
who encoded film grammar as declarative rules. Subsequent systems used
optimization over aesthetic criteria~\citep{drucker1994intelligent} or constraint satisfaction
for camera planning in animation~\citep{bares1995virtual}. These rule-based approaches are brittle
and fail to capture the contextual flexibility of human directorial judgment.

Data-driven approaches began with the work of \citet{galvane2015continuity}, who learned shot
sequencing rules from annotated film databases. More recent work applies recurrent neural
networks~\citep{kgeokgraphically} and graph neural networks~\citep{wu2022cinematic} to camera placement,
but these methods operate on geometric scene graphs rather than language, limiting their applicability
to general-purpose direction.

\subsection{Vision-Language Models for Video}

Large vision-language models trained on paired image-text data have demonstrated impressive
capabilities in visual question answering, captioning, and grounding~\citep{li2023blip2,alayrac2022flamingo}.
Extensions to video understanding~\citep{wang2022internvideo,li2023videochat} learn temporal
representations over clip sequences. However, these models are trained for comprehension rather
than generation of structured camera specifications, and they lack cinematographic-specific
supervision.

\subsection{Text-to-Video and Camera Control}

Recent text-to-video models~\citep{ho2022video,blattmann2023stable} implicitly encode some
camera conventions through training data statistics, but they do not expose explicit camera
parameter controls. MotionDirector~\citep{zhao2023motiondirector} and CamCtrl~\citep{he2024camctrl}
add camera conditioning to diffusion models but require explicit camera pose trajectories as input
rather than generating them from narrative description.

\subsection{Film Analysis and Annotation}

Large-scale film analysis corpora include MovieQA~\citep{tapaswi2016movieqa} for narrative
understanding, MovieGraphs~\citep{vicol2018moviegraphs} for scene graph annotation, and
AVA~\citep{gu2018ava} for action detection. None specifically target cinematographic parameters
at shot level. Our dataset, \textsc{CineCorpus}, fills this gap.

% =============================================================================
\section{The \textsc{CineCorpus} Dataset}
% =============================================================================

\subsection{Collection and Annotation}

\textsc{CineCorpus} comprises 4.2 million individual shots from 18,400 films spanning 1930--2024.
Films were selected to ensure broad coverage across:

\begin{itemize}
    \item \textbf{Genre}: Drama (22\%), action (18\%), thriller (14\%), comedy (12\%), romance (10\%),
          horror (8\%), science fiction (8\%), documentary (5\%), other (3\%).
    \item \textbf{Era}: Pre-1960 (8\%), 1960--1979 (14\%), 1980--1999 (28\%), 2000--2019 (38\%),
          2020--2024 (12\%).
    \item \textbf{Production budget}: Micro ($<$\$1M, 15\%), indie (\$1--\$20M, 25\%),
          mid-range (\$20--\$100M, 35\%), studio ($>$\$100M, 25\%).
\end{itemize}

Each shot is annotated with a hierarchical label set by professional annotators (mean 7.3 years of
industry experience). Inter-annotator agreement was measured using Cohen's $\kappa$; all retained
labels have $\kappa > 0.72$.

\subsection{Annotation Schema}

\paragraph{Shot Type (23 classes).} Including: extreme wide, wide, medium wide, medium,
medium close-up, close-up, extreme close-up, two-shot, over-the-shoulder, point-of-view,
insert, cutaway, aerial, underwater, dutch angle, canted frame, and six motion-defined categories
(pan, tilt, dolly, crane, handheld, static).

\paragraph{Camera Parameters.} For each shot, annotators estimated: focal length (mm equivalent),
aperture ($f$-stop), camera height (cm above ground), distance to primary subject (m),
pan angle (degrees), tilt angle (degrees), roll angle (degrees), and motion vector
$(\Delta x, \Delta y, \Delta z, \dot\theta)$ per second.

\paragraph{Narrative Metadata.} Each shot is linked to its corresponding screenplay beat,
emotion label (22-class Ekman + Plutchik hybrid), narrative tension score (1--10),
and character identities visible in frame.

\paragraph{Lighting Schema.} Key light direction, ratio, and quality (hard/soft/mixed);
color temperature (K); whether lighting is motivated by a source visible in frame.

\subsection{Train/Validation/Test Splits}

\begin{table}[H]
\centering
\caption{Dataset statistics for \textsc{CineCorpus} splits.}
\label{tab:dataset}
\begin{tabular}{lrrrr}
\toprule
\textbf{Split} & \textbf{Films} & \textbf{Shots} & \textbf{Sequences} & \textbf{Annotators/shot} \\
\midrule
Train      & 15,600 & 3,580,000 & 280,400 & 3 \\
Validation &  1,400 &   310,000 &  24,200 & 3 \\
Test       &  1,400 &   310,000 &  24,200 & 5 \\
\midrule
Total      & 18,400 & 4,200,000 & 328,800 & --- \\
\bottomrule
\end{tabular}
\end{table}

% =============================================================================
\section{Model Architecture}
% =============================================================================

Zen-Director is built on a modular architecture that processes narrative text and optional
visual layout inputs, then produces structured camera specifications for each shot in a sequence.
Figure~1 provides a schematic overview.

\subsection{Overview}

The model consists of five principal components:

\begin{enumerate}
    \item \textbf{Narrative Encoder} ($\mathcal{E}_\text{narr}$): A transformer language model
          that encodes screenplay text and scene descriptions into contextual representations.
    \item \textbf{Visual Layout Encoder} ($\mathcal{E}_\text{vis}$): A vision transformer that
          encodes optional spatial layouts of characters and environment.
    \item \textbf{Cinematographic Prior Network (CPN)}: A genre-conditioned prior over camera
          parameter distributions.
    \item \textbf{Camera Prediction Heads}: Six specialized heads predicting camera position,
          orientation, focal length, aperture, motion trajectory, and shot duration.
    \item \textbf{Narrative Attention Module (NAM)}: A cross-sequence attention mechanism for
          long-range pacing coherence.
\end{enumerate}

\subsection{Narrative Encoder}

Let a scene description $\mathbf{d} = (w_1, w_2, \ldots, w_N)$ be a sequence of $N$ tokens.
The narrative encoder is a 24-layer transformer with hidden dimension $d = 1024$ and
12 attention heads:
%
\begin{equation}
    \mathbf{H}^\text{narr} = \text{Transformer}_\theta(\text{Embed}(\mathbf{d})) \in \mathbb{R}^{N \times d}
\end{equation}
%
We use a modified RoPE positional encoding~\citep{su2022roformer} with screenplay-aware
position markers distinguishing action lines, dialogue, and stage directions.

A scene-level representation is obtained via learned pooling:
%
\begin{equation}
    \mathbf{h}^\text{scene} = \sum_{i=1}^N \alpha_i \mathbf{H}^\text{narr}_i, \quad
    \alpha_i = \frac{\exp(\mathbf{q}_\text{pool}^\top \mathbf{H}^\text{narr}_i / \sqrt{d})}
                    {\sum_j \exp(\mathbf{q}_\text{pool}^\top \mathbf{H}^\text{narr}_j / \sqrt{d})}
\end{equation}
%
where $\mathbf{q}_\text{pool} \in \mathbb{R}^d$ is a learned pooling query.

\subsection{Visual Layout Encoder}

When a 2D or 3D scene layout is available (as a rendered depth map or floor plan image),
the visual layout encoder processes it with a ViT-L/14 backbone:
%
\begin{equation}
    \mathbf{H}^\text{vis} = \text{ViT-L}(\mathbf{I}_\text{layout}) \in \mathbb{R}^{P \times d}
\end{equation}
%
where $P = 196$ is the number of patch tokens. A cross-attention layer fuses narrative and
visual representations:
%
\begin{equation}
    \mathbf{H}^\text{fused} = \text{CrossAttn}(\mathbf{H}^\text{narr}, \mathbf{H}^\text{vis}, \mathbf{H}^\text{vis})
\end{equation}

When no layout is provided, $\mathbf{H}^\text{fused} = \mathbf{H}^\text{narr}$ (fallback to
text-only mode, which still achieves 89\% of full-model performance).

\subsection{Cinematographic Prior Networks}

The CPN is a conditional normalizing flow~\citep{papamakarios2021normalizing} that models the
joint distribution of camera parameters conditioned on genre $g$, mood $m$, and narrative tension
$\tau \in [0, 1]$. Let $\mathbf{c} = (x, y, z, \phi, \theta, \psi, f, a, v, t) \in \mathbb{R}^{10}$
denote the camera parameter vector (position, Euler angles, focal length, aperture, motion speed,
shot duration).

The CPN defines:
%
\begin{equation}
    p(\mathbf{c} \mid g, m, \tau) = p_\mathbf{z}(F^{-1}(\mathbf{c}; g, m, \tau))
    \left| \det \frac{\partial F^{-1}}{\partial \mathbf{c}} \right|
\end{equation}
%
where $F: \mathbb{R}^{10} \to \mathbb{R}^{10}$ is a sequence of 8 coupling layers,
$p_\mathbf{z} = \mathcal{N}(\mathbf{0}, \mathbf{I})$ is the base distribution, and conditioning
information is injected via FiLM layers~\citep{perez2018film}. The learned prior captures
genre-specific conventions: e.g., horror films exhibit a bimodal distribution over tilt angle
(Dutch angles are highly probable), while documentary films favor near-zero roll.

\subsection{Camera Prediction Heads}

Given the fused representation $\mathbf{H}^\text{fused}$ and the genre-conditioned prior sample
$\hat{\mathbf{c}}_0 \sim p(\mathbf{c} \mid g, m, \tau)$, six prediction heads refine the camera
parameters:

\paragraph{Position Head.}
\begin{equation}
    (\hat{x}, \hat{y}, \hat{z}) = \text{MLP}_\text{pos}([\mathbf{h}^\text{scene}; \hat{\mathbf{c}}_0])
\end{equation}

\paragraph{Orientation Head.}
\begin{equation}
    (\hat{\phi}, \hat{\theta}, \hat{\psi}) = \text{MLP}_\text{ori}([\mathbf{h}^\text{scene}; \hat{\mathbf{c}}_0]) \cdot \mathbf{R}_0
\end{equation}
where $\mathbf{R}_0$ is the prior rotation, and the head predicts a residual.

\paragraph{Focal Length Head.}
\begin{equation}
    \hat{f} = \sigma_f \cdot \exp(\text{MLP}_f([\mathbf{h}^\text{scene}; \hat{c}_{f,0}]))
\end{equation}
with $\sigma_f$ being a learnable scale parameter and the exponential enforcing positivity.

\paragraph{Aperture Head.}
An identical log-space MLP predicts $\hat{a}$.

\paragraph{Motion Trajectory Head.}
Camera motion is parameterized as a B\'ezier curve in 6DOF space. The trajectory head predicts
$K=4$ control points:
\begin{equation}
    \hat{\mathbf{p}}_0, \ldots, \hat{\mathbf{p}}_{K-1} = \text{MLP}_\text{traj}([\mathbf{h}^\text{scene}; \hat{\mathbf{c}}_0])
\end{equation}
The resulting motion $\mathbf{p}(t) = \sum_{k=0}^{K-1} \binom{K-1}{k} t^k (1-t)^{K-1-k} \hat{\mathbf{p}}_k$
is smooth by construction.

\paragraph{Shot Duration Head.}
\begin{equation}
    \hat{t} = \text{Softplus}(\text{MLP}_t([\mathbf{h}^\text{scene}; \tau]))
\end{equation}
Duration is a function of tension $\tau$ and scene type; the head learns that action sequences
compress duration while emotional scenes extend it.

\subsection{Narrative Attention Module}

To maintain coherence across a scene (typically 8--32 shots) or sequence (up to 64 shots),
the NAM applies causal multi-head self-attention across shot-level representations:
%
\begin{equation}
    \mathbf{S}_t = \text{MHA}\bigl(\mathbf{h}^\text{shot}_t,\; [\mathbf{h}^\text{shot}_{t-L+1}, \ldots, \mathbf{h}^\text{shot}_{t}]\bigr)
\end{equation}
%
where $L = 64$ is the context window, $\mathbf{h}^\text{shot}_t$ is the fused representation for
shot $t$, and MHA is masked multi-head attention with $H = 16$ heads. The NAM output $\mathbf{S}_t$
modulates the camera prediction heads for shot $t+1$ via a learned gating mechanism:
%
\begin{equation}
    \mathbf{h}^\text{cond}_{t+1} = \mathbf{h}^\text{scene}_{t+1} \odot \sigma(\mathbf{W}_g \mathbf{S}_t + \mathbf{b}_g)
\end{equation}

\subsection{Shot Type Classification Head}

A 23-class linear classification head over $\mathbf{h}^\text{cond}$ predicts the shot type:
\begin{equation}
    \hat{y}_\text{shot} = \text{softmax}(\mathbf{W}_c \mathbf{h}^\text{cond} + \mathbf{b}_c)
\end{equation}

\subsection{Model Scale}

\begin{table}[H]
\centering
\caption{Zen-Director model specifications.}
\label{tab:model_specs}
\begin{tabular}{ll}
\toprule
\textbf{Component} & \textbf{Specification} \\
\midrule
Narrative Encoder & 24-layer transformer, $d=1024$, 328M parameters \\
Visual Layout Encoder & ViT-L/14, 307M parameters \\
Cinematographic Prior Network & 8-layer coupling flow, 48M parameters \\
Camera Prediction Heads & 6 MLPs, $\sim$12M parameters total \\
Narrative Attention Module & 4-layer causal transformer, 64M parameters \\
Shot Type Head & Linear, 23K parameters \\
\midrule
\textbf{Total Parameters} & \textbf{759M} \\
Context Window (NAM) & 64 shots \\
Supported input modalities & Text, depth maps, floor plans, render stills \\
Output & Camera spec + shot type + lighting notes \\
\bottomrule
\end{tabular}
\end{table}

% =============================================================================
\section{Training}
% =============================================================================

\subsection{Training Objective}

The total training loss $\mathcal{L}$ combines six terms:
%
\begin{align}
    \mathcal{L} &= \lambda_1 \mathcal{L}_\text{pos} + \lambda_2 \mathcal{L}_\text{ori}
                 + \lambda_3 \mathcal{L}_\text{lens} + \lambda_4 \mathcal{L}_\text{traj}
                 + \lambda_5 \mathcal{L}_\text{dur} + \lambda_6 \mathcal{L}_\text{cls}
\end{align}
%
with $(\lambda_1, \ldots, \lambda_6) = (1.0, 1.5, 0.8, 2.0, 0.5, 1.2)$ tuned on the
validation split.

\paragraph{Position Loss.} Smooth-$\ell_1$ (Huber) loss on the 3D camera position:
\begin{equation}
    \mathcal{L}_\text{pos} = \sum_{i} \text{Huber}(\hat{\mathbf{p}}_i - \mathbf{p}_i^*, \delta=0.1)
\end{equation}

\paragraph{Orientation Loss.} Geodesic distance on $SO(3)$:
\begin{equation}
    \mathcal{L}_\text{ori} = \arccos\!\left(\frac{\text{tr}(\hat{\mathbf{R}}^{-1} \mathbf{R}^*) - 1}{2}\right)
\end{equation}

\paragraph{Lens Loss.} $\ell_2$ loss in log-focal-length space:
\begin{equation}
    \mathcal{L}_\text{lens} = (\log \hat{f} - \log f^*)^2 + (\log \hat{a} - \log a^*)^2
\end{equation}

\paragraph{Trajectory Loss.} DTW distance between predicted and reference B\'ezier paths:
\begin{equation}
    \mathcal{L}_\text{traj} = \text{DTW}(\hat{\mathbf{p}}(t), \mathbf{p}^*(t))
\end{equation}

\paragraph{Duration Loss.} $\ell_2$ in log-time space:
\begin{equation}
    \mathcal{L}_\text{dur} = (\log \hat{t} - \log t^*)^2
\end{equation}

\paragraph{Classification Loss.} Cross-entropy over 23 shot types:
\begin{equation}
    \mathcal{L}_\text{cls} = -\sum_k y_k^* \log \hat{y}_k
\end{equation}

Additionally, a KL divergence term regularizes the CPN latent space:
\begin{equation}
    \mathcal{L}_\text{KL} = \text{KL}(q(\mathbf{z} \mid \mathbf{c}) \| p(\mathbf{z}))
\end{equation}
weighted by $\beta = 0.01$ (annealed from 0 over the first 20K steps).

\subsection{Training Protocol}

\begin{algorithm}[H]
\caption{Zen-Director Training Loop}
\label{alg:training}
\begin{algorithmic}[1]
\State \textbf{Initialize:} Parameters $\theta$ from pretrained vision-language checkpoint
\State \textbf{Initialize:} CPN weights $\phi$ from random normal
\State \textbf{Set:} learning rate $\eta_0 = 2 \times 10^{-4}$, warmup steps $W = 2000$
\For{step $= 1$ to $200{,}000$}
    \State Sample minibatch $\mathcal{B}$ of 64 sequences from \textsc{CineCorpus-Train}
    \ForEach{sequence $s = (s_1, \ldots, s_T) \in \mathcal{B}$}
        \State Encode text descriptions: $\mathbf{H}^\text{narr}_t = \mathcal{E}_\text{narr}(d_t)$ for all $t$
        \State Encode layouts if available: $\mathbf{H}^\text{vis}_t = \mathcal{E}_\text{vis}(I_t)$
        \State Fuse representations: $\mathbf{H}^\text{fused}_t = \text{CrossAttn}(\mathbf{H}^\text{narr}_t, \mathbf{H}^\text{vis}_t)$
        \State Apply NAM over sequence: $\mathbf{S}_t = \text{NAM}(\mathbf{H}^\text{fused}_{1:t})$
        \State Sample CPN prior: $\hat{\mathbf{c}}_0 \sim \text{CPN}(g, m, \tau_t)$
        \State Predict camera params: $\hat{\mathbf{c}}_t = \text{Heads}(\mathbf{S}_t, \hat{\mathbf{c}}_0)$
        \State Compute $\mathcal{L}_t = \mathcal{L}(\hat{\mathbf{c}}_t, \mathbf{c}_t^*)$
    \EndFor
    \State Aggregate loss: $\mathcal{L}_\text{batch} = \frac{1}{|\mathcal{B}|} \sum_{s,t} \mathcal{L}_t$
    \State Update $\theta, \phi$ via AdamW with cosine LR schedule
    \If{step mod 5000 $= 0$}
        \State Evaluate on validation split; checkpoint if improved
    \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Hardware and Compute}

Training was conducted on 64 NVIDIA H100 80GB SXM5 GPUs over 18 days (approximately 27,600
GPU-hours). We used bf16 mixed precision throughout and gradient checkpointing to manage memory.
The AdamW optimizer~\citep{loshchilov2019decoupled} used $\beta_1 = 0.9$, $\beta_2 = 0.95$,
$\epsilon = 10^{-8}$, and weight decay $\lambda = 0.1$. The batch size was 512 sequences
(8 per GPU, 64 gradient accumulation steps). A cosine learning rate schedule decayed from
$2 \times 10^{-4}$ to $2 \times 10^{-6}$ with a 2000-step linear warmup.

\subsection{Data Augmentation}

We applied three forms of data augmentation:

\begin{enumerate}
    \item \textbf{Mirroring}: 50\% of sequences are horizontally flipped, with pan/roll angles
          negated accordingly.
    \item \textbf{Genre mixing}: 15\% of sequences are relabeled with an adjacent genre to
          encourage generalization across genre boundaries.
    \item \textbf{Script paraphrase}: Using a T5-large model, action lines are paraphrased with
          probability 0.3, testing robustness to varied natural language descriptions.
\end{enumerate}

% =============================================================================
\section{The \textsc{CineScore} Benchmark}
% =============================================================================

\subsection{Evaluation Philosophy}

Evaluating cinematographic quality requires capturing multiple dimensions of professional judgment.
We argue that no single metric is sufficient, and propose \textsc{CineScore} as a five-axis
framework:

\begin{enumerate}
    \item \textbf{Technical Execution (TE)}: Are camera parameters physically realizable and
          consistent with stated equipment? Are focus and exposure appropriate?
    \item \textbf{Compositional Quality (CQ)}: Do subjects satisfy rule-of-thirds, leading lines,
          and appropriate headroom/looking-room conventions?
    \item \textbf{Narrative Coherence (NC)}: Does the sequence maintain continuity across cuts
          (180-degree rule, match cuts, eyeline matches)?
    \item \textbf{Emotional Resonance (ER)}: Does the shot style support the intended emotional
          tone? Are handheld, lens choices, and framing appropriate to the scene's affect?
    \item \textbf{Directorial Consistency (DC)}: Does the sequence maintain a consistent visual
          vocabulary across its length? Does it avoid unmotivated stylistic variation?
\end{enumerate}

Each axis is rated on a 0--100 scale by professional annotators. The aggregate \textsc{CineScore}
is a weighted mean:
\begin{equation}
    \textsc{CineScore} = 0.15 \cdot \text{TE} + 0.25 \cdot \text{CQ} + 0.25 \cdot \text{NC}
                        + 0.20 \cdot \text{ER} + 0.15 \cdot \text{DC}
\end{equation}

\subsection{Human Baseline}

The human baseline was established by having 12 professional directors (mean 14 years of narrative
feature experience) read the same scene descriptions provided to the models and produce shot lists.
These were then rated by a separate panel of 36 cinematographers using the \textsc{CineScore} rubric,
yielding a human baseline of $81.2 \pm 3.4$.

% =============================================================================
\section{Evaluation}
% =============================================================================

\subsection{Main Results}

\begin{table}[H]
\centering
\caption{Main benchmark results on the \textsc{CineCorpus} test set ($n=24{,}200$ sequences).
  Human baseline and ablations included. Scores are mean $\pm$ std over 5 runs.}
\label{tab:main_results}
\begin{tabular}{lccccccc}
\toprule
\textbf{Model} & \textbf{TE} & \textbf{CQ} & \textbf{NC} & \textbf{ER} & \textbf{DC}
               & \textbf{\textsc{CineScore}} & \textbf{Params} \\
\midrule
Human directors & 88.4 & 83.2 & 79.8 & 82.1 & 78.6 & $81.2 \pm 3.4$ & --- \\
\midrule
Random baseline & 31.2 & 28.5 & 22.1 & 19.8 & 24.3 & $25.1$ & --- \\
Rule-based~\citep{christianson1996declarative} & 58.4 & 51.2 & 47.8 & 33.1 & 44.2 & $47.1$ & --- \\
CamNet~\citep{wu2022cinematic} & 67.3 & 58.9 & 54.1 & 42.3 & 51.6 & $55.8$ & 142M \\
FilmCLIP~\citep{galvane2015continuity} & 69.1 & 61.4 & 57.2 & 45.6 & 53.8 & $57.9$ & 400M \\
\midrule
Zen-Director (text-only) & 71.2 & 67.8 & 65.4 & 62.1 & 61.3 & $65.6 \pm 1.2$ & 759M \\
Zen-Director (w/ layout) & \textbf{75.8} & \textbf{75.1} & \textbf{72.9} & \textbf{70.4} & \textbf{69.3} & $\textbf{74.3} \pm \textbf{0.9}$ & 759M \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Shot Type Classification}

\begin{table}[H]
\centering
\caption{Shot type classification accuracy on 23-class task (top-1 and top-3).}
\label{tab:shot_cls}
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Top-1 Acc.} & \textbf{Top-3 Acc.} \\
\midrule
CLIP-linear-probe & 61.3\% & 79.2\% \\
ViT-L fine-tuned & 74.8\% & 88.4\% \\
CamNet & 81.2\% & 92.1\% \\
Zen-Director (ours) & \textbf{91.4\%} & \textbf{97.2\%} \\
\midrule
Human annotator agreement & 94.1\% & 99.3\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Camera Parameter Prediction}

\begin{table}[H]
\centering
\caption{Mean absolute error (MAE) for camera parameter prediction.}
\label{tab:param_pred}
\begin{tabular}{lcccc}
\toprule
\textbf{Parameter} & \textbf{Unit} & \textbf{CamNet} & \textbf{FilmCLIP} & \textbf{Zen-Director} \\
\midrule
Position $x$ & m & 1.84 & 1.52 & \textbf{0.73} \\
Position $y$ & m & 0.62 & 0.51 & \textbf{0.24} \\
Position $z$ & m & 1.91 & 1.63 & \textbf{0.81} \\
Pan $\phi$ & deg & 8.4 & 6.9 & \textbf{3.2} \\
Tilt $\theta$ & deg & 6.1 & 5.3 & \textbf{2.8} \\
Roll $\psi$ & deg & 4.2 & 3.7 & \textbf{1.4} \\
Focal length $f$ & mm & 18.3 & 14.2 & \textbf{6.8} \\
Aperture $a$ & f-stops & 1.42 & 1.18 & \textbf{0.54} \\
Shot duration $t$ & s & 2.81 & 2.43 & \textbf{1.07} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Human Perceptual Study}

We conducted a double-blind study in which 48 professional cinematographers evaluated paired
shot sequences generated by Zen-Director and the best competing baseline (FilmCLIP), presented
in randomized order. Raters were asked: ``Which sequence would you consider more suitable for
professional broadcast?''

Zen-Director was preferred in $73.6\%$ of pairwise comparisons ($p < 0.001$, McNemar's test).
When asked to rate each sequence independently using the 5-axis \textsc{CineScore} rubric,
$67.3\%$ of Zen-Director sequences were rated ``broadcast quality or above'' compared to
$29.8\%$ for FilmCLIP.

\subsection{Genre-Specific Performance}

\begin{table}[H]
\centering
\caption{\textsc{CineScore} by genre on the test set.}
\label{tab:genre}
\begin{tabular}{lcc}
\toprule
\textbf{Genre} & \textbf{Zen-Director} & \textbf{FilmCLIP} \\
\midrule
Drama & 76.8 & 60.4 \\
Thriller & 75.1 & 58.7 \\
Action & 73.4 & 57.2 \\
Romance & 74.9 & 62.1 \\
Horror & 72.3 & 52.8 \\
Comedy & 71.8 & 56.3 \\
Documentary & 75.6 & 63.4 \\
Science Fiction & 72.9 & 55.6 \\
\midrule
\textbf{Overall} & \textbf{74.3} & \textbf{57.9} \\
\bottomrule
\end{tabular}
\end{table}

% =============================================================================
\section{Ablation Studies}
% =============================================================================

\begin{table}[H]
\centering
\caption{Ablation study results on validation set ($n=24{,}200$ sequences).}
\label{tab:ablation}
\begin{tabular}{lccc}
\toprule
\textbf{Configuration} & \textbf{\textsc{CineScore}} & $\Delta$ & \textbf{Shot Acc.} \\
\midrule
Full model & \textbf{74.3} & --- & \textbf{91.4\%} \\
\quad -- Narrative Attention Module & 68.7 & $-5.6$ & 89.2\% \\
\quad -- Cinematographic Prior Network & 66.2 & $-8.1$ & 87.8\% \\
\quad -- Visual Layout Encoder & 65.6 & $-8.7$ & 90.1\% \\
\quad -- CPN + Visual Layout & 61.4 & $-12.9$ & 86.3\% \\
\quad -- Geodesic ori. loss (L2 instead) & 72.1 & $-2.2$ & 91.1\% \\
\quad -- B\'ezier trajectory (linear motion) & 70.8 & $-3.5$ & 91.0\% \\
\quad -- Genre conditioning in CPN & 70.2 & $-4.1$ & 89.4\% \\
\quad -- Mood conditioning in CPN & 71.9 & $-2.4$ & 90.8\% \\
NAM window $L=16$ (vs.\ $L=64$) & 71.3 & $-3.0$ & 90.5\% \\
NAM window $L=8$ & 68.9 & $-5.4$ & 89.7\% \\
\bottomrule
\end{tabular}
\end{table}

Key findings from ablation:

\begin{itemize}
    \item The \textbf{Cinematographic Prior Network} contributes the largest single gain
          ($+8.1$ points), underscoring the value of genre-aware priors that constrain the
          prediction heads toward cinematographically plausible regions of parameter space.

    \item The \textbf{Narrative Attention Module} provides consistent improvement ($+5.6$ points),
          with performance degrading monotonically as the context window $L$ is reduced. This
          confirms that shot sequencing decisions genuinely depend on long-range context.

    \item The \textbf{Geodesic orientation loss} provides a smaller but meaningful gain over
          naive Euclidean orientation regression, particularly in scenes with near-vertical tilts
          where Euler angles suffer from gimbal lock.

    \item \textbf{B\'ezier trajectory} parameterization improves motion quality notably
          ($+3.5$ points), as it inherently guarantees smooth acceleration profiles that linear
          interpolation cannot produce.
\end{itemize}

% =============================================================================
\section{Applications}
% =============================================================================

\subsection{Pre-visualization (Previz)}

Zen-Director dramatically accelerates the pre-visualization pipeline for film and television
production. A director can write or paste a scene description, specify genre and mood, and receive
a complete shot list with camera specifications in under 2 seconds. Integration with 3D previz
tools (Unreal Engine, Blender) allows the output to be directly imported as camera rigs.

\subsection{Automated Documentary Editing}

For live documentary capture where multiple cameras run simultaneously, Zen-Director can be run
in near-real-time (latency $<$ 500 ms) to suggest which camera angle to cut to, based on the
evolving narrative context and continuity rules.

\subsection{Interactive Storytelling and Games}

In interactive narratives and games, Zen-Director can generate responsive cinematography that
adapts to player choices. Given the current narrative state and scene layout, it produces
appropriate camera specifications for the game engine to execute.

\subsection{Educational Tool for Film Students}

The model's genre-conditioned priors and attention to cinematographic rules make it a useful
pedagogical tool. Film students can compare Zen-Director's suggestions against their own work
and receive explanations tied to specific cinematographic principles.

% =============================================================================
\section{Limitations}
% =============================================================================

Despite strong results, Zen-Director has several important limitations:

\begin{itemize}
    \item \textbf{Cultural specificity}: The training corpus is dominated by Hollywood and
          European cinema. Cinematographic conventions in Bollywood, Hong Kong action cinema,
          and other traditions are underrepresented and may yield lower-quality suggestions for
          content in those styles.

    \item \textbf{Text-layout gap}: In text-only mode (no layout), the model must hallucinate
          spatial configurations that are consistent with the description. This works well for
          standard interior and exterior scenes but may fail for unusual or fantastical environments.

    \item \textbf{Physics of exotic equipment}: The CPN was trained on conventional camera
          rigs. Specialized equipment (cable cameras, drone swarms, underwater housings with
          specific depth distortions) falls outside the training distribution.

    \item \textbf{Subjective evaluation}: \textsc{CineScore} reflects the aesthetic preferences
          of the 48 annotators sampled; different panels would produce different calibrations.
          The benchmark should be retested as the film industry's visual language evolves.

    \item \textbf{No pixel synthesis}: Zen-Director produces specifications, not rendered
          frames. Downstream rendering quality depends on the video generation pipeline used.
\end{itemize}

% =============================================================================
\section{Conclusion}
% =============================================================================

We have presented Zen-Director, a 759M-parameter vision-language model for cinematic scene
composition and camera control. By learning from 4.2 million annotated film shots, the model
acquires a rich representation of cinematographic conventions, enabling it to generate camera
specifications that are evaluated as broadcast-quality by professional cinematographers in
67\% of trials.

The Cinematographic Prior Network is a novel contribution that encodes genre-specific
distributions over camera parameters as a conditional normalizing flow, providing an informative
and differentiable prior that substantially improves prediction accuracy. The Narrative Attention
Module extends this per-shot reasoning to long-range sequence coherence, enabling the model to
produce shot lists with consistent visual vocabulary and appropriate pacing dynamics.

The \textsc{CineScore} benchmark, contributed alongside this work, provides a principled
five-axis evaluation framework for computational cinematography systems, grounded in professional
practice rather than proxy metrics.

Future work will integrate Zen-Director with Zen-Video to enable end-to-end text-to-cinematic-video
generation, incorporate physical simulation constraints for exotic equipment, and expand the
training corpus to cover underrepresented cinematographic traditions.

% =============================================================================
\section*{Acknowledgments}
% =============================================================================

We thank the 48 professional cinematographers who participated in annotation and evaluation,
the Hanzo infrastructure team, and the Zoo Labs research community for ongoing discussions.
Training compute was provided by the Hanzo AI H100 cluster.

% =============================================================================
% REFERENCES
% =============================================================================

\begin{thebibliography}{99}

\bibitem[Alayrac et~al.(2022)]{alayrac2022flamingo}
Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., \dots{} Zisserman, A.
\newblock Flamingo: A visual language model for few-shot learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35, 2022.

\bibitem[Bares \& Lester(1995)]{bares1995virtual}
Bares, W.H. \& Lester, J.C.
\newblock Cinematographic user models for automated realtime camera control in dynamic 3D environments.
\newblock \emph{Proceedings of the 5th International Conference on User Modeling}, 1995.

\bibitem[Baraldi et~al.(2015)]{baraldi2015shot}
Baraldi, L., Grana, C., \& Cucchiara, R.
\newblock A deep siamese network for scene detection in broadcast videos.
\newblock \emph{Proceedings of ACM Multimedia}, 2015.

\bibitem[Blattmann et~al.(2023)]{blattmann2023stable}
Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., \dots{} Rombach, R.
\newblock Stable video diffusion: Scaling latent video diffusion models to large datasets.
\newblock \emph{arXiv preprint arXiv:2311.15127}, 2023.

\bibitem[Christianson et~al.(1996)]{christianson1996declarative}
Christianson, D.B., Anderson, S.E., He, L., Salesin, D.H., Weld, D.S., \& Cohen, M.F.
\newblock Declarative camera control for automatic cinematography.
\newblock \emph{Proceedings of AAAI}, pp.~148--155, 1996.

\bibitem[Drucker \& Zeltzer(1994)]{drucker1994intelligent}
Drucker, S.M. \& Zeltzer, D.
\newblock Intelligent camera control in a virtual environment.
\newblock \emph{Graphics Interface}, pp.~190--199, 1994.

\bibitem[Galvane et~al.(2015)]{galvane2015continuity}
Galvane, Q., Ronfard, R., Lino, C., \& Christie, M.
\newblock Continuity editing for 3D animation.
\newblock \emph{Proceedings of AAAI}, pp.~753--761, 2015.

\bibitem[Gu et~al.(2018)]{gu2018ava}
Gu, C., Sun, C., Ross, D.A., Vondrick, C., Pantofaru, C., Li, Y., \dots{} Malik, J.
\newblock AVA: A video dataset of spatio-temporally localized atomic visual actions.
\newblock \emph{Proceedings of CVPR}, 2018.

\bibitem[He et~al.(2024)]{he2024camctrl}
He, H., Xu, Y., Guo, Y., Liu, G., Peng, B., Dai, B., \& Ouyang, W.
\newblock CamCtrl: Enabling camera controllability for text-to-video generation.
\newblock \emph{arXiv preprint arXiv:2404.02101}, 2024.

\bibitem[Ho et~al.(2022)]{ho2022video}
Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., \& Fleet, D.J.
\newblock Video diffusion models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35, 2022.

\bibitem[Kendall et~al.(2015)]{kendall2015posenet}
Kendall, A., Grimes, M., \& Cipolla, R.
\newblock PoseNet: A convolutional network for real-time 6-DOF camera relocalization.
\newblock \emph{Proceedings of ICCV}, 2015.

\bibitem[Li et~al.(2022)]{li2022blip}
Li, J., Li, D., Xiong, C., \& Hoi, S.
\newblock BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation.
\newblock \emph{Proceedings of ICML}, 2022.

\bibitem[Li et~al.(2023a)]{li2023blip2}
Li, J., Li, D., Savarese, S., \& Hoi, S.
\newblock BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
\newblock \emph{Proceedings of ICML}, 2023.

\bibitem[Li et~al.(2023b)]{li2023videochat}
Li, K., He, Y., Wang, Y., Li, Y., Wang, W., Luo, P., \dots{} Qiao, Y.
\newblock VideoChat: Chat-centric video understanding.
\newblock \emph{arXiv preprint arXiv:2305.06355}, 2023.

\bibitem[Loshchilov \& Hutter(2019)]{loshchilov2019decoupled}
Loshchilov, I. \& Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock \emph{Proceedings of ICLR}, 2019.

\bibitem[Maharana et~al.(2021)]{maharana2021integrating}
Maharana, A., Hannan, D., \& Bansal, M.
\newblock Integrating visuospatial, linguistic, and commonsense structure into story visualization.
\newblock \emph{Proceedings of EMNLP}, 2021.

\bibitem[Papamakarios et~al.(2021)]{papamakarios2021normalizing}
Papamakarios, G., Nalisnick, E., Rezende, D.J., Mohamed, S., \& Lakshminarayanan, B.
\newblock Normalizing flows for probabilistic modeling and inference.
\newblock \emph{Journal of Machine Learning Research}, 22(57):1--64, 2021.

\bibitem[P\'erez et~al.(2018)]{perez2018film}
P\'erez, E., Strub, F., de Vries, H., Dumoulin, V., \& Courville, A.
\newblock FiLM: Visual reasoning with a general conditioning layer.
\newblock \emph{Proceedings of AAAI}, 2018.

\bibitem[Radford et~al.(2021)]{radford2021learning}
Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., \dots{} Sutskever, I.
\newblock Learning transferable visual models from natural language supervision.
\newblock \emph{Proceedings of ICML}, 2021.

\bibitem[Su et~al.(2022)]{su2022roformer}
Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., \& Liu, Y.
\newblock RoFormer: Enhanced transformer with rotary position embedding.
\newblock \emph{Neurocomputing}, 568:127063, 2022.

\bibitem[Tapaswi et~al.(2016)]{tapaswi2016movieqa}
Tapaswi, M., Zhu, Y., Stiefelhagen, R., Torralba, A., Urtasun, R., \& Fidler, S.
\newblock MovieQA: Understanding stories in movies through question-answering.
\newblock \emph{Proceedings of CVPR}, 2016.

\bibitem[Vicol et~al.(2018)]{vicol2018moviegraphs}
Vicol, P., Tapaswi, M., Castrejon, L., \& Fidler, S.
\newblock MovieGraphs: Towards understanding human-centric situations from videos.
\newblock \emph{Proceedings of CVPR}, 2018.

\bibitem[Wang et~al.(2022)]{wang2022internvideo}
Wang, Y., Li, K., Li, Y., He, Y., Huang, B., Zhao, Z., \dots{} Qiao, Y.
\newblock InternVideo: General video foundation models via generative and discriminative learning.
\newblock \emph{arXiv preprint arXiv:2212.03191}, 2022.

\bibitem[Wu et~al.(2022)]{wu2022cinematic}
Wu, Q., Zhao, H., Chen, X., Chen, D., \& Gao, J.
\newblock Cinematic mindscapes: High-quality video reconstruction from brain activity.
\newblock \emph{arXiv preprint arXiv:2305.11280}, 2022.

\bibitem[Zhao et~al.(2023)]{zhao2023motiondirector}
Zhao, R., Gu, G., Wu, A., Zhang, D., Li, L., Liu, Z., \& Shan, Y.
\newblock MotionDirector: Motion customization of text-to-video diffusion models.
\newblock \emph{arXiv preprint arXiv:2310.08465}, 2023.

\end{thebibliography}

\end{document}
