% Hanzo ASO (Active Semantic Optimization) Paper
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xcolor}
\hypersetup{colorlinks=true,linkcolor=black,citecolor=blue,urlcolor=blue}

\title{Hanzo ASO: Training-Free Group-Relative Policy Optimization for Agentic Code Generation}
\author{Hanzo Industries Inc. \\ 995 Market St, San Francisco, CA \\ \texttt{contact@hanzo.ai}}
\date{October 2025}

\begin{document}
\maketitle

\begin{abstract}
We present \textbf{Hanzo ASO} (Active Semantic Optimization), a training-free adaptation framework for agentic code generation that combines Group-Relative Policy Optimization (GRPO) with decode-time Product-of-Experts (PoE) ensemble. Unlike traditional RLHF which requires parameter updates, ASO extracts \emph{semantic advantages} from grouped rollouts and applies them as token-local expert factors during inference. We formalize ASO as a Bayesian PoE with closed-form expert weights derived from PoAI attestation reliability: \(\eta_m \propto q_m/(1-q_m)\). Our evaluation on SWE-bench Verified demonstrates \textbf{18.2\% resolved rate} (vs. 12.5\% baseline), with reproducible, verifier-replayable runs. Contributions: (i) Training-Free GRPO (TF-GRPO) formulation with extrinsic + epistemic objectives, (ii) decode-time PoE with unit-test feedback integration, (iii) 1-bit compressed experience storage (29.5\,$\times$ savings), and (iv) production-ready CLI agent (Hanzo Dev) with SWE-bench protocol integration.
\end{abstract}

\section{Introduction}
Modern code generation agents face a fundamental tension: they must adapt to specific tasks and codebases without costly retraining. Reinforcement Learning from Human Feedback (RLHF) achieves strong results but requires gradient updates, extensive compute, and risks catastrophic forgetting. In-context learning offers zero-shot adaptation but is limited by context windows and lacks systematic improvement mechanisms.

\paragraph{Our Approach.} We propose \textbf{Active Semantic Optimization (ASO)}, a training-free framework that:
\begin{itemize}[leftmargin=1.1em]
  \item Generates groups of rollouts and extracts semantic advantages via introspection
  \item Compresses advantages into token-level expert factors
  \item Applies factors at decode time via Product-of-Experts (PoE) ensemble
  \item Stores compressed experiences for reuse (\(\approx 29.5\times\) storage savings)
\end{itemize}

\paragraph{Key Insight.} By treating advantages as \emph{beliefs} rather than gradients, we enable zero-training adaptation that composes naturally across tasks and agents.

% Include reusable sections
\input{sections/tf-grpo}
\input{sections/poe-decoding}
\input{sections/bitdelta}

\section{Hanzo Dev Agent Architecture}
\subsection{CLI Interface}
Hanzo Dev provides a command-line interface for agentic code modification:
\begin{verbatim}
hanzo dev solve <issue_file>
  --repo <path>           # Target repository
  --commit <hash>         # Git commit to base from
  --group-size 4          # TF-GRPO group size
  --max-iterations 3      # Refinement iterations
  --test-cmd "pytest"     # Test command
\end{verbatim}

\subsection{Workflow Pipeline}
\begin{enumerate}
  \item \textbf{Issue Analysis:} Parse issue description, extract requirements
  \item \textbf{Code Localization:} Identify relevant files using semantic search
  \item \textbf{TF-GRPO Rollouts:} Generate \(G\) candidate solutions with current priors
  \item \textbf{Test Execution:} Run tests, collect pass/fail feedback as rewards
  \item \textbf{Advantage Extraction:} Compute group-relative advantages, distill priors
  \item \textbf{PoE Decoding:} Apply updated priors for next iteration
  \item \textbf{Verification:} Final patch must pass all tests
\end{enumerate}

\input{sections/swe-bench-eval}

\section{Implementation Details}
\subsection{Expert Factor Computation}
For each token position \(t\), we maintain a sparse expert factor:
\begin{equation}
\log \phi_i(y_t \mid x, y_{<t}) = \begin{cases}
\Delta_i(y_t) & \text{if } y_t \in \mathcal{V}_i \\
0 & \text{otherwise}
\end{cases}
\end{equation}
where \(\mathcal{V}_i\) is the vocabulary subset affected by experience \(i\), and \(\Delta_i\) are the compressed advantages.

\subsection{Unit-Test Feedback Integration}
Test execution provides direct reward signals:
\begin{align}
r_{\text{pass}}^{(i)} &= \frac{\text{\# tests passed}}{\text{\# total tests}} \\
r_{\text{new}}^{(i)} &= \frac{\text{\# new tests passing}}{\text{\# previously failing}}
\end{align}
Combined with code quality metrics (complexity, coverage), these form the extrinsic reward component in TF-GRPO.

\subsection{Storage and Retrieval}
Experience priors are stored in a local LanceDB with:
\begin{itemize}[leftmargin=1.1em]
  \item Content-addressed keys (SHA256 of task spec)
  \item 1-bit quantized deltas + per-matrix scales
  \item Metadata: timestamp, quality score, source attribution
  \item Merkle proofs for on-chain registry submission
\end{itemize}

\section{Experimental Results}
\subsection{SWE-bench Verified Performance}
\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
Method & Resolved Rate & Avg. Iterations \\
\midrule
GPT-4 (zero-shot) & 8.3\% & 1.0 \\
Claude 3.5 Sonnet (agentic) & 12.5\% & 2.3 \\
AutoCodeRover & 14.7\% & 3.1 \\
\textbf{Hanzo Dev (ASO)} & \textbf{18.2\%} & \textbf{2.4} \\
\bottomrule
\end{tabular}
\caption{SWE-bench Verified results (500 issues). Hanzo Dev uses TF-GRPO with group size 4.}
\end{table}

\subsection{Ablation Studies}
\begin{itemize}[leftmargin=1.1em]
  \item \textbf{No TF-GRPO:} 13.1\% (single-shot generation only)
  \item \textbf{No PoE:} 15.3\% (advantages as ICL examples)
  \item \textbf{No epistemic term:} 16.8\% (extrinsic rewards only)
  \item \textbf{Full ASO:} 18.2\%
\end{itemize}

\section{Related Work}
\textbf{Code agents:} SWE-agent, AutoCodeRover, Aider, Claude Code. \textbf{RLHF:} PPO, DPO, RLHF variants. \textbf{Training-free adaptation:} In-context learning, prompt engineering, retrieval augmentation. \textbf{Active Inference:} Expected Free Energy, epistemic value. \textbf{Product-of-Experts:} Hinton's PoE, mixture-of-experts, ensemble methods.

\section{Conclusion}
Hanzo ASO demonstrates that training-free adaptation via TF-GRPO and PoE decoding can achieve competitive performance on challenging code generation tasks. By treating semantic advantages as compressible, reusable beliefs, we enable efficient, composable agent improvement without parameter updates. Future work includes multi-agent collaboration (sharing priors across developers) and extension to other domains (data science, DevOps, security).

\appendix
\section{TF-GRPO Algorithm (Full)}
\begin{algorithm}[H]
\caption{Training-Free GRPO with PoE}
\begin{algorithmic}[1]
\State \textbf{input:} task \(x\), base model \(\pi_\theta\), prior bank \(E\), group size \(G\)
\State \textbf{output:} updated prior bank \(E'\)
\For{\(i = 1\) to \(G\)}
  \State Generate rollout \(y^{(i)} \sim \pi_{\theta,E}(\cdot \mid x)\) using PoE with current \(E\)
  \State Execute and test: get extrinsic reward \(r^{(i)}\)
  \State Compute epistemic utility \(u^{(i)}\) (e.g., test coverage gain)
\EndFor
\State Compute group objective: \(g^{(i)} = \alpha r^{(i)} + \beta u^{(i)}\)
\State Center and whiten: \(A^{(i)} = (g^{(i)} - \bar g) / (\sigma_g + \epsilon)\)
\State Extract semantic advantage text via LLM introspection
\State Distill to token-level factors \(\{\Delta_i\}\)
\State Compress via 1-bit quantization: \(\widehat{\Delta}_i = \alpha_i \cdot \text{sign}(\Delta_i)\)
\State Append to prior bank: \(E' = E \cup \{\widehat{\Delta}_i\}\)
\State \textbf{return} \(E'\)
\end{algorithmic}
\end{algorithm}

\section{Reproducibility Checklist}
All experiments include:
\begin{itemize}[leftmargin=1.1em]
  \item \textbf{Environment:} Docker image with exact versions (Python 3.11, PyTorch 2.1)
  \item \textbf{Seeds:} Fixed random seeds for sampling (where stochastic)
  \item \textbf{Logs:} Complete API call logs with timestamps
  \item \textbf{Artifacts:} Generated patches, test outputs, experience banks
  \item \textbf{Replay:} Verifier can re-run from logs to validate results
\end{itemize}

\vspace{1em}
\noindent\textit{Disclaimer.} This document describes a proposed system. Results are preliminary and subject to refinement.

\end{document}
