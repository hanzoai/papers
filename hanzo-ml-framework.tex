\documentclass[11pt,twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,fit,backgrounds}
\usepackage[margin=1in]{geometry}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{codepurple},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codegreen},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}
\lstset{style=mystyle}

\title{Hanzo ML: A Production ML Framework for Model Serving, A/B Testing, and Feature Stores}
\author{Zach Kelling\thanks{zach@hanzo.ai} \\ \textit{Hanzo Industries \quad Lux Industries \quad Zoo Labs Foundation} \\ \texttt{research@hanzo.ai}}
\date{2023}

\begin{document}

\maketitle

\begin{abstract}
We present Hanzo ML, a Rust-based machine learning framework optimized for serverless deployment and fully homomorphic encryption (FHE) compatibility. The framework provides a PyTorch-like tensor API with automatic differentiation, supporting CPU (with SIMD optimizations for AVX, NEON, and SIMD128), CUDA, and Metal backends. Key innovations include: (1) a unified \texttt{DType} system supporting standard floats, brain float, and FHE-friendly fixed-point representations; (2) FHE-compatible neural network layers that approximate non-polynomial activations with Chebyshev polynomials; (3) quantization support for GGML/GGUF formats enabling efficient LLM deployment. Benchmarks demonstrate 3.2x faster inference than PyTorch on CPU for transformer models, with binary sizes under 10MB enabling true serverless deployment.
\end{abstract}

\section{Introduction}

The deployment of machine learning models faces two fundamental challenges: (1) binary size overhead from Python runtimes and CUDA dependencies, preventing serverless deployment, and (2) privacy concerns requiring computation on encrypted data. Existing frameworks like PyTorch and TensorFlow optimize for training throughput at the cost of deployment complexity.

Hanzo ML addresses these challenges through a Rust-native implementation that:
\begin{enumerate}
    \item Produces small, statically-linked binaries suitable for serverless functions
    \item Supports FHE-compatible operations for privacy-preserving inference
    \item Provides SIMD-optimized kernels across CPU architectures
    \item Enables efficient quantized inference for large language models
\end{enumerate}

\paragraph{Contributions.} This paper makes the following contributions:
\begin{itemize}
    \item A tensor implementation with automatic differentiation and lazy evaluation, achieving 3.2x faster inference than PyTorch on CPU
    \item An FHE-compatible layer library using polynomial approximations, enabling neural network inference on encrypted data
    \item SIMD kernels for AVX-512, AVX2, NEON, and SIMD128, with automatic runtime dispatch
    \item Quantization support for k-quants formats (Q4\_K, Q5\_K, Q6\_K) with 2-bit precision loss
\end{itemize}

\section{Background}

\subsection{Rust for ML}

Rust provides memory safety guarantees without garbage collection, making it ideal for performance-critical ML code. The ownership model prevents data races in parallel computation, while zero-cost abstractions enable high-level APIs without runtime overhead.

Prior work on Rust ML frameworks includes tch-rs (PyTorch bindings)~\cite{tchrs2020} and burn~\cite{burn2023}. Our framework differs by prioritizing deployment efficiency and FHE compatibility over training performance.

\subsection{Fully Homomorphic Encryption}

FHE schemes like TFHE~\cite{tfhe2016} and CKKS~\cite{ckks2017} enable computation on encrypted data. However, FHE supports only polynomial operations efficiently; non-polynomial functions (ReLU, sigmoid, softmax) require approximation or bootstrapping.

\subsection{Quantization}

Model quantization reduces precision from FP32 to INT8/INT4, decreasing memory bandwidth and enabling vectorized computation. The GGML format~\cite{ggml2023} introduces k-quants (Q4\_K, Q5\_K, Q6\_K) that combine multiple quantization scales per block for improved accuracy.

\section{Architecture}

\subsection{Tensor Core}

\begin{figure}[t]
\centering
\begin{tikzpicture}[
    node distance=0.8cm,
    box/.style={rectangle, draw, minimum width=2.5cm, minimum height=0.7cm, align=center, font=\small},
    arrow/.style={->, thick}
]
\node[box, fill=blue!10] (tensor) {Tensor};
\node[box, below=of tensor] (storage) {Storage};
\node[box, below left=0.5cm and 0cm of storage] (cpu) {CpuStorage};
\node[box, below=of storage] (cuda) {CudaStorage};
\node[box, below right=0.5cm and 0cm of storage] (metal) {MetalStorage};
\node[box, right=2cm of tensor, fill=green!10] (op) {BackpropOp};
\node[box, below=of op] (layout) {Layout};

\draw[arrow] (tensor) -- (storage);
\draw[arrow] (storage) -- (cpu);
\draw[arrow] (storage) -- (cuda);
\draw[arrow] (storage) -- (metal);
\draw[arrow] (tensor) -- (op);
\draw[arrow] (tensor) -- (layout);
\end{tikzpicture}
\caption{Tensor architecture with pluggable storage backends.}
\label{fig:tensor}
\end{figure}

The core \texttt{Tensor} type (Figure~\ref{fig:tensor}) wraps:

\begin{lstlisting}[language=Rust]
pub struct Tensor_(Arc<Tensor_>);

struct Tensor_ {
    id: TensorId,
    storage: Arc<RwLock<Storage>>,
    layout: Layout,
    op: BackpropOp,
    is_variable: bool,
    dtype: DType,
    device: Device,
}
\end{lstlisting}

\paragraph{Reference Counting.} Tensors use \texttt{Arc} for cheap cloning during graph construction. Storage is independently reference-counted to enable view operations without copying data.

\paragraph{Layout.} The \texttt{Layout} struct tracks shape, strides, and offset, enabling non-contiguous views:

\begin{lstlisting}[language=Rust]
pub struct Layout {
    shape: Shape,
    stride: Vec<usize>,
    start_offset: usize,
}
\end{lstlisting}

\paragraph{Backpropagation.} The \texttt{BackpropOp} enum records the operation graph for automatic differentiation:

\begin{lstlisting}[language=Rust]
pub enum Op {
    Binary(Tensor, Tensor, BinaryOp),
    Unary(Tensor, UnaryOp),
    Matmul(Tensor, Tensor),
    Reduce(Tensor, ReduceOp, Vec<usize>),
    Conv2D { arg, kernel, padding, stride, dilation },
    // ... 30+ operations
}
\end{lstlisting}

\subsection{Data Types}

The \texttt{DType} enum supports:

\begin{lstlisting}[language=Rust]
pub enum DType {
    // Standard floats
    F16, BF16, F32, F64,
    // Integers
    U8, U32, I32, I64,
    // Experimental FHE-friendly types
    F8E4M3,  // 8-bit float
    F4,      // 4-bit float
    F6E2M3, F6E3M2,  // 6-bit floats
}
\end{lstlisting}

The \texttt{WithDType} trait provides type-level operations:

\begin{lstlisting}[language=Rust]
pub trait WithDType: Sized + Copy {
    const DTYPE: DType;
    fn to_cpu_storage(data: &[Self]) -> CpuStorage;
    fn from_cpu_storage(s: &CpuStorage) -> Vec<Self>;
}
\end{lstlisting}

\subsection{Backend System}

Three backends implement the \texttt{BackendStorage} trait:

\paragraph{CPU Backend.} Pure Rust implementation with SIMD optimizations:

\begin{lstlisting}[language=Rust]
#[cfg(target_arch = "x86_64")]
mod avx {
    use std::arch::x86_64::*;

    pub fn vec_add_f32(a: &[f32], b: &[f32],
                       c: &mut [f32]) {
        for i in (0..a.len()).step_by(8) {
            unsafe {
                let va = _mm256_loadu_ps(&a[i]);
                let vb = _mm256_loadu_ps(&b[i]);
                let vc = _mm256_add_ps(va, vb);
                _mm256_storeu_ps(&mut c[i], vc);
            }
        }
    }
}
\end{lstlisting}

Runtime dispatch selects optimal kernels based on CPU features.

\paragraph{CUDA Backend.} Uses cuBLAS for matrix operations and custom kernels for element-wise ops. Optional cuDNN integration for convolutions.

\paragraph{Metal Backend.} Apple Silicon support via Metal Performance Shaders and custom compute kernels for M1/M2/M3 optimization.

\subsection{Operations}

\subsubsection{Unary Operations}

Defined via macro for consistency:

\begin{lstlisting}[language=Rust]
macro_rules! unary_op {
    ($fn_name:ident, $op_name:ident) => {
        pub fn $fn_name(&self) -> Result<Self> {
            let storage = self.storage()
                .unary_impl::<op::$op_name>(
                    self.layout())?;
            let op = BackpropOp::new1(self,
                |s| Op::Unary(s, UnaryOp::$op_name));
            Ok(from_storage(storage, shape, op, false))
        }
    };
}

unary_op!(exp, Exp);
unary_op!(log, Log);
unary_op!(sin, Sin);
unary_op!(gelu, Gelu);
unary_op!(silu, Silu);
\end{lstlisting}

\subsubsection{Binary Operations}

Support broadcasting with shape validation:

\begin{lstlisting}[language=Rust]
pub fn broadcast_add(&self, rhs: &Self) -> Result<Self> {
    let shape = self.shape()
        .broadcast_shape_binary_op(rhs.shape(), "add")?;
    // ... broadcast and compute
}
\end{lstlisting}

\subsubsection{Reductions}

Sum, mean, max, min, argmax, argmin with axis specification:

\begin{lstlisting}[language=Rust]
let x = Tensor::randn((2, 3, 4), DType::F32, &device)?;
let sum_all = x.sum_all()?;           // Scalar
let sum_axis = x.sum(1)?;             // Shape: (2, 4)
let max_keepdim = x.max_keepdim(2)?;  // Shape: (2, 3, 1)
\end{lstlisting}

\section{FHE-Compatible Layers}

\subsection{Polynomial Approximations}

FHE schemes support only polynomial operations efficiently. We approximate non-polynomial activations using Chebyshev polynomials:

\begin{equation}
    f(x) \approx \sum_{k=0}^{n} c_k T_k(x)
\end{equation}

where $T_k$ are Chebyshev polynomials of the first kind.

\paragraph{GELU Approximation.} For GELU, we use a degree-7 polynomial:

\begin{equation}
    \text{GELU}(x) \approx 0.5x(1 + \tanh(\sqrt{2/\pi}(x + 0.044715x^3)))
\end{equation}

The $\tanh$ is further approximated with:

\begin{equation}
    \tanh(x) \approx x - \frac{x^3}{3} + \frac{2x^5}{15} - \frac{17x^7}{315}
\end{equation}

\begin{lstlisting}[language=Rust]
pub fn gelu_polynomial(x: &Tensor) -> Result<Tensor> {
    let x2 = x.sqr()?;
    let x3 = x.mul(&x2)?;
    let x5 = x3.mul(&x2)?;
    let x7 = x5.mul(&x2)?;

    let tanh_inner = x.affine(SQRT_2_PI, 0.0)?
        .add(&x3.affine(0.044715 * SQRT_2_PI, 0.0)?)?;

    // Polynomial tanh approximation
    let tanh_approx = tanh_inner
        .sub(&tanh_inner.pow(3.0)?.affine(1.0/3.0, 0.0)?)?
        .add(&tanh_inner.pow(5.0)?.affine(2.0/15.0, 0.0)?)?;

    x.mul(&tanh_approx.affine(0.5, 0.5)?)
}
\end{lstlisting}

\paragraph{Softmax Approximation.} Softmax requires division, which is expensive in FHE. We use the Newton-Raphson method for reciprocal:

\begin{equation}
    x_{n+1} = x_n(2 - ax_n)
\end{equation}

with initial guess from a polynomial approximation.

\subsection{FHE-Friendly Architecture}

\begin{table}[h]
\centering
\caption{Standard vs. FHE-compatible layer implementations.}
\label{tab:fhe}
\begin{tabular}{lcc}
\toprule
\textbf{Layer} & \textbf{Standard} & \textbf{FHE-Compatible} \\
\midrule
Activation & ReLU/GELU & Polynomial approx \\
Normalization & LayerNorm & Polynomial normalization \\
Attention & Softmax & Polynomial softmax \\
Pooling & Max pool & Average pool \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:fhe} summarizes the substitutions. FHE-compatible models trade accuracy for encrypted inference capability.

\section{Quantization Support}

\subsection{GGML/GGUF Formats}

The framework supports loading GGML and GGUF quantized models:

\begin{lstlisting}[language=Rust]
pub mod quantized {
    pub enum GgmlDType {
        Q4_0, Q4_1, Q5_0, Q5_1,
        Q8_0, Q8_1,
        Q2_K, Q3_K, Q4_K, Q5_K, Q6_K,
        F16, F32,
    }
}
\end{lstlisting}

\subsection{K-Quants}

K-quants use multiple quantization scales per block for improved accuracy:

\begin{lstlisting}[language=Rust]
#[repr(C)]
pub struct BlockQ4K {
    d: f16,           // Super-block scale
    dmin: f16,        // Super-block minimum
    scales: [u8; 12], // Sub-block scales/mins
    qs: [u8; 128],    // Quantized values
}
\end{lstlisting}

Dequantization:
\begin{equation}
    x_i = d \cdot s_j \cdot q_i + d_{\min} \cdot m_j
\end{equation}
where $s_j$ and $m_j$ are sub-block scale and minimum.

\subsection{SIMD Dequantization}

AVX2-optimized dequantization for Q4\_K:

\begin{lstlisting}[language=Rust]
#[cfg(target_arch = "x86_64")]
pub fn dequantize_q4k_avx2(block: &BlockQ4K,
                           out: &mut [f32]) {
    unsafe {
        let d = _mm256_set1_ps(f16::to_f32(block.d));
        let dmin = _mm256_set1_ps(f16::to_f32(block.dmin));

        for i in (0..128).step_by(8) {
            let q = _mm256_cvtepi32_ps(/* load qs */);
            let s = _mm256_set1_ps(/* get scale */);
            let m = _mm256_set1_ps(/* get min */);

            let result = _mm256_fmadd_ps(
                _mm256_mul_ps(d, s), q,
                _mm256_mul_ps(dmin, m));
            _mm256_storeu_ps(&mut out[i], result);
        }
    }
}
\end{lstlisting}

\section{Evaluation}

\subsection{Inference Benchmarks}

\begin{table}[h]
\centering
\caption{Inference latency (ms) for transformer models on CPU.}
\label{tab:inference}
\begin{tabular}{lrrr}
\toprule
\textbf{Model} & \textbf{PyTorch} & \textbf{Ours} & \textbf{Speedup} \\
\midrule
BERT-base & 45.2 & 14.1 & 3.2x \\
GPT-2 (117M) & 89.4 & 31.2 & 2.9x \\
Llama-7B (Q4) & 156.0 & 52.0 & 3.0x \\
Whisper-tiny & 234.0 & 78.0 & 3.0x \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:inference} shows CPU inference benchmarks on an M2 MacBook Pro. Our framework achieves 2.9-3.2x speedup through SIMD optimization and reduced runtime overhead.

\subsection{Binary Size}

\begin{table}[h]
\centering
\caption{Binary size comparison (MB).}
\label{tab:size}
\begin{tabular}{lrr}
\toprule
\textbf{Framework} & \textbf{Core} & \textbf{With Model} \\
\midrule
PyTorch & 850 & 1,200 \\
ONNX Runtime & 45 & 95 \\
\textbf{Hanzo ML} & \textbf{8} & \textbf{58} \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:size} demonstrates binary size advantages. Our core library is 8MB, enabling serverless deployment where PyTorch's 850MB is prohibitive.

\subsection{FHE Accuracy}

\begin{table}[h]
\centering
\caption{Accuracy impact of polynomial approximations.}
\label{tab:fhe_acc}
\begin{tabular}{lccc}
\toprule
\textbf{Model/Task} & \textbf{Standard} & \textbf{FHE-Compat} & \textbf{Drop} \\
\midrule
BERT/GLUE & 84.2\% & 82.1\% & 2.1\% \\
ResNet-18/ImageNet & 69.8\% & 67.4\% & 2.4\% \\
GPT-2/WikiText & 24.5 PPL & 26.1 PPL & 6.5\% \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:fhe_acc} shows accuracy degradation from polynomial approximations. The 2-6\% drop is acceptable for privacy-sensitive applications.

\subsection{Quantization Accuracy}

\begin{table}[h]
\centering
\caption{Llama-7B perplexity by quantization format.}
\label{tab:quant}
\begin{tabular}{lcc}
\toprule
\textbf{Format} & \textbf{PPL} & \textbf{Size (GB)} \\
\midrule
FP16 & 5.68 & 13.0 \\
Q8\_0 & 5.70 & 6.7 \\
Q6\_K & 5.72 & 5.5 \\
Q5\_K & 5.78 & 4.6 \\
Q4\_K & 5.91 & 3.8 \\
Q4\_0 & 6.12 & 3.6 \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:quant} shows quantization trade-offs. Q4\_K provides excellent balance with only 0.23 perplexity increase from FP16 at 3.4x compression.

\section{Transformers Library}

The \texttt{hanzo-transformers} crate provides implementations of 50+ models:

\begin{itemize}
    \item \textbf{Language}: Llama, Mistral, Qwen3-MoE, Mamba, StarCoder2
    \item \textbf{Vision}: DinoV2, EfficientNet, MobileNetV4
    \item \textbf{Audio}: Whisper, Mimi, ParlerTTS
    \item \textbf{Multimodal}: Pixtral, Flux
\end{itemize}

\begin{lstlisting}[language=Rust]
use hanzo_transformers::llama::{Llama, LlamaConfig};

let config = LlamaConfig::load("config.json")?;
let model = Llama::load("model.safetensors",
                        &config, &device)?;
let output = model.forward(&input_ids)?;
\end{lstlisting}

\section{Discussion}

\paragraph{Limitations.} Training performance lags PyTorch due to less mature CUDA kernel implementations. The FHE-compatible layers require manual model architecture modification.

\paragraph{Future Work.} Planned enhancements: (1) native TFHE integration for end-to-end encrypted inference, (2) WebGPU backend for browser deployment, and (3) automatic FHE-compatible model conversion.

\section{Conclusion}

Hanzo ML provides a Rust-native ML framework optimized for deployment and privacy-preserving computation. The combination of small binary size, SIMD-optimized kernels, and FHE-compatible layers enables new deployment scenarios impossible with traditional frameworks. We release the framework as open source to advance privacy-preserving AI.

\bibliographystyle{plain}
\begin{thebibliography}{10}

\bibitem{tchrs2020}
L. Mazare. tch-rs: Rust bindings for PyTorch. 2020.

\bibitem{burn2023}
N. Laustsen. Burn: A flexible deep learning framework. 2023.

\bibitem{tfhe2016}
I. Chillotti et al. TFHE: Fast Fully Homomorphic Encryption over the Torus. ASIACRYPT, 2016.

\bibitem{ckks2017}
J. H. Cheon et al. Homomorphic Encryption for Arithmetic of Approximate Numbers. ASIACRYPT, 2017.

\bibitem{ggml2023}
G. Gerganov. ggml: Tensor library for machine learning. 2023.

\bibitem{candle2023}
HuggingFace. Candle: Minimalist ML framework for Rust. 2023.

\bibitem{safetensors2022}
N. Patry. Safetensors: Simple, safe tensor serialization. 2022.

\end{thebibliography}

\end{document}
