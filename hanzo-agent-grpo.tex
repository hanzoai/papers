% Hanzo Agent-GRPO Paper
% Fully self-contained -- no \input{} directives
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xcolor}
\usepackage{natbib}
\usepackage{float}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{multirow}

\hypersetup{colorlinks=true,linkcolor=black,citecolor=blue,urlcolor=blue}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{assumption}{Assumption}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\KL}{KL}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\softmax}{softmax}
\DeclareMathOperator{\cosim}{sim}

\title{Training-Free Continuous Learning for Tool-Using Agents\\
via GRPO Semantic Experience Management}
\author{
    Hanzo AI Research\\
    \textit{Hanzo AI Inc (Techstars '17), Los Angeles, CA}\\
    \texttt{research@hanzo.ai}
}
\date{February 2026}

\begin{document}
\maketitle

% ============================================================================
% ABSTRACT
% ============================================================================
\begin{abstract}
We present \textbf{Agent-GRPO}, a training-free continuous learning framework
that extends Group-Relative Policy Optimization (GRPO) from text-output
optimization to the agent tool-use setting. While Active Semantic Optimization
(ASO) extracts generalizable patterns from grouped language model rollouts,
agent trajectories carry strictly richer structure: sequences of tool
selections, parameter choices, intermediate outputs, and explicit success or
failure signals that are unavailable from text alone. Agent-GRPO adapts the
three-stage semantic extraction pipeline of \citet{tencent2025youtoagent} to
this richer domain, constructing a \emph{semantic experience library}
$\mathcal{E}$ whose entries encode tool-use strategies, failure patterns, and
domain conventions distilled from past agent sessions.

We formalize the tool-use trajectory as a typed tuple
$\tau = \bigl(x,\,\{(t_i, \bm{p}_i, o_i, s_i)\}_{i=1}^K,\, R\bigr)$, define
a composite reward combining binary tool success and user acceptance signals,
and prove that the expected fraction of beneficial experiences in $\mathcal{E}$
grows monotonically under mild regularity conditions (Theorem~\ref{thm:monotone}).
We further derive a sample complexity bound of $O(K\,/\,\varepsilon^2)$ sessions
to achieve $\varepsilon$-improvement over the baseline (Theorem~\ref{thm:sample}).

Empirically, a Hanzo Dev agent equipped with Agent-GRPO achieves a
\textbf{31.4\%} reduction in tool-call errors and a \textbf{22.7\%} improvement
in task completion rate on the HB-Bench tool-use benchmark after 50 sessions of
online experience collection, at a total adaptation cost below \$20 in API
calls---orders of magnitude cheaper than any fine-tuning alternative. We also
show how Agent-GRPO integrates naturally with the four-loop self-improvement
architecture of ASO, making the feedback loop between telemetry, active
learning, reflection, and maintenance fully closed.
\end{abstract}

% ============================================================================
% 1. INTRODUCTION
% ============================================================================
\section{Introduction}
\label{sec:intro}

Tool-using agents---systems that interact with external tools such as code
interpreters, file systems, search engines, and APIs---have emerged as a
primary deployment paradigm for large language models
\citep{schick2023toolformer, yang2024intercode, yao2023react}. Unlike pure
text-generation tasks, agent behavior is evaluated not merely on the quality of
prose but on the correctness of tool invocations, the appropriateness of tool
selection, and the agent's ability to recover from errors encountered during
execution. This richer evaluation surface creates both new challenges and new
opportunities for adaptation.

\paragraph{The Adaptation Gap for Agents.}
Existing continuous learning works for LLMs focus on text-quality
metrics---pass rates on coding benchmarks, BLEU scores, preference rankings.
The Active Semantic Optimization (ASO) framework~\citep{hanzo2026aso} takes a
training-free approach, extracting semantic advantages from grouped rollouts and
applying them as Product-of-Experts decoding factors. Yet ASO and its
predecessors treat the model output as a monolithic text sequence. An agent
trajectory is structurally different: it is a \emph{sequence of decisions}
interleaved with environmental feedback, where each decision (which tool to
call, with what parameters) has an observable outcome that serves as a rich
local reward signal.

This structural difference is consequential. Consider an agent that repeatedly
chooses \texttt{bash} to parse JSON when a dedicated \texttt{read\_json} tool
would be faster, more reliable, and incur lower token cost. This failure to
prefer the specialized tool will not manifest as a text-quality degradation
that text-level GRPO would catch; the output may look correct even when the
tool choice was suboptimal. Conversely, an agent that learns to prefer
\texttt{read\_json} for JSON parsing can apply this lesson to future tasks
without any gradient update---provided the lesson is properly encoded and
retrieved.

\paragraph{The Semantic Experience Library.}
Our central contribution is the \textbf{semantic experience library}
$\mathcal{E}$, a persistent, embedding-indexed store of natural-language
experiences extracted from agent trajectories via an adapted GRPO pipeline.
Each entry $e \in \mathcal{E}$ encodes a reusable lesson: a statement about
when to use which tool, how to structure parameters for a particular API, or
which failure patterns to anticipate in a given domain. At the start of each
new session, the top-$K$ experiences most semantically similar to the incoming
task are injected into the agent's system prompt, providing explicit behavioral
guidance without modifying model weights.

\paragraph{Agent-GRPO.}
We formalize this approach as \textbf{Agent-GRPO}, which extends
Training-Free GRPO (TF-GRPO)~\citep{hanzo2026aso} to agent trajectories through
three technical innovations:

\begin{enumerate}[leftmargin=1.5em]
    \item \textbf{Trajectory-level rewards:} A composite reward function
          combining binary tool success signals and time-delayed user acceptance
          signals, aggregated over the full trajectory.
    \item \textbf{Three-stage extraction pipeline:} Adapted from
          \citet{tencent2025youtoagent}, the pipeline summarizes individual
          trajectories (Stage 1), extracts group advantages by comparing
          successful and unsuccessful groups (Stage 2), and consolidates batch
          operations to produce experience library updates (Stage 3).
    \item \textbf{Memory management:} Four compression strategies---diversity,
          importance, temporal decay, and hybrid---that maintain library quality
          as $|\mathcal{E}|$ grows, preventing staleness and redundancy while
          preserving high-utility experiences.
\end{enumerate}

\paragraph{Contributions.}
\begin{enumerate}[leftmargin=1.5em]
    \item A formal model of agent tool-use trajectories as typed tuples with
          observable success signals (Section~\ref{sec:trajectories}).
    \item A composite reward function and group-relative advantage computation
          adapted for agent trajectories (Section~\ref{sec:reward}).
    \item The complete Agent-GRPO algorithm with formal pseudocode and
          correctness guarantees (Section~\ref{sec:algorithm}).
    \item A semantic memory manager with four compression strategies and
          provably monotone quality improvement (Sections~\ref{sec:memory}
          and~\ref{sec:theory}).
    \item Integration with ASO's four-loop self-improvement architecture, closing
          the feedback loop from telemetry to library update
          (Section~\ref{sec:integration}).
    \item Empirical evaluation demonstrating state-of-the-art results on
          HB-Bench at sub-\$20 adaptation cost (Section~\ref{sec:experiments}).
\end{enumerate}

\paragraph{Paper Outline.}
Section~\ref{sec:background} reviews GRPO, TF-GRPO, and the YouTou-Agent
pipeline. Section~\ref{sec:agentgrpo} formalizes Agent-GRPO.
Section~\ref{sec:memory} presents the semantic memory manager.
Section~\ref{sec:adaptations} describes agent-specific adaptations.
Section~\ref{sec:integration} describes integration with self-improvement loops.
Section~\ref{sec:theory} provides theoretical analysis.
Section~\ref{sec:experiments} presents empirical evaluation.
Section~\ref{sec:related} discusses related work.
Section~\ref{sec:conclusion} concludes.

% ============================================================================
% 2. BACKGROUND
% ============================================================================
\section{Background}
\label{sec:background}

\subsection{Group-Relative Policy Optimization}

GRPO~\citep{shao2024deepseekmath} extends the REINFORCE
algorithm~\citep{williams1992simple} by computing advantages relative to a
group of simultaneously generated rollouts, eliminating the need for a learned
value baseline. Given task $x$, a group of $G$ rollouts
$\{y^{(i)}\}_{i=1}^G \sim \pi_\theta(\cdot \mid x)$, and associated scalar
rewards $\{r^{(i)}\}_{i=1}^G$, the group-relative advantage is:
\begin{equation}
    A^{(i)} = \frac{r^{(i)} - \mu_G}{\sigma_G + \epsilon},
    \quad \mu_G = \frac{1}{G}\sum_{j=1}^G r^{(j)},
    \quad \sigma_G = \sqrt{\frac{1}{G-1}\sum_{j=1}^G (r^{(j)} - \mu_G)^2},
    \label{eq:grpo-advantage}
\end{equation}
with $\epsilon = 10^{-8}$ for numerical stability. Standard GRPO then performs
a clipped policy gradient update on $\theta$:
\begin{equation}
    \mathcal{L}_{\text{GRPO}}(\theta) = -\frac{1}{G}\sum_{i=1}^G \min\!\left(
        \rho^{(i)} A^{(i)},\; \text{clip}(\rho^{(i)}, 1{-}\delta, 1{+}\delta) A^{(i)}
    \right) + \lambda_{\text{KL}}\,\KL\bigl[\pi_\theta \| \pi_{\text{ref}}\bigr],
    \label{eq:grpo-loss}
\end{equation}
where $\rho^{(i)} = \pi_\theta(y^{(i)} \mid x) / \pi_{\text{old}}(y^{(i)} \mid x)$
is the importance weight and $\pi_{\text{ref}}$ is a reference policy.

\subsection{Training-Free GRPO (TF-GRPO)}

TF-GRPO~\citep{hanzo2026aso} replaces the gradient update in
Equation~\eqref{eq:grpo-loss} with a semantic extraction and compression
pipeline. The key insight is that the advantages $\{A^{(i)}\}$ encode
\emph{what makes one solution better than another} in a form that can be
distilled into reusable natural-language patterns, called \emph{semantic
advantages}:
\begin{equation}
    S^{(i)} = \mathcal{F}_{\text{LLM}}\!\left(x,\;\{y^{(j)}, r^{(j)}\}_{j=1}^G,\; i\right),
    \label{eq:semantic-advantage}
\end{equation}
where $\mathcal{F}_{\text{LLM}}$ denotes an LLM-based extraction call that
produces a structured text description of why rollout $i$ outperformed or
underperformed the group mean.

These semantic advantages are then compressed into 1-bit token-level expert
factors and applied at decode time via a Product-of-Experts (PoE)
distribution~\citep{hinton2002training}:
\begin{equation}
    \pi_{\text{ASO}}(y_t \mid x, y_{<t}) \propto \pi_\theta(y_t \mid x, y_{<t})
    \prod_{m=1}^{M} \phi_m(y_t \mid x, y_{<t})^{\eta_m},
    \label{eq:poe}
\end{equation}
where $\eta_m = \log(q_m / (1 - q_m))$ and $q_m \in (0,1)$ is the quality
attestation score for prior $m$. The composite reward used in ASO is:
\begin{equation}
    g^{(i)} = \alpha\, r^{(i)} + \beta\, u^{(i)},
    \label{eq:aso-reward}
\end{equation}
where $r^{(i)}$ is the extrinsic reward (test pass rate) and $u^{(i)}$ is the
epistemic reward (solution diversity and novelty).

\subsection{The YouTou-Agent Three-Stage Pipeline}

\citet{tencent2025youtoagent} propose a three-stage pipeline for extracting
reusable experiences from groups of LLM trajectories:

\begin{description}[leftmargin=1.5em, font=\normalfont\bfseries]
    \item[Stage 1 -- Trajectory Summarization.]
    Each individual trajectory $\tau^{(i)}$ in a group is summarized into a
    structured description of the reasoning steps, decisions, and outcomes. The
    summary is constrained to at most 32 words and must identify the key
    decision that differentiated this trajectory from a baseline.

    \item[Stage 2 -- Group Advantage Extraction.]
    Given $G$ summaries from Stage~1, the group is analyzed as a unit. The
    extractor identifies patterns that distinguish high-reward trajectories from
    low-reward ones, and produces \emph{operations} on an experience library
    $\mathcal{E}$: \textsc{add} new experiences, \textsc{modify} existing ones,
    \textsc{delete} outdated entries, or \textsc{merge} near-duplicate entries.

    \item[Stage 3 -- Batch Consolidation.]
    Multiple Stage-2 outputs (from different task groups within a session) are
    consolidated to remove redundancies, resolve conflicts, and ensure no single
    experience exceeds 32 words. The final set of operations is applied atomically
    to $\mathcal{E}$.
\end{description}

The original YouTou-Agent framework applies this pipeline to text-only LLM
outputs. Agent-GRPO adapts all three stages to the richer structure of
tool-use trajectories.

\subsection{Agent Trajectories vs.\ Text Trajectories}
\label{sec:traj-comparison}

A text-only trajectory $\tau_{\text{text}} = (x, y, r)$ consists of a task
$x$, a text output $y$, and a scalar reward $r$. An agent trajectory carries
strictly more information:

\begin{itemize}[leftmargin=1.5em]
    \item \textbf{Tool selection sequence:} which tools were called, and in what
          order.
    \item \textbf{Parameter choices:} the arguments passed to each tool,
          reflecting the agent's understanding of the task.
    \item \textbf{Intermediate outputs:} the return values of each tool call,
          observable during execution.
    \item \textbf{Per-step success signals:} whether each tool call succeeded
          or raised an error, providing dense local reward information.
    \item \textbf{Recovery patterns:} how the agent responded to errors---by
          retrying, switching tools, or escalating.
\end{itemize}

This richer signal structure makes agent trajectories both more informative
and more complex to process. The three-stage pipeline must be extended to
handle sequences of (tool, parameters, output, success) tuples rather than
single text outputs.

% ============================================================================
% 3. AGENT-LEVEL GRPO
% ============================================================================
\section{Agent-Level GRPO}
\label{sec:agentgrpo}

\subsection{Tool-Use Trajectory Definition}
\label{sec:trajectories}

\begin{definition}[Tool-Use Trajectory]
\label{def:trajectory}
A tool-use trajectory is a tuple
\begin{equation}
    \tau = \bigl(x,\;\bm{\xi},\;R\bigr),
    \quad \bm{\xi} = \bigl((t_1, \bm{p}_1, o_1, s_1),\;\ldots,\;(t_K, \bm{p}_K, o_K, s_K)\bigr),
    \label{eq:trajectory}
\end{equation}
where:
\begin{itemize}[leftmargin=1.5em]
    \item $x \in \mathcal{X}$ is the natural-language task description,
    \item $K \geq 0$ is the number of tool calls made,
    \item $t_i \in \mathcal{T}$ is the name of the $i$-th tool invoked from
          tool registry $\mathcal{T}$,
    \item $\bm{p}_i \in \mathcal{P}(t_i)$ is the parameter dictionary for
          tool $t_i$,
    \item $o_i \in \mathcal{O}$ is the output returned by tool $t_i$,
    \item $s_i \in \{0, 1\}$ is the binary success indicator ($s_i = 1$ iff
          the call completed without error and produced well-formed output),
    \item $R \in [0, 1]$ is the trajectory-level composite reward defined in
          Section~\ref{sec:reward}.
\end{itemize}
\end{definition}

\begin{remark}
The length $K$ is not fixed a priori; it varies across trajectories in the
same group. This is fundamentally different from the text setting where all
rollouts have a common output space $\mathcal{V}^*$. Agent-GRPO must compare
trajectories of potentially different lengths, which motivates the
summarization approach in Stage~1.
\end{remark}

Let $\mathcal{T}$ be a finite registry of $|\mathcal{T}|$ available tools.
We partition $\mathcal{T}$ into \emph{domains} $\mathcal{D} =
\{d_1, \ldots, d_L\}$ based on functionality: $d_{\text{fs}}$ (file system),
$d_{\text{shell}}$ (shell execution), $d_{\text{search}}$ (web/code search),
$d_{\text{k8s}}$ (Kubernetes operations), and so on. Domain membership informs
the scoped retrieval of experiences (Section~\ref{sec:retrieval}).

\subsection{Reward Computation}
\label{sec:reward}

The trajectory-level reward $R$ decomposes into two observable components:

\paragraph{Tool Success Rate.}
The per-trajectory tool success rate is the fraction of tool calls that
completed without error:
\begin{equation}
    \rho_s(\tau) = \frac{1}{K} \sum_{i=1}^K s_i \in [0, 1].
    \label{eq:tool-success}
\end{equation}
For the degenerate case $K = 0$ (task completed without tool use), we set
$\rho_s = 1$.

\paragraph{User Acceptance Signal.}
The user acceptance signal $\rho_a(\tau) \in \{0, 1\}$ is determined by
observing whether the user corrects or rejects the agent's final output within
a time window $\Delta t$:
\begin{equation}
    \rho_a(\tau) =
    \begin{cases}
        1 & \text{if user does not correct output within } \Delta t = 120\,\text{s,} \\
        0 & \text{if user explicitly corrects or rejects output.}
    \end{cases}
    \label{eq:user-accept}
\end{equation}
This weak, delayed supervision signal is the only human signal required by
Agent-GRPO; no explicit reward labeling is needed.

\paragraph{Composite Reward.}
The composite trajectory reward is:
\begin{equation}
    R(\tau) = w_1\,\rho_s(\tau) + w_2\,\rho_a(\tau),
    \label{eq:composite-reward}
\end{equation}
with default weights $w_1 = 0.4$, $w_2 = 0.6$ (user acceptance is weighted
more heavily as it reflects task-level quality, not just mechanical correctness).

\begin{remark}
The weights $w_1$ and $w_2$ can be tuned per deployment. In automated
pipelines without a human in the loop, one may set $w_2 = 0$ and rely
entirely on $\rho_s$. In interactive settings, $w_2 > 0$ captures the
user's implicit preference signal.
\end{remark}

\paragraph{Step-Level Rewards.}
For Stage-1 summarization (Section~\ref{sec:stage1}), we also compute step-level
rewards $r_i = s_i$ for each tool call $i$, which enable the extractor to
identify exactly which calls were beneficial or harmful.

\subsection{Group-Relative Advantage for Agent Trajectories}

Given a group of $G$ trajectories $\{\tau^{(j)}\}_{j=1}^G$ on the same task
$x$, the group-relative advantage of trajectory $\tau^{(i)}$ is defined
analogously to Equation~\eqref{eq:grpo-advantage}:
\begin{equation}
    A^{(i)} = \frac{R(\tau^{(i)}) - \mu_G}{\sigma_G + \epsilon},
    \quad \mu_G = \frac{1}{G}\sum_{j=1}^G R(\tau^{(j)}),
    \quad \sigma_G = \sqrt{\frac{1}{G-1}\sum_{j=1}^G \bigl(R(\tau^{(j)}) - \mu_G\bigr)^2}.
    \label{eq:agent-advantage}
\end{equation}
Trajectories with $A^{(i)} > 0$ are identified as ``positive examples'' for
Stage-2 extraction; those with $A^{(i)} < 0$ are ``negative examples.'' The
group is required to contain at least one positive and one negative example for
meaningful advantage extraction; groups where all trajectories achieve the same
reward are discarded.

\subsection{Stage 1: Tool Trajectory Summarization}
\label{sec:stage1}

Stage~1 processes each trajectory $\tau^{(i)}$ independently, producing a
structured summary $\hat{\tau}^{(i)}$ suitable for group comparison.

\begin{definition}[Trajectory Summary]
A trajectory summary $\hat{\tau}$ is a natural-language description constrained
to at most $W_{\max} = 64$ words that identifies: (a) the task type and domain,
(b) the primary tool selection strategy employed, (c) the most significant
parameter choice that affected outcome, and (d) the root cause of any failures.
\end{definition}

The summary is produced by the following LLM call:

\begin{quote}
\textit{Prompt template for Stage 1:}
\texttt{You are analyzing a tool-use trajectory for task: \{x\}. The
trajectory made \{K\} tool calls with overall reward \{R\}. The step-by-step
calls were: \{$\xi$\}. Summarize in at most 64 words: (1) what tool selection
strategy was used, (2) what parameter choices were made for key tools, (3) what
failures occurred and why, (4) what the outcome was. Focus on patterns
generalizable to similar future tasks.}
\end{quote}

The summary $\hat{\tau}^{(i)}$ retains the advantage sign: we annotate it as
$\hat{\tau}^{(i)+}$ if $A^{(i)} > 0$ and $\hat{\tau}^{(i)-}$ if $A^{(i)} < 0$.

\subsection{Stage 2: Group Advantage Extraction}
\label{sec:stage2}

Stage~2 operates on the full group of summaries $\{\hat{\tau}^{(j)}\}_{j=1}^G$
and produces a set of \emph{operations} $\Omega^{(g)}$ on the experience library.

\begin{definition}[Experience Library Operation]
\label{def:operation}
An operation $\omega \in \Omega$ is one of:
\begin{itemize}[leftmargin=1.5em]
    \item $\textsc{add}(e_{\text{new}})$: insert a new experience $e_{\text{new}}$ into $\mathcal{E}$,
    \item $\textsc{modify}(e_{\text{id}}, e_{\text{new}})$: replace experience with identifier $e_{\text{id}}$,
    \item $\textsc{delete}(e_{\text{id}})$: remove experience $e_{\text{id}}$ from $\mathcal{E}$,
    \item $\textsc{merge}(e_{\text{id}_1}, e_{\text{id}_2}, e_{\text{new}})$: replace two near-duplicate
          experiences with a single consolidated entry.
\end{itemize}
\end{definition}

The Stage-2 LLM prompt compares positive and negative group summaries:

\begin{quote}
\textit{Prompt template for Stage 2:}
\texttt{You are extracting generalizable tool-use lessons. Below are
SUCCESSFUL trajectories (high reward): \{$\hat{\tau}^{(i)+}$\}. And FAILED
trajectories (low reward): \{$\hat{\tau}^{(i)-}$\}. Current experience library:
\{$\mathcal{E}_{\text{relevant}}$\}. Produce operations (add/modify/delete/merge)
that update the library. Each new/modified experience must: (a) be $\leq$32
words, (b) be actionable and specific, (c) explain WHEN and HOW to apply.
Format: OPERATION | experience\_id (if modify/delete/merge) | text.}
\end{quote}

The output $\Omega^{(g)}$ from Stage~2 contains all proposed library
modifications arising from task group $g$.

\subsection{Stage 3: Batch Consolidation}
\label{sec:stage3}

Stage~3 merges the operation sets $\{\Omega^{(g)}\}_{g=1}^{N_g}$ from all
$N_g$ task groups processed in the current session, producing a final
consolidated operation set $\Omega^*$ that is applied atomically to $\mathcal{E}$.

Consolidation proceeds in three passes:

\begin{enumerate}[leftmargin=1.5em]
    \item \textbf{Conflict resolution:} If two groups issue conflicting
          \textsc{modify} or \textsc{delete} operations on the same $e_{\text{id}}$,
          the operation from the group with higher mean group advantage
          $\bar{A}^{(g)} = \frac{1}{G}\sum_i |A^{(i)}_g|$ takes precedence.
    \item \textbf{Deduplication:} \textsc{add} operations whose proposed text
          has cosine similarity above $\theta_{\text{dup}} = 0.85$ with an
          existing library entry are converted to \textsc{modify} operations.
    \item \textbf{Length enforcement:} Any experience text exceeding 32 words
          is compressed via an LLM call: \texttt{Summarize in $\leq$32 words,
          preserving the actionable core: \{text\}}.
\end{enumerate}

\subsection{The AGENT-GRPO Algorithm}
\label{sec:algorithm}

Algorithm~\ref{alg:agent-grpo} presents the complete Agent-GRPO procedure.

\begin{algorithm}[H]
\caption{AGENT-GRPO: Agent-Level Training-Free GRPO}
\label{alg:agent-grpo}
\begin{algorithmic}[1]
\Require Session trajectories $\mathcal{S} = \{(\tau^{(j)}_g)\}_{g,j}$,
         experience library $\mathcal{E}$,
         group size $G$, consolidation threshold $\theta_{\text{dup}}$
\Ensure Updated experience library $\mathcal{E}'$

\State $\Omega \gets \emptyset$ \Comment{Accumulated operations}
\For{each task group $g$ in $\mathcal{S}$}
    \State $\{\tau^{(j)}\}_{j=1}^G \gets$ trajectories for group $g$
    \State \textbf{// Compute rewards and advantages}
    \For{$j = 1$ to $G$}
        \State $R^{(j)} \gets w_1\,\rho_s(\tau^{(j)}) + w_2\,\rho_a(\tau^{(j)})$
    \EndFor
    \State $\mu_G, \sigma_G \gets \text{GroupStats}(\{R^{(j)}\})$
    \If{$\sigma_G < \epsilon$}
        \State \textbf{continue} \Comment{No variance: group provides no learning signal}
    \EndIf
    \For{$j = 1$ to $G$}
        \State $A^{(j)} \gets (R^{(j)} - \mu_G) / (\sigma_G + \epsilon)$
    \EndFor

    \State \textbf{// Stage 1: Trajectory Summarization}
    \For{$j = 1$ to $G$}
        \State $\hat{\tau}^{(j)} \gets \text{Summarize}(\tau^{(j)},\, \text{sign}(A^{(j)}))$
    \EndFor

    \State \textbf{// Stage 2: Group Advantage Extraction}
    \State $\mathcal{E}_{\text{rel}} \gets \text{Retrieve}(\mathcal{E},\, x_g,\, K_{\text{stage2}}=5)$
    \State $\Omega^{(g)} \gets \text{ExtractAdvantages}(\{\hat{\tau}^{(j)}\},\, \mathcal{E}_{\text{rel}})$
    \State $\Omega \gets \Omega \cup \Omega^{(g)}$
\EndFor

\State \textbf{// Stage 3: Batch Consolidation}
\State $\Omega^* \gets \text{Consolidate}(\Omega,\, \theta_{\text{dup}})$

\State \textbf{// Apply operations}
\State $\mathcal{E}' \gets \text{Apply}(\mathcal{E},\, \Omega^*)$

\State \Return $\mathcal{E}'$
\end{algorithmic}
\end{algorithm}

\paragraph{Complexity.}
Let $N_g$ be the number of task groups, $G$ the group size, and $L_{\text{LLM}}$
the cost of a single LLM extraction call. Stage~1 requires $N_g G$ LLM calls.
Stage~2 requires $N_g$ calls. Stage~3 requires $O(|\Omega|)$ comparisons plus
at most $|\Omega|$ LLM compression calls. Total cost: $O(N_g(G + 1))$ LLM
calls, typically 50--200 calls per session at approximately \$0.05--\$0.10
each, yielding a per-session adaptation cost of \$2.50--\$20.

% ============================================================================
% 4. SEMANTIC MEMORY MANAGER
% ============================================================================
\section{Semantic Memory Manager}
\label{sec:memory}

\subsection{Experience Representation}

\begin{definition}[Experience Entry]
\label{def:experience}
An experience entry $e \in \mathcal{E}$ is a tuple:
\begin{equation}
    e = \bigl(\texttt{id},\; \texttt{text},\; c,\; d,\; \bm{v},\; u,\; k\bigr),
    \label{eq:experience}
\end{equation}
where:
\begin{itemize}[leftmargin=1.5em]
    \item $\texttt{id} \in \mathbb{N}$ is a unique identifier,
    \item $\texttt{text} \in \Sigma^{\leq 32}$ is the natural-language experience
          (at most 32 words),
    \item $c \in (0, 1)$ is the confidence score (initialized to $c_0 = 0.5$
          for new entries, updated via online learning),
    \item $d \in \mathcal{D}$ is the domain label,
    \item $\bm{v} \in \mathbb{R}^{384}$ is the sentence embedding produced by
          \texttt{all-MiniLM-L6-v2}~\citep{reimers2019sentence},
    \item $u \in \mathbb{N}$ is the usage count (number of times retrieved and
          applied),
    \item $k \in \mathbb{N}$ is the epoch of creation (session index).
\end{itemize}
\end{definition}

The confidence score is updated via a Bayesian-flavored online rule after each
session in which experience $e$ was applied:
\begin{equation}
    c \leftarrow
    \begin{cases}
        \min(c + \gamma(1 - c),\; c_{\max}) & \text{if applying } e \text{ improved task reward,} \\
        \max(c - \gamma c,\; c_{\min}) & \text{if applying } e \text{ did not improve reward,}
    \end{cases}
    \label{eq:confidence-update}
\end{equation}
where $\gamma = 0.1$ is the learning rate, $c_{\min} = 0.05$, and
$c_{\max} = 0.95$.

\subsection{Retrieval}
\label{sec:retrieval}

At the start of each agent session with task $x$, the memory manager retrieves
the top-$K$ experiences most relevant to $x$.

\paragraph{Query Embedding.}
The task query is embedded: $\bm{q} = \text{Enc}(x) \in \mathbb{R}^{384}$.

\paragraph{Domain-Filtered Cosine Similarity.}
For each experience $e \in \mathcal{E}$, the retrieval score is:
\begin{equation}
    \text{score}(e, x) =
    \begin{cases}
        \dfrac{\bm{q} \cdot \bm{v}_e}{\|\bm{q}\|\,\|\bm{v}_e\|} & \text{if } d_e \in D_x \text{ or } D_x = \emptyset, \\
        -\infty & \text{otherwise,}
    \end{cases}
    \label{eq:retrieval-score}
\end{equation}
where $D_x \subseteq \mathcal{D}$ is the set of domains inferred from the task
(empty if domain is ambiguous, in which case all domains are eligible). The
top-$K$ experiences by score are returned.

\paragraph{Context Injection.}
Retrieved experiences are prepended to the agent's system prompt in ranked order:
\begin{quote}
\texttt{[G0] \{e[0].text\}}\\
\texttt{[G1] \{e[1].text\}}\\
$\vdots$\\
\texttt{[G\{K-1\}] \{e[K-1].text\}}
\end{quote}
This format signals to the model that the bracketed items are learned
guidelines (``G'' for guideline), distinguishing them from task instructions
and conversation history. Default $K = 5$ retrieved experiences per session.

\subsection{Confidence-Weighted Retrieval Score}

The pure cosine similarity score can be augmented by confidence weighting to
prioritize well-validated experiences:
\begin{equation}
    \tilde{\text{score}}(e, x) = (1 - \lambda)\,\frac{\bm{q} \cdot \bm{v}_e}{\|\bm{q}\|\,\|\bm{v}_e\|}
    + \lambda\, c_e,
    \label{eq:conf-weighted-score}
\end{equation}
with $\lambda = 0.2$. This blends semantic relevance with empirical reliability.

\subsection{Memory Compression}
\label{sec:compression}

As the experience library grows, unconstrained addition would eventually
degrade retrieval precision (too many near-duplicate or stale entries) and
increase context injection cost. The memory manager applies one of four
compression strategies when $|\mathcal{E}| > N_{\max}$ (default $N_{\max} = 500$).

\paragraph{Strategy 1: Diversity Compression.}
Apply $K$-means clustering to the embedding matrix
$V = [\bm{v}_1, \ldots, \bm{v}_{|\mathcal{E}|}]^\top$. Within each cluster
$\mathcal{C}_k$, retain only the experience with highest confidence:
\begin{equation}
    e^*_k = \argmax_{e \in \mathcal{C}_k} c_e.
    \label{eq:diversity-compression}
\end{equation}
This ensures the surviving library spans the full semantic space of learned
patterns.

\paragraph{Strategy 2: Importance Compression.}
Sort experiences by confidence and discard the bottom-$p$ fraction:
\begin{equation}
    \mathcal{E}' = \bigl\{e \in \mathcal{E} : c_e \geq Q_{1-p}(\{c_e : e \in \mathcal{E}\})\bigr\},
    \label{eq:importance-compression}
\end{equation}
where $Q_{1-p}$ is the $(1-p)$-quantile. Default $p = 0.2$ (discard lowest
20\% by confidence).

\paragraph{Strategy 3: Temporal Decay Compression.}
Assign each experience a time weight via exponential decay:
\begin{equation}
    w_t(e) = \exp\bigl(-\lambda(k_{\text{now}} - k_e)\bigr),
    \label{eq:temporal-decay}
\end{equation}
where $k_e$ is the epoch of creation, $k_{\text{now}}$ is the current epoch,
and $\lambda > 0$ is the decay constant (default $\lambda = 0.05$). Experiences
with $w_t(e) < w_{\min}$ are pruned. Temporal decay is appropriate in
fast-changing environments where older experiences may reflect outdated tool
behavior.

\paragraph{Strategy 4: Hybrid Compression.}
The hybrid strategy combines confidence, temporal weight, and a diversity
penalty into a single utility score:
\begin{equation}
    U(e) = \alpha \cdot c_e^{\,\beta} \cdot w_t(e)^{\,\gamma} \cdot d_{\text{div}}(e)^{\,\delta},
    \label{eq:hybrid-utility}
\end{equation}
where $d_{\text{div}}(e) = \min_{e' \neq e, e' \in \mathcal{E}}\|\bm{v}_e - \bm{v}_{e'}\|_2$
is a diversity distance that penalizes experiences that are too similar to
others in the library (encouraging diversity), and $\alpha, \beta, \gamma, \delta > 0$
are tunable exponents (defaults: $\beta = 1.0$, $\gamma = 0.5$, $\delta = 0.3$).
Experiences are pruned in ascending order of $U(e)$ until $|\mathcal{E}| \leq N_{\max}$.

\begin{remark}
In practice, we recommend hybrid compression for production deployments, as
it balances all three considerations. Diversity compression alone can
inadvertently discard valid experiences that happen to share semantic space
with a higher-confidence entry. Importance compression alone can cause the
library to become stale. Temporal decay alone discards valuable stable patterns.
\end{remark}

\subsection{Memory Manager Algorithm}

\begin{algorithm}[H]
\caption{SemanticMemoryManager.Update}
\label{alg:memory-update}
\begin{algorithmic}[1]
\Require Experience library $\mathcal{E}$, operation set $\Omega^*$,
         capacity $N_{\max}$, compression strategy $\mathcal{C}$
\Ensure Updated $\mathcal{E}'$

\State $\mathcal{E}' \gets \mathcal{E}$
\For{each $\omega \in \Omega^*$}
    \If{$\omega = \textsc{add}(e_{\text{new}})$}
        \State $e_{\text{new}}.\bm{v} \gets \text{Enc}(e_{\text{new}}.\texttt{text})$
        \State $\mathcal{E}' \gets \mathcal{E}' \cup \{e_{\text{new}}\}$
    \ElsIf{$\omega = \textsc{modify}(e_{\text{id}}, e_{\text{new}})$}
        \State Find $e$ with $e.\texttt{id} = e_{\text{id}}$ in $\mathcal{E}'$
        \State $e_{\text{new}}.\bm{v} \gets \text{Enc}(e_{\text{new}}.\texttt{text})$
        \State $e_{\text{new}}.c \gets e.c$ \Comment{Preserve confidence history}
        \State $\mathcal{E}' \gets (\mathcal{E}' \setminus \{e\}) \cup \{e_{\text{new}}\}$
    \ElsIf{$\omega = \textsc{delete}(e_{\text{id}})$}
        \State $\mathcal{E}' \gets \mathcal{E}' \setminus \{e : e.\texttt{id} = e_{\text{id}}\}$
    \ElsIf{$\omega = \textsc{merge}(e_{\text{id}_1}, e_{\text{id}_2}, e_{\text{new}})$}
        \State $c_{\text{merged}} \gets \max(c_{e_{\text{id}_1}}, c_{e_{\text{id}_2}})$
        \State $e_{\text{new}}.c \gets c_{\text{merged}}$
        \State $e_{\text{new}}.\bm{v} \gets \text{Enc}(e_{\text{new}}.\texttt{text})$
        \State $\mathcal{E}' \gets (\mathcal{E}' \setminus \{e_{\text{id}_1}, e_{\text{id}_2}\}) \cup \{e_{\text{new}}\}$
    \EndIf
\EndFor
\If{$|\mathcal{E}'| > N_{\max}$}
    \State $\mathcal{E}' \gets \text{Compress}(\mathcal{E}',\; \mathcal{C},\; N_{\max})$
\EndIf
\State \Return $\mathcal{E}'$
\end{algorithmic}
\end{algorithm}

% ============================================================================
% 5. AGENT-SPECIFIC ADAPTATIONS
% ============================================================================
\section{Agent-Specific Adaptations}
\label{sec:adaptations}

Beyond the structural adaptations to the GRPO pipeline, Agent-GRPO incorporates
three classes of agent-specific experience types.

\subsection{Tool Selection Experiences}

Tool selection experiences encode when to prefer one tool over another for
a class of tasks. These are the most valuable experiences for reducing tool
call errors, as incorrect tool selection typically leads to cascading failures.

\paragraph{Examples of Tool Selection Experiences.}
\begin{itemize}[leftmargin=1.5em]
    \item \textit{For JSON extraction from \texttt{package.json}, prefer
          \texttt{read\_json} over \texttt{bash\,+\,jq}; 3$\times$ faster,
          no shell escaping required.}
    \item \textit{For Kubernetes pod logs, use \texttt{kubectl\_logs}
          with \texttt{--tail=100} before \texttt{kubectl\_exec}; avoids
          unnecessary shell sessions.}
    \item \textit{When searching code for function definitions, prefer
          \texttt{ast\_search} over \texttt{grep}; handles multiline
          definitions correctly.}
\end{itemize}

Tool selection experiences are tagged with domain $d \in \mathcal{D}$ and
assigned high initial confidence ($c_0 = 0.7$) when extracted from groups with
large positive advantages ($|A^{(i)}| > 1.5$).

\subsection{Failure Pattern Experiences}

Failure pattern experiences encode common failure modes and their remediation
strategies. They are derived primarily from Stage-2 analysis of negative
examples ($A^{(i)} < -0.5$).

\paragraph{Examples of Failure Pattern Experiences.}
\begin{itemize}[leftmargin=1.5em]
    \item \textit{Git push to \texttt{main} fails on protected repos; always
          create a PR branch first.}
    \item \textit{Docker build fails if \texttt{COPY} precedes \texttt{RUN
          apt-get}; reorder to install deps first.}
    \item \textit{Kubernetes \texttt{kubectl apply} without \texttt{--namespace}
          silently targets \texttt{default}; always specify namespace.}
\end{itemize}

Failure pattern experiences carry a \texttt{failure\_mode} annotation that
enables targeted retrieval when the agent encounters error messages matching
known patterns. The retrieval query is augmented with the error message:
$x' = \text{concat}(x, \text{error\_message})$ before embedding.

\subsection{Domain-Specific Convention Experiences}

Convention experiences encode task-domain norms that are not explicit in tool
documentation but emerge from observed agent behavior across many sessions.
These are partitioned by domain $d \in \mathcal{D}$.

\paragraph{Coding Domain ($d_{\text{code}}$).}
\begin{itemize}[leftmargin=1.5em]
    \item Commit message format: \texttt{type(scope): description}.
    \item Test before commit: always run \texttt{pytest} or \texttt{go test}
          before pushing.
    \item PR description template: Summary, Test Plan, Breaking Changes.
\end{itemize}

\paragraph{DevOps Domain ($d_{\text{devops}}$).}
\begin{itemize}[leftmargin=1.5em]
    \item Deployment sequence: build $\to$ push image $\to$ update manifest
          $\to$ apply $\to$ verify rollout.
    \item Rollback procedure: \texttt{kubectl rollout undo} before any
          destructive cleanup.
    \item Health check: \texttt{kubectl get pods} within 60\,s of deployment.
\end{itemize}

\paragraph{Data Science Domain ($d_{\text{ds}}$).}
\begin{itemize}[leftmargin=1.5em]
    \item Always set random seeds before experiments.
    \item Log hyperparameters before training starts.
    \item Validate data shapes immediately after loading.
\end{itemize}

Convention experiences are assigned domain $d$ during Stage-1 summarization
(the LLM is instructed to classify the domain of each trajectory). They are
retrieved with domain filtering active ($D_x = \{d\}$), preventing convention
experiences from one domain from polluting retrieval for another.

\subsection{Hierarchical Experience Structure}

The three experience types form a natural hierarchy:
\begin{equation}
    \text{Convention} \supseteq \text{Tool Selection} \supseteq \text{Failure Pattern}
\end{equation}
in terms of generality. Convention experiences apply broadly; tool selection
experiences apply to specific tool pairs; failure pattern experiences apply
to specific error conditions. At retrieval time, we sample experiences from
all three types, with a soft budget: at most $K_c = 2$ convention experiences,
$K_t = 2$ tool selection experiences, and $K_f = 1$ failure pattern experience
in the default $K = 5$ budget.

% ============================================================================
% 6. INTEGRATION WITH SELF-IMPROVEMENT SYSTEM
% ============================================================================
\section{Integration with the Four-Loop Self-Improvement Architecture}
\label{sec:integration}

The Hanzo Dev agent implements a four-loop self-improvement architecture
inspired by~\citet{shinn2023reflexion} and extended with proactive tool
creation~\citep{wang2023voyager}. Agent-GRPO integrates with each loop.

\subsection{Loop 0: Telemetry}

Loop 0 instruments every tool call with structured telemetry:
\begin{equation}
    \text{TelemetryEvent} = \bigl(\texttt{session\_id},\; t_i,\; \bm{p}_i,\; o_i,\; s_i,\; \Delta t_i,\; \text{context}\bigr),
    \label{eq:telemetry}
\end{equation}
where $\Delta t_i$ is the wall-clock duration of the tool call and \text{context}
captures the surrounding conversational context.

Telemetry events are aggregated at the end of each session to construct the
raw trajectory $\tau$ according to Definition~\ref{def:trajectory}. Loop 0
thus provides the \emph{raw data} for Agent-GRPO's trajectory processing
pipeline without any additional instrumentation cost.

\subsection{Loop 1: Build-It-Now}

Loop 1 monitors for repeated tool-call failures and proactively proposes
new tool implementations~\citep{wang2023voyager}. Agent-GRPO informs this loop
via two mechanisms:

\begin{enumerate}[leftmargin=1.5em]
    \item \textbf{Failure pattern retrieval:} When Loop 1 detects a recurring
          failure (e.g., the same error message appearing across three or more
          sessions), it queries $\mathcal{E}$ for failure pattern experiences
          matching the error. If no remediation experience exists, Loop 1
          escalates to tool creation.
    \item \textbf{Tool selection experience audit:} Experience entries of the
          form ``prefer \texttt{X} over \texttt{Y}'' indicate that tool \texttt{X}
          satisfies a need better than \texttt{Y}. If \texttt{X} does not exist
          in the tool registry $\mathcal{T}$, Loop 1 receives a tool creation
          proposal.
\end{enumerate}

\subsection{Loop 2: Active Learning}

Loop 2 captures explicit user corrections as high-confidence, high-priority
experiences. When a user corrects the agent's output (triggering
$\rho_a(\tau) = 0$), the correction is processed by a dedicated prompt:

\begin{quote}
\textit{The user corrected the agent's response. Original task: \{x\}. Agent
output: \{y\}. User correction: \{y'\}. In $\leq$32 words, what should the
agent have done differently? Format as an actionable guideline.}
\end{quote}

The resulting experience is added to $\mathcal{E}$ with $c = 0.8$ (high
confidence, since it reflects direct human preference) and is exempt from the
standard GRPO extraction pipeline.

\subsection{Loop 3: Reflection}

At the end of each session, Loop 3 produces a session summary:
\begin{itemize}[leftmargin=1.5em]
    \item Total tool calls, success rate, task completion rate.
    \item Top-3 recurring failure modes.
    \item Top-3 most-retrieved experiences (by session usage).
    \item Proposed new experiences (preliminary, before GRPO refinement).
\end{itemize}

This summary serves as the primary input to Agent-GRPO: the session
trajectories $\mathcal{S}$ fed to Algorithm~\ref{alg:agent-grpo} are
precisely the trajectories recorded during this session. Loop 3 fires
Agent-GRPO asynchronously after session end, so experience library updates
are available for the next session.

\subsection{Loop 4: Maintenance}

Loop 4 performs periodic library maintenance using statistics accumulated
across sessions:

\begin{itemize}[leftmargin=1.5em]
    \item \textbf{Staleness audit:} Experiences with $u = 0$ after
          $K_{\text{stale}} = 20$ sessions are flagged for deletion.
    \item \textbf{Confidence audit:} Experiences with $c < c_{\text{min}}$
          after at least $u \geq 5$ uses are deleted.
    \item \textbf{Redundancy audit:} Experience pairs with cosine similarity
          above $\theta_{\text{dup}}$ are merged via LLM consolidation.
    \item \textbf{Compression trigger:} When $|\mathcal{E}| > N_{\max}$,
          Loop 4 triggers the hybrid compression strategy
          (Section~\ref{sec:compression}).
\end{itemize}

The maintenance interval is configurable (default: every 10 sessions).

\subsection{Closed-Loop Diagram}

The integration of all four loops with Agent-GRPO forms a closed improvement
cycle:
\begin{equation}
    \underbrace{\text{Loop 0}}_{\text{Telemetry}}
    \;\xrightarrow{\tau}\;
    \underbrace{\text{Loop 3}}_{\text{Reflection}}
    \;\xrightarrow{\mathcal{S}}\;
    \underbrace{\text{Agent-GRPO}}_{\text{Alg.~\ref{alg:agent-grpo}}}
    \;\xrightarrow{\Omega^*}\;
    \underbrace{\mathcal{E}}_{\text{Library}}
    \;\xrightarrow{\text{top-}K}\;
    \underbrace{\text{Agent}}_{\text{next session}}
    \;\circlearrowleft
    \label{eq:closed-loop}
\end{equation}
with Loop 2 (user corrections) providing an out-of-band high-confidence
injection path and Loop 4 performing periodic quality control.

% ============================================================================
% 7. THEORETICAL ANALYSIS
% ============================================================================
\section{Theoretical Analysis}
\label{sec:theory}

We now establish formal guarantees for Agent-GRPO under a set of mild
assumptions.

\begin{assumption}[Bounded Rewards]
\label{asm:bounded}
For all trajectories $\tau$, the composite reward satisfies $R(\tau) \in [0, 1]$.
\end{assumption}

\begin{assumption}[Positive Information Content]
\label{asm:info}
Each task group with $\sigma_G > 0$ yields at least one \textsc{add} operation
in expectation: $\mathbb{E}[|\Omega^{(g)}_{\textsc{add}}|] \geq 1$.
\end{assumption}

\begin{assumption}[Experience Relevance]
\label{asm:relevance}
An experience $e$ is \emph{beneficial} for task $x$ if $\mathbb{E}[R(\tau) \mid e \text{ retrieved}] > \mathbb{E}[R(\tau) \mid e \text{ not retrieved}]$.
Let $B_k = |\{e \in \mathcal{E}_k : e \text{ is beneficial for a uniformly random } x \sim \mathcal{X}\}|$
denote the number of beneficial experiences at epoch $k$.
\end{assumption}

\begin{assumption}[Extraction Accuracy]
\label{asm:accuracy}
The extraction pipeline (Stages 1--3) is correct with probability $p_c > 1/2$:
each \textsc{add} operation produces a beneficial experience with probability
at least $p_c$, and each \textsc{delete}/\textsc{modify}/\textsc{merge} operation
does not decrease $B_k$ in expectation.
\end{assumption}

\begin{theorem}[Monotone Library Improvement]
\label{thm:monotone}
Under Assumptions~\ref{asm:bounded}--\ref{asm:accuracy}, if each session
processes at least one task group with $\sigma_G > 0$, then the expected number
of beneficial experiences is non-decreasing across epochs:
\begin{equation}
    \mathbb{E}[B_{k+1}] \geq \mathbb{E}[B_k], \quad \forall k \geq 0.
    \label{eq:monotone}
\end{equation}
Furthermore, if $p_c > 1/2$, then $\mathbb{E}[B_{k+1}] > \mathbb{E}[B_k]$
when at least one group in epoch $k$ has $\sigma_G > 0$.
\end{theorem}

\begin{proof}
Let $A_k$ denote the set of \textsc{add} operations produced in epoch $k$
and $D_k$ the set of \textsc{delete}/\textsc{modify}/\textsc{merge} operations.
By Assumption~\ref{asm:info}, $|A_k| \geq 1$ in expectation when $\sigma_G > 0$.

Each \textsc{add} operation produces a beneficial experience independently with
probability $p_c > 1/2$ (Assumption~\ref{asm:accuracy}). Thus the expected
increase from \textsc{add} operations is:
\begin{equation}
    \mathbb{E}[\Delta B_k^+] = |A_k| \cdot p_c > 0.
\end{equation}

By Assumption~\ref{asm:accuracy}, each destructive operation has non-negative
expected effect: $\mathbb{E}[\Delta B_k^-] \geq 0$.

Therefore:
\begin{equation}
    \mathbb{E}[B_{k+1}] = \mathbb{E}[B_k + \Delta B_k^+ + \Delta B_k^-]
    \geq \mathbb{E}[B_k] + |A_k| \cdot p_c > \mathbb{E}[B_k],
\end{equation}
establishing strict monotone improvement. When no group has $\sigma_G > 0$,
$|A_k| = 0$ and neither \textsc{add} nor destructive operations fire, so
$B_{k+1} = B_k$ and the inequality holds with equality.
\end{proof}

\begin{remark}
Theorem~\ref{thm:monotone} does not guarantee that $B_k \to \infty$;
memory compression (Section~\ref{sec:compression}) bounds $|\mathcal{E}_k|$,
and compression may occasionally remove beneficial experiences. A refined
result incorporating compression is stated as Corollary~\ref{cor:compressed}.
\end{remark}

\begin{corollary}[Monotone Improvement Under Compression]
\label{cor:compressed}
If compression is performed with the hybrid strategy (Strategy 4) with
parameter $\beta > \delta$ (confidence weighted more than diversity), then
for any $\varepsilon > 0$ there exists $N_{\max}(\varepsilon)$ such that for
$N_{\max} \geq N_{\max}(\varepsilon)$, Theorem~\ref{thm:monotone} holds up
to an additive term $-\varepsilon$ on the right-hand side:
$\mathbb{E}[B_{k+1}] \geq \mathbb{E}[B_k] - \varepsilon$.
\end{corollary}

\begin{proof}[Proof Sketch]
Hybrid compression removes experiences with lowest utility $U(e)$ first. With
$\beta > \delta$, confidence $c_e$ dominates the utility score. Beneficial
experiences accumulate high confidence over time (Equation~\eqref{eq:confidence-update}),
so the probability of a beneficial experience being pruned is bounded by
$O(e^{-\beta \cdot c_{\text{good}}})$, where $c_{\text{good}}$ is the typical
confidence of a beneficial experience after several uses. Setting $N_{\max}$
large enough that the expected number of pruned beneficial experiences per
epoch is at most $\varepsilon$ completes the argument.
\end{proof}

\begin{theorem}[Sample Complexity]
\label{thm:sample}
Let $\varepsilon > 0$ be a target improvement in expected task reward over the
zero-experience baseline. Under Assumptions~\ref{asm:bounded}--\ref{asm:accuracy},
the expected number of sessions required to achieve $\varepsilon$-improvement is:
\begin{equation}
    T(\varepsilon) = O\!\left(\frac{K_x}{\varepsilon^2\,(2p_c - 1)^2}\right),
    \label{eq:sample-complexity}
\end{equation}
where $K_x$ is the number of distinct task types in the task distribution
$\mathcal{X}$ and $p_c > 1/2$ is the extraction accuracy of Assumption~\ref{asm:accuracy}.
\end{theorem}

\begin{proof}
The expected improvement in task reward from retrieving a beneficial experience
is at least $\delta_{\min} > 0$ (the minimum individual experience effect size,
which exists by compactness of $[0,1]$).

From Theorem~\ref{thm:monotone}, after $t$ sessions the expected number of
beneficial experiences relevant to a task of type $x$ is at least:
\begin{equation}
    \mathbb{E}[B_k(x)] \geq \min\!\left(t\,p_c\,/K_x,\; K_{\text{ret}}\right),
\end{equation}
where $K_{\text{ret}}$ is the retrieval budget. When $t\,p_c / K_x \geq 1$,
at least one beneficial experience relevant to a random task is expected to be
in the library. By the central limit theorem applied to independent session
rewards, the variance of the reward estimator after $t$ sessions is $O(1/t)$.
Setting the improvement signal-to-noise ratio $\geq 1/\varepsilon^2$ and
solving for $t$ gives the stated bound.
\end{proof}

\begin{lemma}[Reward Consistency]
\label{lem:consistency}
The composite reward $R(\tau) = w_1 \rho_s(\tau) + w_2 \rho_a(\tau)$ is a
consistent estimator of task success in the following sense: for any two
trajectories $\tau, \tau'$ on the same task $x$ with
$\Pr[\text{task complete} \mid \tau] > \Pr[\text{task complete} \mid \tau']$,
we have $\mathbb{E}[R(\tau)] > \mathbb{E}[R(\tau')]$.
\end{lemma}

\begin{proof}
Task completion requires all tool calls to succeed ($\rho_s = 1$) and the
user to accept the result ($\rho_a = 1$). Since $w_1, w_2 > 0$ and
$\rho_s, \rho_a \geq 0$, and since task completion implies both $\rho_s = 1$
and $\rho_a = 1$:
\begin{align}
    \mathbb{E}[R(\tau)] &= w_1\,\mathbb{E}[\rho_s(\tau)] + w_2\,\mathbb{E}[\rho_a(\tau)] \\
    &\geq w_1\,\Pr[\text{task complete} \mid \tau] + w_2\,\Pr[\text{task complete} \mid \tau].
\end{align}
The strict inequality follows from $w_1 + w_2 > 0$ and the assumption that
task completion probabilities differ.
\end{proof}

\begin{proposition}[Memory Compression Preserves Utility]
\label{prop:compression}
Let $\mathcal{E}_\mathcal{C}$ be the experience library after hybrid
compression with threshold $N_{\max}$. Then the expected retrieval quality
(defined as the expected reward improvement from retrieved experiences)
satisfies:
\begin{equation}
    \mathbb{E}[\text{RetrievalQuality}(\mathcal{E}_\mathcal{C})]
    \geq (1 - \eta)\,\mathbb{E}[\text{RetrievalQuality}(\mathcal{E})],
    \label{eq:compression-quality}
\end{equation}
where $\eta \in [0, 1)$ is a compression efficiency factor that satisfies
$\eta \leq |\mathcal{E}| / N_{\max} - 1$ for $|\mathcal{E}| > N_{\max}$.
\end{proposition}

\begin{proof}[Proof Sketch]
Hybrid compression preferentially retains high-$U(e)$ experiences. The
expected retrieval quality is dominated by the top-$K$ most similar and
highest-confidence experiences. Since compression removes the bottom-utility
fraction first, and retrieval only uses the top-$K$, the quality degradation
is bounded by the probability that a top-$K$ experience falls below the
compression threshold, which is $O(1 - N_{\max}/|\mathcal{E}|)$.
\end{proof}

\subsection{Cost Analysis}

Table~\ref{tab:cost} compares Agent-GRPO with alternative adaptation
approaches for a representative software engineering agent.

\begin{table}[H]
\centering
\caption{Adaptation cost comparison for a tool-using agent across 100 sessions
($\sim$1,000 tool calls total). All costs are estimated for mid-2025 API pricing.}
\label{tab:cost}
\begin{tabularx}{\textwidth}{lXrr}
\toprule
\textbf{Method} & \textbf{Description} & \textbf{Compute Cost} & \textbf{GPU-hours} \\
\midrule
Full fine-tuning (RLHF)   & Gradient updates on full model weights & \$100,000+ & 1,000+ \\
LoRA fine-tuning           & Parameter-efficient gradient updates     & \$10,000--\$50,000 & 100--500 \\
Prompt engineering         & Manual trial-and-error prompting         & \$500--\$2,000 & 0 \\
RAG (static)               & Retrieve from fixed document corpus      & \$200--\$500 & 0 \\
Reflexion~\citep{shinn2023reflexion} & Session-level reflection, no persistence & \$50--\$200 & 0 \\
\textbf{Agent-GRPO (ours)} & \textbf{GRPO extraction + persistent library} & \textbf{\$15--\$20} & \textbf{0} \\
\bottomrule
\end{tabularx}
\end{table}

The per-session cost of Agent-GRPO is approximately:
\begin{equation}
    C_{\text{session}} \approx N_g \cdot G \cdot c_{\text{stage1}} + N_g \cdot c_{\text{stage2}} + c_{\text{stage3}},
    \label{eq:cost}
\end{equation}
where $c_{\text{stage1}} \approx \$0.02$ (small context, fast model),
$c_{\text{stage2}} \approx \$0.05$ (larger context), $c_{\text{stage3}} \approx \$0.10$
(batch consolidation with full library context), $N_g \approx 10$ task groups,
and $G = 4$ trajectories per group. This yields
$C_{\text{session}} \approx 10 \cdot 4 \cdot \$0.02 + 10 \cdot \$0.05 + \$0.10 = \$1.40$.
Over 10 sessions, total adaptation cost is approximately \$14, well within
the stated sub-\$20 budget.

% ============================================================================
% 8. EXPERIMENTS
% ============================================================================
\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Setup}

\paragraph{Benchmark.}
We evaluate on \textbf{HB-Bench}, a tool-use benchmark comprising 150 tasks
across five domains: software engineering (40 tasks), DevOps (30), data
processing (30), web research (30), and system administration (20). Each task
requires 3--15 tool invocations and has a binary completion criterion evaluated
by an automated judge.

\paragraph{Agent Implementation.}
The base agent is a Hanzo Dev agent~\citep{hanzo2026aso} backed by a 32B
parameter frozen LLM with access to 24 tools including file I/O, shell
execution, code search, Kubernetes operations, and web search. All experiments
use $G = 4$ trajectories per group and $K = 5$ retrieved experiences per
session.

\paragraph{Baselines.}
We compare against:
\begin{enumerate}[leftmargin=1.5em]
    \item \textbf{Zero-shot:} No experience injection; frozen base model.
    \item \textbf{RAG-Static:} Fixed tool documentation injected as context.
    \item \textbf{Reflexion:} Session-level reflection without persistent memory~\citep{shinn2023reflexion}.
    \item \textbf{ASO (text-only):} TF-GRPO on text outputs only, ignoring
          tool structure~\citep{hanzo2026aso}.
    \item \textbf{Agent-GRPO (ours):} Full tool trajectory extraction with
          semantic experience library.
\end{enumerate}

\subsection{Main Results}

\begin{table}[H]
\centering
\caption{Task completion rate (\%) on HB-Bench after 50 sessions. All results
are averaged over 5 independent runs; standard deviations in parentheses.}
\label{tab:main-results}
\begin{tabularx}{\textwidth}{lXrrr}
\toprule
\textbf{Method} & \textbf{SWE} & \textbf{DevOps} & \textbf{Data} & \textbf{Overall} \\
\midrule
Zero-shot           & 42.1 (1.8) & 38.6 (2.1) & 51.3 (1.5) & 44.0 (1.4) \\
RAG-Static          & 47.3 (1.6) & 43.2 (1.9) & 54.7 (1.7) & 48.4 (1.2) \\
Reflexion           & 52.8 (2.0) & 49.7 (2.3) & 58.2 (1.8) & 53.6 (1.5) \\
ASO (text-only)     & 56.1 (1.9) & 51.4 (2.0) & 61.8 (1.6) & 56.4 (1.4) \\
\textbf{Agent-GRPO} & \textbf{63.7 (1.7)} & \textbf{60.2 (1.8)} & \textbf{68.4 (1.5)} & \textbf{64.1 (1.3)} \\
\bottomrule
\end{tabularx}
\end{table}

Agent-GRPO achieves a \textbf{20.1 percentage point} improvement over zero-shot
and a \textbf{7.7 percentage point} improvement over ASO (text-only). The
improvement is most pronounced in the DevOps domain (+21.6\% over zero-shot),
where domain-specific convention experiences (Section~\ref{sec:adaptations})
provide substantial guidance on deployment sequences and rollback procedures.

\subsection{Tool-Call Error Rate}

\begin{table}[H]
\centering
\caption{Tool-call error rate (\%) across methods. Lower is better.}
\label{tab:error-rate}
\begin{tabularx}{\textwidth}{lXrr}
\toprule
\textbf{Method} & \textbf{Session 1} & \textbf{Session 25} & \textbf{Session 50} \\
\midrule
Zero-shot           & 18.4 & 18.2 & 18.5 \\
RAG-Static          & 15.7 & 15.5 & 15.9 \\
Reflexion           & 17.1 & 14.3 & 13.9 \\
ASO (text-only)     & 16.9 & 13.8 & 12.7 \\
\textbf{Agent-GRPO} & 18.1 & \textbf{10.2} & \textbf{12.6} \\
\bottomrule
\end{tabularx}
\end{table}

Agent-GRPO shows a marked error rate reduction between sessions 1 and 25
(\textbf{44.8\% relative reduction}), reflecting the rapid accumulation of
failure pattern experiences in the early epochs. The slight increase from
session 25 to 50 reflects library compression removing some overfitted
failure-pattern experiences.

\subsection{Ablation Studies}

\begin{table}[H]
\centering
\caption{Ablation on HB-Bench overall task completion rate (\%) after 50 sessions.
Each variant removes one component of Agent-GRPO.}
\label{tab:ablation}
\begin{tabular}{lc}
\toprule
\textbf{Configuration} & \textbf{Overall (\%)} \\
\midrule
Full Agent-GRPO                       & 64.1 \\
\quad w/o Stage 1 summarization       & 59.8 (-4.3) \\
\quad w/o Stage 3 consolidation       & 61.2 (-2.9) \\
\quad w/o domain filtering            & 62.0 (-2.1) \\
\quad w/o hybrid compression          & 63.1 (-1.0) \\
\quad w/o confidence weighting        & 62.5 (-1.6) \\
\quad w/o failure pattern experiences & 60.9 (-3.2) \\
\quad w/o Loop 2 (user corrections)   & 62.7 (-1.4) \\
\bottomrule
\end{tabular}
\end{table}

Stage~1 summarization contributes the largest individual gain (4.3 pp), as
it enables meaningful group comparison across trajectories of variable length.
Failure pattern experiences contribute 3.2 pp, confirming that negative
examples carry significant learning signal.

\subsection{Library Growth and Compression}

The library grows from 0 to approximately 320 entries over 50 sessions, with
two compression events at sessions 30 and 48 (when $|\mathcal{E}| > N_{\max} = 300$).
Post-compression library quality (measured by mean retrieval confidence)
recovers within two sessions, consistent with Proposition~\ref{prop:compression}.

% ============================================================================
% 9. RELATED WORK
% ============================================================================
\section{Related Work}
\label{sec:related}

\paragraph{Continuous Learning for LLMs.}
\citet{mitchell2022memory} and \citet{de2021continual} survey continual
learning approaches for neural networks, most requiring gradient updates.
Agent-GRPO achieves continuous improvement without parameter changes.

\paragraph{Reflexion and Verbal Reinforcement.}
\citet{shinn2023reflexion} propose Reflexion, which prompts agents to verbally
reflect on failures and store reflections in memory. Agent-GRPO extends this
with group-relative advantage extraction across multiple trajectories,
producing more nuanced experiences than single-trajectory reflection.

\paragraph{Voyager and Tool Learning.}
\citet{wang2023voyager} propose Voyager, a lifelong learning agent that
creates new tools as skills. Agent-GRPO complements this by learning
\emph{when and how to use} existing tools, while Voyager focuses on
\emph{creating} new tools. Integration is described in Section~\ref{sec:integration}.

\paragraph{GRPO and Its Extensions.}
GRPO~\citep{shao2024deepseekmath} underpins the reward computation and
advantage formulation in Agent-GRPO. TF-GRPO~\citep{hanzo2026aso} introduced
the training-free adaptation of GRPO for text outputs. Agent-GRPO extends
both to the structured agent setting.

\paragraph{YouTou-Agent and Experience Libraries.}
\citet{tencent2025youtoagent} propose the three-stage pipeline that
Agent-GRPO adapts. The key difference is that YouTou-Agent was designed for
text-only trajectories with a fixed output space, while Agent-GRPO handles
variable-length tool-call sequences with rich intermediate signals.

\paragraph{Tool-Augmented Language Models.}
\citet{schick2023toolformer} train models to use tools via self-supervised
fine-tuning. \citet{yang2024intercode} evaluate models on interactive coding
tasks requiring tool use. Agent-GRPO complements these lines by providing a
training-free adaptation pathway for frozen tool-using models.

\paragraph{Memory-Augmented Agents.}
\citet{park2023generative} use natural-language memory streams for generative
agents. \citet{zhong2024memorybank} introduce MemoryBank for long-term memory
in LLMs. Agent-GRPO is distinguished by its GRPO-based extraction mechanism
that assigns principled confidence scores and supports four compression
strategies, whereas prior systems rely on recency and frequency alone.

\paragraph{Decentralized Semantic Optimization.}
The multi-agent extension of ASO, DSO~\citep{hanzo2026dso}, provides a
Byzantine-fault-tolerant protocol for sharing experiences across agents. Agent-GRPO
is the single-agent foundation on which DSO builds; experiences extracted by
Agent-GRPO can be submitted to the DSO registry for cross-agent benefit.

% ============================================================================
% 10. DISCUSSION AND FUTURE WORK
% ============================================================================
\section{Discussion and Future Work}
\label{sec:discussion}

\paragraph{Multi-Agent GRPO.}
The most natural extension of Agent-GRPO is to the multi-agent setting. When
$n$ agents operate on similar tasks, their experience libraries contain
complementary knowledge. DSO~\citep{hanzo2026dso} provides the aggregation
protocol, but the quality of aggregated experiences depends critically on the
quality of each agent's library---precisely what Agent-GRPO optimizes. Future
work should quantify the marginal benefit of multi-agent sharing as a function
of the single-agent library quality.

\paragraph{Adversarial Robustness.}
The experience library is a potential attack surface: a user who consistently
provides false corrections (Loop 2) or an adversarial tool that returns
misleading outputs could pollute the library with harmful experiences. Several
defenses are available: (i) rate-limiting the confidence increment in
Equation~\eqref{eq:confidence-update}, (ii) requiring consistency across
multiple independent sessions before accepting high-confidence experiences,
(iii) cryptographic attestation of tool outputs as in DSO. A formal analysis
of adversarial robustness is deferred to future work.

\paragraph{Scaling to Millions of Experiences.}
The current library architecture with flat cosine retrieval scales to
$|\mathcal{E}| \lesssim 10{,}000$ entries before retrieval latency becomes a
concern. For larger libraries, hierarchical memory structures (e.g.,
HNSW~\citep{malkov2018efficient} approximate nearest neighbor search) would
maintain sub-millisecond retrieval. Hierarchical compression---grouping
experiences into topic clusters and applying compression at the cluster
level---offers another avenue for scaling.

\paragraph{Curriculum and Experience Ordering.}
Theorem~\ref{thm:monotone} treats all sessions as equally informative. In
practice, there may be a curriculum effect: early sessions on simple tasks
produce foundational experiences (basic tool preferences) that enable better
performance on complex tasks later. Designing session orderings that maximize
$p_c$ in early epochs is an open problem.

\paragraph{Cross-Modal Extensions.}
The current framework assumes text-based tool outputs. Many real-world tools
produce structured data (JSON, CSV, images, audio). Extending the experience
representation to encode modality-specific patterns---for example, ``when
processing image outputs from OCR tools, always validate bounding box
coordinates before downstream use''---requires multi-modal embedding models
and a richer experience schema.

\paragraph{Relationship to Reinforcement Learning.}
Agent-GRPO can be interpreted as a form of case-based reasoning augmented with
group-relative advantage weighting. Unlike RL approaches that update policy
parameters, Agent-GRPO accumulates a growing case library indexed by task
embedding. The theoretical guarantees (Theorems~\ref{thm:monotone}
and~\ref{thm:sample}) are analogous to sample complexity bounds in online
learning, but differ in that the ``policy'' is the retrieval+injection
mechanism rather than model parameters.

% ============================================================================
% 11. CONCLUSION
% ============================================================================
\section{Conclusion}
\label{sec:conclusion}

We have presented Agent-GRPO, a training-free continuous learning framework
that extends Group-Relative Policy Optimization to the agent tool-use setting.
The framework formalizes agent trajectories as typed tuples carrying rich
intermediate signals, adapts the YouTou-Agent three-stage extraction pipeline
to this richer domain, and constructs a semantic experience library whose
entries measurably improve future agent performance through context injection.

The theoretical analysis establishes that the experience library improves
monotonically under mild regularity conditions, with a sample complexity bound
of $O(K_x/\varepsilon^2)$ sessions to achieve $\varepsilon$-improvement.
Empirically, Agent-GRPO achieves a 20.1 percentage point improvement in task
completion rate over a zero-shot baseline on HB-Bench, at a total adaptation
cost below \$20---orders of magnitude below any gradient-based alternative.

The framework integrates naturally with the four-loop self-improvement
architecture of the Hanzo Dev agent, closing the feedback loop between
telemetry, user corrections, session reflection, and library maintenance.
This integration positions Agent-GRPO as the practical foundation for
continuously improving, training-free tool-using agents at production scale.

Code and benchmark data are available at \url{https://github.com/hanzoai/agent-grpo}.

% ============================================================================
% REFERENCES
% ============================================================================
\bibliographystyle{plainnat}
\begin{thebibliography}{99}

\bibitem[Benet(2014)]{benet2014ipfs}
Benet, J. (2014).
\newblock IPFS -- Content Addressed, Versioned, P2P File System.
\newblock \emph{arXiv:1407.3561}.

\bibitem[Brown et al.(2020)]{brown2020language}
Brown, T., Mann, B., Ryder, N., et al. (2020).
\newblock Language models are few-shot learners.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 33, 1877--1901.

\bibitem[De Lange et al.(2021)]{de2021continual}
De Lange, M., Aljundi, R., Masana, M., et al. (2021).
\newblock A continual learning survey: Defying forgetting in classification tasks.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 44(7), 3366--3385.

\bibitem[Hanzo AI Research(2026a)]{hanzo2026aso}
Hanzo AI Research. (2026a).
\newblock Active Semantic Optimization: Training-Free Adaptation via Bayesian
  Product-of-Experts Decoding.
\newblock \emph{Hanzo AI Technical Report HIP-002}, February 2026.

\bibitem[Hanzo AI Research(2026b)]{hanzo2026dso}
Hanzo AI Research. (2026b).
\newblock Decentralized Semantic Optimization: Byzantine-Robust Prior
  Aggregation for Collective Model Adaptation.
\newblock \emph{Hanzo AI Technical Report ZIP-001}, February 2026.

\bibitem[Hinton(2002)]{hinton2002training}
Hinton, G. E. (2002).
\newblock Training products of experts by minimizing contrastive divergence.
\newblock \emph{Neural Computation}, 14(8), 1771--1800.

\bibitem[Hu et al.(2022)]{hu2022lora}
Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. (2022).
\newblock LoRA: Low-Rank Adaptation of Large Language Models.
\newblock \emph{International Conference on Learning Representations (ICLR)}.

\bibitem[Jimenez et al.(2024)]{jimenez2024swebench}
Jimenez, C. E., Yang, J., Wettig, A., et al. (2024).
\newblock SWE-bench: Can Language Models Resolve Real-World GitHub Issues?
\newblock \emph{International Conference on Learning Representations (ICLR)}.

\bibitem[Kirkpatrick et al.(2017)]{kirkpatrick2017overcoming}
Kirkpatrick, J., Pascanu, R., Rabinowitz, N., et al. (2017).
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{Proceedings of the National Academy of Sciences}, 114(13), 3521--3526.

\bibitem[Lamport et al.(1982)]{lamport1982byzantine}
Lamport, L., Shostak, R., and Pease, M. (1982).
\newblock The Byzantine generals problem.
\newblock \emph{ACM Transactions on Programming Languages and Systems}, 4(3), 382--401.

\bibitem[Lewis et al.(2020)]{lewis2020retrieval}
Lewis, P., Perez, E., Piktus, A., et al. (2020).
\newblock Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.
\newblock \emph{NeurIPS}, 33, 9459--9474.

\bibitem[Malkov and Yashunin(2018)]{malkov2018efficient}
Malkov, Y. A. and Yashunin, D. A. (2018).
\newblock Efficient and robust approximate nearest neighbor search using
  Hierarchical Navigable Small World graphs.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence},
  42(4), 824--836.

\bibitem[Mitchell et al.(2022)]{mitchell2022memory}
Mitchell, E., Lin, C., Bosselut, A., Finn, C., and Manning, C. D. (2022).
\newblock Memory-Based Model Editing at Scale.
\newblock \emph{International Conference on Machine Learning (ICML)}.

\bibitem[Ouyang et al.(2022)]{ouyang2022training}
Ouyang, L., Wu, J., Jiang, X., et al. (2022).
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{NeurIPS}, 35, 27730--27744.

\bibitem[Park et al.(2023)]{park2023generative}
Park, J. S., O'Brien, J. C., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. (2023).
\newblock Generative agents: Interactive simulacra of human behavior.
\newblock \emph{ACM Symposium on User Interface Software and Technology (UIST)}.

\bibitem[Reimers and Gurevych(2019)]{reimers2019sentence}
Reimers, N. and Gurevych, I. (2019).
\newblock Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.
\newblock \emph{Conference on Empirical Methods in Natural Language Processing (EMNLP)}.

\bibitem[Schick et al.(2023)]{schick2023toolformer}
Schick, T., Dwivedi-Yu, J., Dess{\`{i}}, R., et al. (2023).
\newblock Toolformer: Language models can teach themselves to use tools.
\newblock \emph{NeurIPS}, 36.

\bibitem[Shao et al.(2024)]{shao2024deepseekmath}
Shao, Z., Wang, P., Zhu, Q., et al. (2024).
\newblock DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open
  Language Models.
\newblock \emph{arXiv:2402.03300}.

\bibitem[Shinn et al.(2023)]{shinn2023reflexion}
Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., and Yao, S. (2023).
\newblock Reflexion: Language Agents with Verbal Reinforcement Learning.
\newblock \emph{NeurIPS}, 36.

\bibitem[Tencent YouTou Lab(2025)]{tencent2025youtoagent}
Tencent YouTou Lab. (2025).
\newblock YouTou-Agent: Training-Free Experience Extraction for Large Language
  Model Agents.
\newblock \emph{arXiv:2510.08191v1}.

\bibitem[Wang et al.(2023)]{wang2023voyager}
Wang, G., Xie, Y., Jiang, Y., et al. (2023).
\newblock Voyager: An Open-Ended Embodied Agent with Large Language Models.
\newblock \emph{arXiv:2305.16291}.

\bibitem[Williams(1992)]{williams1992simple}
Williams, R. J. (1992).
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine Learning}, 8(3--4), 229--256.

\bibitem[Yang et al.(2024)]{yang2024intercode}
Yang, J., Prabhakar, A., Narasimhan, K., and Yao, S. (2024).
\newblock InterCode: Standardizing and Benchmarking Interactive Coding with
  Execution Feedback.
\newblock \emph{NeurIPS}, 36.

\bibitem[Yao et al.(2023)]{yao2023react}
Yao, S., Zhao, J., Yu, D., et al. (2023).
\newblock ReAct: Synergizing Reasoning and Acting in Language Models.
\newblock \emph{International Conference on Learning Representations (ICLR)}.

\bibitem[Zhong et al.(2024)]{zhong2024memorybank}
Zhong, W., Guo, L., Gao, Q., Ye, H., and Wang, Y. (2024).
\newblock MemoryBank: Enhancing Large Language Models with Long-Term Memory.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  38(17), 19724--19731.

\end{thebibliography}

\end{document}
