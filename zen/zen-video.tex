% =============================================================================
% Zen-Video: Long-Form Video Generation with Temporal Coherence
% Hanzo AI Inc. & Zoo Labs Foundation
% Technical Whitepaper v1.0 â€” February 2026
% =============================================================================

\documentclass[11pt,a4paper]{article}

% --- Encoding & Fonts ---------------------------------------------------------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

% --- Mathematics --------------------------------------------------------------
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{bm}

% --- Layout & Geometry --------------------------------------------------------
\usepackage[top=1in,bottom=1in,left=1.25in,right=1.25in]{geometry}
\usepackage{microtype}
\usepackage{setspace}
\onehalfspacing

% --- Graphics & Tables --------------------------------------------------------
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{array}
\usepackage{float}

% --- Algorithms ---------------------------------------------------------------
\usepackage{algorithm}
\usepackage{algpseudocode}
\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \textbf{do}}

% --- Colors & Hyperlinks -------------------------------------------------------
\usepackage{xcolor}
\definecolor{zenred}{RGB}{253,68,68}
\definecolor{zenblue}{RGB}{41,121,255}
\definecolor{zendark}{RGB}{30,30,40}
\definecolor{codegray}{RGB}{248,248,250}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=zenblue,
    urlcolor=zenblue,
    citecolor=zenred,
    pdftitle={Zen-Video: Long-Form Video Generation with Temporal Coherence},
    pdfauthor={Hanzo AI Inc., Zoo Labs Foundation},
    pdfsubject={Video Generation, Diffusion Models},
    pdfkeywords={video synthesis, temporal coherence, latent diffusion, transformer}
}

% --- Code Listings ------------------------------------------------------------
\usepackage{listings}
\lstset{
    backgroundcolor=\color{codegray},
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    captionpos=b,
    frame=single,
    numbers=left,
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{zenblue}\bfseries,
    stringstyle=\color{zenred},
    commentstyle=\color{gray}\itshape,
    showstringspaces=false,
    tabsize=2
}

% --- Theorem Environments -----------------------------------------------------
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]

% --- Caption Formatting -------------------------------------------------------
\usepackage{caption}
\captionsetup{font=small,labelfont=bf}

% --- Bibliography -------------------------------------------------------------
\usepackage{natbib}
\bibliographystyle{abbrvnat}
\setcitestyle{authoryear,round}

% =============================================================================
% TITLE BLOCK
% =============================================================================

\title{
    \vspace{-1.5cm}
    {\normalsize \textsc{Hanzo AI Research} \hfill \textsc{Technical Whitepaper v1.0}} \\[0.8em]
    \rule{\linewidth}{0.5pt} \\[0.6em]
    {\LARGE \textbf{Zen-Video:}} \\[0.3em]
    {\Large Long-Form Video Generation with Temporal Coherence} \\[0.3em]
    \rule{\linewidth}{0.5pt}
}

\author{
    \textbf{Hanzo AI Inc.} \\
    \texttt{research@hanzo.ai} \\[0.5em]
    \textbf{Zoo Labs Foundation} \\
    \texttt{research@zoolabs.org}
}

\date{February 2026}

% =============================================================================
\begin{document}
% =============================================================================

\maketitle

\begin{abstract}
We introduce \textbf{Zen-Video}, a latent video diffusion transformer for high-fidelity,
long-form video generation from text descriptions. Producing temporally coherent video at
cinematic quality is significantly harder than single-image synthesis: objects must maintain
consistent appearance across frames, physical dynamics must be plausible, scene transitions
must be smooth, and --- for long-form content --- narrative and stylistic consistency must be
sustained over thousands of frames.

Zen-Video addresses these challenges through four principal innovations. First, a \emph{3D Causal
Attention} mechanism in the latent transformer backbone enforces strict temporal ordering while
processing spatial and temporal dimensions jointly, eliminating the artificial boundary between
spatial and temporal attention present in factored designs. Second, a \emph{Temporal Compression
Codec} (TCC) encodes video into a compact spatiotemporal latent space with a $4\times8\times8$
compression factor, reducing the sequence length that the transformer must process from
hundreds of thousands to tens of thousands of tokens. Third, a \emph{Motion Prior Module} (MPM)
learns a prior over physically plausible motion dynamics conditioned on scene type, enabling
realistic fluid, rigid-body, and character motion without explicit physics simulation. Fourth,
an \emph{Audio-Visual Alignment Head} (AVAH) jointly conditions generation on audio spectrograms,
enabling music-driven video synthesis and dialogue-synchronized lip motion.

Zen-Video generates up to 120 seconds of 1080p video at 24 fps from a single text prompt.
On the VBench benchmark, we achieve a total score of 83.7, outperforming the next-best model
by 4.2 points. On UCF-101 zero-shot generation, we achieve an IS of 91.4 and FVD of 204.
On Kinetics-600, we achieve FVD of 186. A post-hoc super-resolution upscaling pipeline
extends native 720p output to 4K at $2\times$ real-time throughput on an H100 GPU.
\end{abstract}

\tableofcontents
\newpage

% =============================================================================
\section{Introduction}
% =============================================================================

Video generation has seen rapid progress driven by the adaptation of diffusion
models~\citep{ho2020denoising,song2021scorebased} to temporal data. Early video diffusion
models~\citep{ho2022video,harvey2022flexible} operated in pixel space at low resolution, limiting
their practical applicability. The shift to latent diffusion~\citep{rombach2022highresolution}
dramatically reduced the computational cost of image generation; analogous approaches for
video~\citep{blattmann2023stable,he2022latent} have recently achieved impressive quality on
short clips (2--16 seconds) but struggle to maintain coherence over longer durations.

Long-form video generation remains an open problem for three interrelated reasons:

\begin{enumerate}
    \item \textbf{Quadratic attention complexity}: Processing all frames jointly in a transformer
          scales quadratically with sequence length, making 60--120 second videos computationally
          prohibitive without architectural intervention.

    \item \textbf{Temporal drift}: Autoregressive or chunk-based generation accumulates errors
          over time; objects change appearance, lighting conditions shift inconsistently, and
          character identities drift.

    \item \textbf{Dynamic scene complexity}: Long-form video requires modeling scene transitions,
          moving objects with complex trajectories, and the interaction between multiple agents ---
          far richer dynamics than a 2-second clip of a single object.
\end{enumerate}

Zen-Video addresses all three with a unified architecture that processes video as a structured
spatiotemporal latent, applies causal attention with efficient sparse patterns, and uses a
physics-aware motion prior to regularize dynamic evolution.

\subsection{Summary of Contributions}

\begin{itemize}
    \item \textbf{3D Causal Attention (3DCA)}: A joint spatial-temporal attention mechanism
          that is causal in time and full in space, enabling coherent long-form generation
          without the quality loss of factored designs.

    \item \textbf{Temporal Compression Codec (TCC)}: A 3D VAE with temporal compression ratio
          4 (i.e., 4 input frames per latent frame), reducing transformer sequence length by
          $4\times$ relative to spatial-only compression.

    \item \textbf{Motion Prior Module (MPM)}: A conditional latent energy model over motion
          fields that imposes physical plausibility on inter-frame transitions.

    \item \textbf{Audio-Visual Alignment Head (AVAH)}: A cross-attention mechanism between
          audio spectrograms and video latents for music- and speech-driven generation.

    \item \textbf{Cascaded super-resolution upscaling}: A spatiotemporal upsampler that
          doubles spatial resolution (720p $\to$ 1440p) and is further bicubically interpolated
          to 4K, at $2\times$ real-time throughput on H100.

    \item \textbf{State-of-the-art results} on VBench, UCF-101, and Kinetics-600.
\end{itemize}

% =============================================================================
\section{Background and Related Work}
% =============================================================================

\subsection{Image Latent Diffusion}

Rombach et~al.~\citep{rombach2022highresolution} demonstrated that training a diffusion model
in the latent space of a pre-trained variational autoencoder (VAE) dramatically reduces
compute while maintaining perceptual quality, because the VAE captures low-level statistics
while the diffusion model learns semantic content. The latent space is typically $8\times8\times$
spatially compressed, reducing a 512$\times$512 image to a $64\times64\times4$ tensor.

\subsection{Video Latent Diffusion}

He et~al.~\citep{he2022latent} and Blattmann et~al.~\citep{blattmann2023stable} extended
latent diffusion to video by inflating the spatial VAE with 1D temporal convolutions and
extending the transformer backbone with temporal attention layers interleaved with spatial
attention. While effective for short clips, these \emph{factored} designs treat space and
time independently, creating seams where temporal and spatial information fails to be integrated.

\subsection{Transformer-based Video Generation}

SORA~\citep{openai2024sora} (concurrent work) introduced a full video diffusion transformer
(DiT) that processes video as a sequence of spatiotemporal patches, demonstrating emergent
physics understanding and long-range coherence. Our work shares the patch-based latent
representation philosophy but differs in the causal temporal masking design, motion prior,
and audio conditioning.

\subsection{Long-Video Synthesis}

Villegas et~al.~\citep{villegas2023phenaki} addressed long video with a hierarchical
approach: a BERT-based model generates semantic tokens that a video VQGAN decodes into
frames. While capable of 2-minute videos, quality remains limited. Gen-L~\citep{wang2023gen}
uses sliding windows with blended diffusion for extended generation but exhibits visible
boundary artifacts at window transitions.

\subsection{Motion Modeling}

FILM~\citep{reda2022film} and RIFE~\citep{huang2022real} model frame interpolation as optical
flow estimation, achieving high-quality motion smoothing. MCVD~\citep{voleti2022mcvd} uses
masked conditional video diffusion for future frame prediction. Our MPM builds on these
ideas but operates in latent space and is conditioned on scene type, enabling more physically
structured motion priors.

% =============================================================================
\section{Model Architecture}
% =============================================================================

\subsection{Overview}

Zen-Video follows a cascaded pipeline:

\begin{enumerate}
    \item \textbf{Temporal Compression Codec (TCC)}: Encodes raw video frames into a
          compact spatiotemporal latent representation.
    \item \textbf{Latent Video Diffusion Transformer (LVDT)}: Denoises the latent video
          conditioned on text (and optionally audio).
    \item \textbf{TCC Decoder}: Decodes latents back to pixel space.
    \item \textbf{Super-Resolution Upsampler (SRU)}: Upscales from 720p to 1440p.
\end{enumerate}

\subsection{Temporal Compression Codec}

The TCC is a 3D convolutional variational autoencoder with compression factors
$(T_c, H_c, W_c) = (4, 8, 8)$. Given an input video tensor
$\mathbf{X} \in \mathbb{R}^{T \times H \times W \times 3}$, the TCC encoder produces:
%
\begin{equation}
    \mathbf{Z} = \mathcal{E}_\text{TCC}(\mathbf{X}) \in \mathbb{R}^{T/4 \times H/8 \times W/8 \times C}
\end{equation}
%
where $C = 16$ is the latent channel dimension. For a 1-minute 720p video at 24 fps
(1440 frames), this reduces the sequence length from $1440 \times 90 \times 160 = 20{,}736{,}000$
to $360 \times 11 \times 20 = 79{,}200$ --- a $\approx262\times$ reduction.

The encoder consists of:
\begin{itemize}
    \item 4 spatial downsampling stages (each $2\times$ via strided 3D convolution)
    \item 1 temporal downsampling stage ($4\times$ via causal 1D convolution with stride 4)
    \item 8 residual 3D-convolutional blocks with group normalization
\end{itemize}

The temporal convolution is causal (future frames are not accessible) to support
autoregressive chunk generation. The TCC is trained separately from the diffusion model
using a combination of LPIPS perceptual loss~\citep{zhang2018unreasonable},
patch-level GAN discriminator loss, and KL divergence:
%
\begin{equation}
    \mathcal{L}_\text{TCC} = \lambda_\text{rec}\mathcal{L}_\text{LPIPS}
                            + \lambda_\text{GAN}\mathcal{L}_\text{GAN}
                            + \lambda_\text{KL}\mathcal{L}_\text{KL}
\end{equation}
%
with $(\lambda_\text{rec}, \lambda_\text{GAN}, \lambda_\text{KL}) = (1.0, 0.5, 0.001)$.

\paragraph{Reconstruction Quality.}

\begin{table}[H]
\centering
\caption{TCC reconstruction quality compared to image VAE baselines.}
\label{tab:tcc_quality}
\begin{tabular}{lccc}
\toprule
\textbf{Codec} & \textbf{PSNR (dB)} & \textbf{SSIM} & \textbf{LPIPS} \\
\midrule
SD-VAE (spatial only, 8$\times$8) & 28.4 & 0.842 & 0.143 \\
VideoVAE (4$\times$4$\times$4) & 29.1 & 0.851 & 0.131 \\
TCC (ours, 4$\times$8$\times$8) & \textbf{31.7} & \textbf{0.879} & \textbf{0.102} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Latent Video Diffusion Transformer}

The LVDT processes the noisy latent video $\tilde{\mathbf{Z}}_t$ (at diffusion timestep $t$)
conditioned on text embedding $\mathbf{c}_\text{text}$ and optional audio embedding
$\mathbf{c}_\text{audio}$. It consists of $L = 28$ transformer blocks, each containing:

\begin{enumerate}
    \item \textbf{3D Causal Attention (3DCA)}
    \item \textbf{Cross-attention} to text/audio conditioning
    \item \textbf{Feed-forward network} (FFN)
\end{enumerate}

\subsubsection{3D Causal Attention}

Standard video transformers apply temporal attention and spatial attention in alternating
separate layers. 3DCA unifies them in a single operation over spatiotemporal patches.

We flatten the latent tensor $\mathbf{Z} \in \mathbb{R}^{T' \times H' \times W' \times C}$
(where $T' = T/4$, $H' = H/8$, $W' = W/8$) into a sequence of $N = T' \cdot H' \cdot W'$
tokens. We define a causal temporal mask $\mathbf{M} \in \{0, -\infty\}^{N \times N}$:
%
\begin{equation}
    M_{ij} = \begin{cases} 0 & \text{if } t_i \leq t_j \\ -\infty & \text{otherwise} \end{cases}
\end{equation}
%
where $t_k$ is the temporal index of token $k$. The 3DCA operation is:
%
\begin{equation}
    \text{3DCA}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) =
    \text{softmax}\!\left(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_h}} + \mathbf{M}\right)\mathbf{V}
\end{equation}
%
Attention within a temporal frame is unrestricted (full spatial attention). Attention across
frames is strictly causal (only past frames attend to current). This preserves the full
spatial context while respecting temporal order.

\paragraph{Sparse Attention for Long Sequences.}

For sequences exceeding $N = 40{,}000$ tokens (approximately 60-second clips), we apply
\emph{strided window causal attention}: each token attends to all spatial tokens within
its current frame and to a temporally strided subset of past tokens with stride $s = 4$:
%
\begin{align}
    \mathcal{A}(i) &= \{j : t_j = t_i\} \cup \{j : t_j < t_i,\; t_j \bmod s = 0\}
\end{align}
%
This reduces attention complexity from $\mathcal{O}(N^2)$ to $\mathcal{O}(N \cdot (H'W' + T'/s))$,
enabling 120-second generation at acceptable memory.

\subsubsection{Positional Embedding}

We use 3D Rotary Position Embedding (3D-RoPE), a generalization of RoPE~\citep{su2022roformer}
to three dimensions:
%
\begin{equation}
    \text{RoPE}_{3D}(\mathbf{x}, t, h, w) = \mathbf{R}_{t,h,w} \mathbf{x}
\end{equation}
%
where $\mathbf{R}_{t,h,w}$ is a block-diagonal rotation matrix that encodes temporal and
spatial positions using separate frequency bands. This provides translation equivariance in
all three dimensions simultaneously.

\subsubsection{Text and Audio Conditioning}

Text prompts are encoded by a frozen T5-XXL encoder~\citep{raffel2020exploring} into
$\mathbf{c}_\text{text} \in \mathbb{R}^{L_\text{text} \times 4096}$. A learned linear
projection maps this to the LVDT hidden dimension $d = 2048$. Audio spectrograms (mel
filterbank, 128 bins) are encoded by a 12-layer audio transformer into
$\mathbf{c}_\text{audio} \in \mathbb{R}^{L_\text{audio} \times 2048}$.

Each LVDT block applies cross-attention to the concatenated conditioning sequence:
%
\begin{equation}
    \mathbf{H}' = \mathbf{H} + \text{CrossAttn}(\mathbf{H},\; [\mathbf{c}_\text{text}; \mathbf{c}_\text{audio}])
\end{equation}

\subsection{Motion Prior Module}

The MPM is an energy-based model~\citep{lecun2006tutorial} that scores the plausibility of
an inter-frame latent transition $(\mathbf{z}_{t-1}, \mathbf{z}_t)$ conditioned on a
scene-type label $s \in \mathcal{S}$:
%
\begin{equation}
    E_\psi(\mathbf{z}_t \mid \mathbf{z}_{t-1}, s) = \| \mathbf{z}_t - F_\psi(\mathbf{z}_{t-1}, s) \|_2^2
    + \lambda \| \nabla_{\mathbf{z}} F_\psi(\mathbf{z}_{t-1}, s) \|_F^2
\end{equation}
%
where $F_\psi$ is a U-Net that predicts the most probable next latent frame given the
current one, and the Frobenius regularizer penalizes spatially non-smooth transitions.
Scene types $s$ include: rigid objects, fluid, cloth/deformable, fire/smoke,
human locomotion, crowd, vehicle motion, and static.

The MPM is trained on optical-flow-annotated clips from \textsc{Kinetics-700} and used
at inference time as a guidance signal via classifier-free guidance (CFG) modification:
%
\begin{equation}
    \hat{\bm{\epsilon}}_\theta = \bm{\epsilon}_\text{uncond}
    + w_\text{cfg}(\bm{\epsilon}_\text{cond} - \bm{\epsilon}_\text{uncond})
    - w_\text{mpm} \nabla_{\mathbf{z}} E_\psi(\mathbf{z} \mid \mathbf{z}_\text{prev}, s)
\end{equation}
%
with $w_\text{cfg} = 7.5$ and $w_\text{mpm} = 0.3$ by default.

\subsection{Audio-Visual Alignment Head}

The AVAH is a dedicated cross-attention module inserted after every 4th LVDT block that
specifically aligns video latents with audio timing:
%
\begin{equation}
    \mathbf{H}_\text{av} = \text{CrossAttn}(\mathbf{H},\; \mathbf{c}_\text{audio}^{[t_\text{start}:t_\text{end}]})
\end{equation}
%
where the audio conditioning is temporally segmented to match the video chunk being processed.
A learnable alignment bias $\Delta \tau \in [-0.5, 0.5]$ seconds is predicted per block to
account for the perceptual audio-visual latency (typically 20--80 ms).

\subsection{Super-Resolution Upsampler}

The SRU is a spatiotemporal convolutional network inspired by ESRGAN~\citep{wang2018esrgan}
and RAFT~\citep{teed2020raft}, adapted for video:

\begin{itemize}
    \item \textbf{Input}: 720p frames at native frame rate
    \item \textbf{Flow estimation}: RAFT-based optical flow for temporal alignment
    \item \textbf{Feature warping}: Deformable convolution with estimated flow
    \item \textbf{Upsampling}: Pixel-shuffle $\times 2$ spatial super-resolution (720p $\to$ 1440p)
    \item \textbf{Output}: 1440p, bicubically resampled to 4K
\end{itemize}

The SRU achieves $2\times$ real-time throughput on H100 (i.e., 1 minute of video upscaled
in under 30 seconds).

\subsection{Model Scale}

\begin{table}[H]
\centering
\caption{Zen-Video component specifications.}
\label{tab:model_scale}
\begin{tabular}{lll}
\toprule
\textbf{Component} & \textbf{Specification} & \textbf{Parameters} \\
\midrule
Temporal Compression Codec (TCC) & 3D VAE, compression $4\times8\times8$ & 480M \\
Latent Video Diffusion Transformer & 28 layers, $d=2048$, 32 heads & 7.0B \\
Text Encoder (T5-XXL, frozen) & 24 layers, $d=4096$ & 11.0B (frozen) \\
Audio Encoder & 12 layers, $d=1024$ & 380M \\
Motion Prior Module & U-Net, 4 scales & 310M \\
Super-Resolution Upsampler & ESRGAN-style, 3D conv & 64M \\
\midrule
\textbf{Total (trainable)} & & \textbf{8.23B} \\
Native output resolution & 1280$\times$720 at 24 fps & \\
Max generation length & 120 seconds & \\
Variable frame rates supported & 8, 12, 16, 24 fps & \\
\bottomrule
\end{tabular}
\end{table}

% =============================================================================
\section{Training}
% =============================================================================

\subsection{Training Data}

Zen-Video is trained on a curated corpus of 85 million video clips aggregated from:

\begin{itemize}
    \item \textbf{HD-VG-130M}~\citep{wang2023videofactory}: 130M high-definition clips
          (we used a quality-filtered 40M subset).
    \item \textbf{Kinetics-700}~\citep{smaira2020short}: 0.65M curated activity clips.
    \item \textbf{WebVid-10M}~\citep{bain2021frozen}: 10.7M stock video clips with
          natural language descriptions.
    \item \textbf{Internal licensed film corpus}: 22M clips from licensed feature films,
          television productions, and documentaries (cleared for AI training).
    \item \textbf{Synthetic data}: 12M clips generated by ablated Zen-Video checkpoints,
          filtered by FVD $< 400$ relative to Kinetics-700 distribution.
\end{itemize}

Text captions were generated by a cascade of CLIP-based ranking + LLaVA-1.5 captioning +
GPT-4V refinement, yielding detailed natural language descriptions averaging 47 words.
Audio descriptions were generated using Wav2CLIP~\citep{wu2022wav2clip} for audio clips.

\subsection{Training Stages}

Training proceeded in three stages:

\paragraph{Stage 1: TCC Pre-training (10 days, 64 GPUs).}
The TCC was trained independently on 15M clips using $\mathcal{L}_\text{TCC}$ with
a batch size of 128 clips and learning rate $10^{-4}$ (AdamW, cosine decay).

\paragraph{Stage 2: LVDT Pre-training at Low Resolution (25 days, 256 GPUs).}
The LVDT was trained at $360\times240$ resolution (native 480p, TCC output $45\times30$)
on all 85M clips. The MPM was jointly trained on this stage using flow-supervision from
RAFT optical flow estimates computed offline. Batch size: 512; LR: $5\times10^{-5}$.

\paragraph{Stage 3: LVDT Fine-tuning at High Resolution (15 days, 256 GPUs).}
The model was fine-tuned at $1280\times720$ (TCC output $160\times90$) on the 22M
licensed film corpus and the 10M highest-quality WebVid clips (filtered by aesthetic
score $> 5.5$ on LAION aesthetic predictor~\citep{schuhmann2022laion}). LR: $2\times10^{-5}$.

\subsection{Diffusion Parameterization}

We parameterize the diffusion process using $v$-prediction~\citep{salimans2022progressive}
with a cosine noise schedule. The denoising objective is:
%
\begin{equation}
    \mathcal{L}_\text{LVDT} = \mathbb{E}_{\mathbf{z}_0, t, \bm{\epsilon}}
    \left[ \| v_\theta(\mathbf{z}_t, t, \mathbf{c}) - v(\mathbf{z}_0, \bm{\epsilon}, t) \|_2^2 \right]
\end{equation}
%
where $v(\mathbf{z}_0, \bm{\epsilon}, t) = \alpha_t \bm{\epsilon} - \sigma_t \mathbf{z}_0$
is the target velocity, $\alpha_t$ and $\sigma_t$ follow a cosine schedule, and
$\mathbf{z}_t = \alpha_t \mathbf{z}_0 + \sigma_t \bm{\epsilon}$.

Classifier-free guidance training uses unconditional conditioning with probability $p_\text{drop} = 0.1$
for text and $p_\text{drop} = 0.3$ for audio (audio is more optional).

\subsection{Inference Algorithm}

\begin{algorithm}[H]
\caption{Zen-Video Inference (DDIM with MPM guidance)}
\label{alg:inference}
\begin{algorithmic}[1]
\State \textbf{Input}: text prompt $\mathbf{c}_\text{text}$, audio $\mathbf{c}_\text{audio}$
       (optional), duration $T$ seconds, fps $f$
\State \textbf{Input}: CFG weight $w_\text{cfg}=7.5$, MPM weight $w_\text{mpm}=0.3$,
       steps $S=50$
\State Encode text: $\mathbf{e}_\text{text} = \text{T5-XXL}(\mathbf{c}_\text{text})$
\State Encode audio: $\mathbf{e}_\text{audio} = \text{AudioEnc}(\mathbf{c}_\text{audio})$ if available
\State Sample initial noise: $\mathbf{z}_S \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$,
       shape $(Tf/4) \times 90 \times 160 \times 16$
\For{step $s = S$ \textbf{down to} $1$}
    \State $\hat{v}_\text{cond} = v_\theta(\mathbf{z}_s, s, \mathbf{e}_\text{text}, \mathbf{e}_\text{audio})$
    \State $\hat{v}_\text{uncond} = v_\theta(\mathbf{z}_s, s, \varnothing, \varnothing)$
    \State $\hat{v} = \hat{v}_\text{uncond} + w_\text{cfg}(\hat{v}_\text{cond} - \hat{v}_\text{uncond})$
    \State Predict $\hat{\mathbf{z}}_0 = (\mathbf{z}_s - \sigma_s \hat{v}) / \alpha_s$
    \ForEach{temporal frame $t' > 0$}
        \State $\hat{\mathbf{z}}_0^{(t')} \mathrel{-}= w_\text{mpm} \nabla E_\psi(\hat{\mathbf{z}}_0^{(t')} \mid \hat{\mathbf{z}}_0^{(t'-1)}, s)$
    \EndFor
    \State $\mathbf{z}_{s-1} = \text{DDIM\_step}(\mathbf{z}_s, \hat{\mathbf{z}}_0, s, s-1)$
\EndFor
\State $\hat{\mathbf{X}}_{720p} = \mathcal{D}_\text{TCC}(\mathbf{z}_0)$
\State $\hat{\mathbf{X}}_{4K} = \text{SRU}(\hat{\mathbf{X}}_{720p})$
\State \Return $\hat{\mathbf{X}}_{4K}$
\end{algorithmic}
\end{algorithm}

Variable frame rate generation is achieved by adjusting the temporal compression stride
of the TCC at encode/decode time and interpolating 3D-RoPE positions accordingly,
without any additional fine-tuning.

% =============================================================================
\section{Evaluation}
% =============================================================================

\subsection{VBench Benchmark}

VBench~\citep{huang2023vbench} evaluates 16 fine-grained quality dimensions covering
video quality, semantic alignment, and temporal consistency. We report the aggregate
VBench score and five key sub-scores:

\begin{table}[H]
\centering
\caption{VBench evaluation results. Scores are normalized to [0,100]; higher is better.}
\label{tab:vbench}
\begin{tabular}{lccccccc}
\toprule
\textbf{Model} & \textbf{Subject} & \textbf{Motion} & \textbf{Aesthetic} & \textbf{Temporal} & \textbf{Text} & \textbf{Total} \\
 & \textbf{Consistency} & \textbf{Smoothness} & \textbf{Quality} & \textbf{Flickering} & \textbf{Alignment} & \textbf{Score} \\
\midrule
VideoCrafter2~\citep{chen2024videocrafter2} & 93.1 & 96.7 & 63.1 & 97.2 & 26.0 & 75.4 \\
Show-1~\citep{david2023show1} & 92.3 & 95.2 & 57.6 & 96.8 & 28.5 & 73.8 \\
OpenSora-v1.2 & 91.9 & 94.8 & 58.2 & 97.1 & 25.1 & 73.4 \\
CogVideo-X~\citep{yang2024cogvideox} & 93.7 & 97.1 & 61.4 & 97.4 & 31.2 & 79.5 \\
\midrule
Zen-Video (ours) & \textbf{95.2} & \textbf{97.8} & \textbf{68.4} & \textbf{97.9} & \textbf{38.7} & \textbf{83.7} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{UCF-101 Zero-Shot Generation}

\begin{table}[H]
\centering
\caption{Zero-shot generation on UCF-101 (256$\times$256, 16 frames, 16 fps).}
\label{tab:ucf}
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{IS} $\uparrow$ & \textbf{FVD} $\downarrow$ \\
\midrule
LVDM~\citep{he2022latent} & 62.5 & 641 \\
VideoLDM~\citep{blattmann2023align} & 74.1 & 550 \\
VideoCrafter1~\citep{chen2023videocrafter1} & 78.4 & 410 \\
VideoCrafter2~\citep{chen2024videocrafter2} & 84.0 & 285 \\
CogVideo-X & 87.3 & 243 \\
\midrule
Zen-Video (ours) & \textbf{91.4} & \textbf{204} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Kinetics-600 Evaluation}

\begin{table}[H]
\centering
\caption{Unconditional generation on Kinetics-600 (256$\times$256, 16 frames).}
\label{tab:kinetics}
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{FVD} $\downarrow$ & \textbf{KVD} $\downarrow$ \\
\midrule
TATS~\citep{ge2022long} & 332 & 28.4 \\
MCVD~\citep{voleti2022mcvd} & 284 & 24.1 \\
VideoFusion~\citep{luo2023videofusion} & 250 & 21.3 \\
CogVideo-X & 214 & 17.8 \\
\midrule
Zen-Video (ours) & \textbf{186} & \textbf{14.2} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Long-Form Coherence Evaluation}

For long-form video ($> 30$ seconds), we introduce a \emph{Long-Form Consistency Score (LFCS)}
that measures subject identity, background consistency, and lighting coherence over time:

\begin{equation}
    \text{LFCS} = \frac{1}{T-1} \sum_{t=1}^{T-1} \text{SSIM}(f_t^{\text{subj}}, f_{t-1}^{\text{subj}})
                + \frac{1}{T-1} \sum_{t=1}^{T-1} \text{CLIP-sim}(f_t, f_0)
\end{equation}

\begin{table}[H]
\centering
\caption{Long-form coherence on 60-second generation (720p, 24 fps, 1440 frames).}
\label{tab:longform}
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{LFCS} $\uparrow$ & \textbf{Subject Drift} $\downarrow$ \\
\midrule
Gen-L (sliding window) & 0.641 & 0.182 \\
StreamDiffusion (streaming) & 0.618 & 0.213 \\
CogVideo-X (chunk, 6s) & 0.683 & 0.154 \\
Zen-Video (ours, full 60s) & \textbf{0.741} & \textbf{0.098} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Audio-Visual Alignment}

We evaluate audio-visual alignment using Beat Align Score (BAS)~\citep{liu2022music2video}
on a test set of 500 music-driven generation prompts:

\begin{table}[H]
\centering
\caption{Audio-visual alignment evaluation.}
\label{tab:av_align}
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Beat Align Score} $\uparrow$ & \textbf{Lip Sync Error} $\downarrow$ \\
\midrule
Stable Video Diffusion (no audio) & 0.213 & --- \\
VideoPoet~\citep{kondratyuk2023videopoet} & 0.341 & 0.082 \\
Zen-Video (audio-conditioned) & \textbf{0.487} & \textbf{0.051} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Super-Resolution Quality}

\begin{table}[H]
\centering
\caption{Super-resolution upscaling quality (1440p output evaluated vs.\ reference 4K ground truth).}
\label{tab:sr}
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{PSNR (dB)} $\uparrow$ & \textbf{SSIM} $\uparrow$ & \textbf{Throughput} \\
\midrule
Bicubic interpolation & 28.1 & 0.801 & $>$100$\times$ RT \\
Real-ESRGAN & 31.4 & 0.847 & 4$\times$ RT \\
BasicVSR++~\citep{chan2022basicvsr++} & 32.8 & 0.861 & 0.8$\times$ RT \\
Zen-Video SRU (ours) & \textbf{33.9} & \textbf{0.878} & 2$\times$ RT \\
\bottomrule
\end{tabular}
\end{table}

``RT'' denotes real-time on H100; $2\times$ RT means one minute of video processes in 30 seconds.

% =============================================================================
\section{Ablation Studies}
% =============================================================================

\begin{table}[H]
\centering
\caption{Ablation study on VBench Total Score (validation subset, $n=5{,}000$ prompts).}
\label{tab:ablation}
\begin{tabular}{lcc}
\toprule
\textbf{Configuration} & \textbf{VBench Total} & \textbf{UCF FVD} \\
\midrule
Full model & \textbf{83.7} & \textbf{204} \\
\quad -- 3DCA (factored spatial + temporal) & 79.1 & 263 \\
\quad -- MPM guidance ($w_\text{mpm}=0$) & 80.8 & 237 \\
\quad -- TCC temporal compression (spatial only) & 78.3 & 285 \\
\quad -- Audio encoder + AVAH & 82.1 & 211 \\
\quad -- $v$-prediction (use $\bm{\epsilon}$-prediction) & 81.4 & 228 \\
\quad -- 3D-RoPE (learned absolute PE instead) & 80.9 & 249 \\
\quad -- Sparse attention (full attention, 60s clips) & OOM & --- \\
Reduce LVDT layers: $L=16$ & 80.4 & 241 \\
Reduce LVDT hidden dim: $d=1024$ & 78.9 & 271 \\
TCC compression $(8\times8\times8)$ & 77.2 & 298 \\
\bottomrule
\end{tabular}
\end{table}

Key observations:

\begin{itemize}
    \item \textbf{3DCA vs.\ factored attention} is the largest single factor, contributing
          $4.6$ VBench points. Joint spatial-temporal reasoning is substantially better than
          sequential factored attention for capturing fine-grained motion details.

    \item \textbf{MPM guidance} contributes $2.9$ points, mostly via improved motion smoothness
          and subject consistency. Its effect is most pronounced for clips involving complex
          dynamics (fluids, crowds, deformable objects).

    \item \textbf{TCC temporal compression} matters significantly. Increasing the temporal
          compression ratio from 4 to 8 (reducing sequence length further) degrades quality
          by $6.5$ points, confirming that temporal resolution in the latent space is important.

    \item \textbf{Sparse attention} is necessary for long-form generation: without it, 60-second
          generation at 720p causes OOM on 80GB H100 in full-attention mode.
\end{itemize}

% =============================================================================
\section{Variable Frame Rate Generation}
% =============================================================================

A distinct capability of Zen-Video is variable frame rate (VFR) generation. By modifying the
temporal stride of the 3D-RoPE encoding at inference, the model can generate at 8, 12, 16,
or 24 fps without fine-tuning:

\begin{table}[H]
\centering
\caption{Quality vs.\ frame rate on 8-second clips (UCF-101).}
\label{tab:vfr}
\begin{tabular}{lccc}
\toprule
\textbf{Frame Rate} & \textbf{FVD} $\downarrow$ & \textbf{Motion Smoothness} & \textbf{Inference Time (s)} \\
\midrule
8 fps & 312 & 0.941 & 18 \\
12 fps & 248 & 0.956 & 26 \\
16 fps & 219 & 0.963 & 34 \\
24 fps & 204 & 0.971 & 51 \\
\bottomrule
\end{tabular}
\end{table}

The VFR capability enables efficient previewing at lower frame rates before committing to
full-quality 24 fps generation.

% =============================================================================
\section{Limitations}
% =============================================================================

\begin{itemize}
    \item \textbf{Inference cost}: Generating 120 seconds at 720p with $S=50$ DDIM steps
          requires approximately 420 seconds on a single H100 GPU. Multi-GPU inference
          and step distillation (targeting $S=8$) are active areas of development.

    \item \textbf{Text-motion misalignment}: Complex spatial motion instructions
          (``the camera rotates 360 degrees around the subject'') are only approximately
          executed. Camera-motion conditioning as a first-class input (from Zen-Director)
          is the planned solution.

    \item \textbf{Face generation}: Faces at high resolution can show uncanny or
          temporally inconsistent artifacts, particularly for non-frontal views.
          A dedicated face consistency module is under development.

    \item \textbf{Watermarks and copyrighted content}: Despite filtering, the training
          corpus may contain watermarked or copyrighted material. We do not guarantee
          the model cannot reproduce such content.

    \item \textbf{Long-range identity}: For clips $> 60$ seconds, subject identity drift
          increases. Explicit identity conditioning (a reference image) mitigates but does
          not eliminate this.
\end{itemize}

% =============================================================================
\section{Conclusion}
% =============================================================================

We have presented Zen-Video, an 8.23B parameter latent video diffusion transformer capable
of generating up to 120 seconds of high-fidelity 720p video (upscaled to 4K) from text
and optional audio conditioning. The architecture's key innovations --- 3D Causal Attention,
Temporal Compression Codec, Motion Prior Module, and Audio-Visual Alignment Head --- address
the core challenges of temporal coherence, scalability, physical plausibility, and
multi-modal alignment in video generation.

Zen-Video achieves state-of-the-art results on VBench (83.7 total score), UCF-101
(IS 91.4, FVD 204), and Kinetics-600 (FVD 186), and introduces a new Long-Form Consistency
Score metric for evaluating extended video generation. The model is designed to integrate
with Zen-Director for text-to-cinematic-video pipelines and with Zen-Foley for complete
audio-visual content creation.

Future work will focus on inference acceleration via consistency distillation, identity-conditioned
generation for consistent character synthesis, and extending the maximum generation length
to feature-film scale ($> 90$ minutes).

% =============================================================================
\section*{Acknowledgments}
% =============================================================================

We thank the data annotation teams at Hanzo AI, the infrastructure team for H100 cluster
management, and the Zoo Labs research community for ongoing technical discussions.
Special thanks to the WebVid and Kinetics dataset maintainers.

% =============================================================================
% REFERENCES
% =============================================================================

\begin{thebibliography}{99}

\bibitem[Bain et~al.(2021)]{bain2021frozen}
Bain, M., Nagrani, A., Zisserman, A., \& Vedaldi, A.
\newblock Frozen in time: A joint video and image encoder for end-to-end retrieval.
\newblock \emph{Proceedings of ICCV}, 2021.

\bibitem[Blattmann et~al.(2023a)]{blattmann2023align}
Blattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim, S.W., Fidler, S., \& Kreis, K.
\newblock Align your latents: High-resolution video synthesis with latent diffusion models.
\newblock \emph{Proceedings of CVPR}, 2023.

\bibitem[Blattmann et~al.(2023b)]{blattmann2023stable}
Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., \dots{} Rombach, R.
\newblock Stable video diffusion: Scaling latent video diffusion models to large datasets.
\newblock \emph{arXiv preprint arXiv:2311.15127}, 2023.

\bibitem[Chan et~al.(2022)]{chan2022basicvsr++}
Chan, K.C.K., Zhou, S., Xu, X., \& Loy, C.C.
\newblock BasicVSR++: Improving video super-resolution with enhanced propagation and alignment.
\newblock \emph{Proceedings of CVPR}, 2022.

\bibitem[Chen et~al.(2023)]{chen2023videocrafter1}
Chen, H., Xia, M., He, Y., Zhang, Y., Cun, X., Yang, S., \dots{} Wang, J.
\newblock VideoCrafter1: Open diffusion models for high-quality video generation.
\newblock \emph{arXiv preprint arXiv:2310.19512}, 2023.

\bibitem[Chen et~al.(2024)]{chen2024videocrafter2}
Chen, H., Zhang, Y., Cun, X., Xia, M., Wang, X., Weng, C., \& Wang, J.
\newblock VideoCrafter2: Overcoming data limitations for high-quality video diffusion models.
\newblock \emph{Proceedings of CVPR}, 2024.

\bibitem[David et~al.(2023)]{david2023show1}
David, J., Yi, R., Yang, H., Ni, B., Liu, Z., \& Shou, M.Z.
\newblock Show-1: Marrying pixel and latent diffusion models for text-to-video generation.
\newblock \emph{arXiv preprint arXiv:2309.03905}, 2023.

\bibitem[Ge et~al.(2022)]{ge2022long}
Ge, S., Hayes, T., Yang, H., Yin, X., Pang, G., Jacobs, D., \dots{} Parikh, D.
\newblock Long video generation with time-agnostic VQGAN and time-sensitive transformer.
\newblock \emph{Proceedings of ECCV}, 2022.

\bibitem[Harvey et~al.(2022)]{harvey2022flexible}
Harvey, W., Naderiparizi, S., Masrani, V., Weilbach, C., \& Wood, F.
\newblock Flexible diffusion modeling of long videos.
\newblock \emph{Advances in Neural Information Processing Systems}, 35, 2022.

\bibitem[He et~al.(2022)]{he2022latent}
He, Y., Yang, T., Zhang, Y., Shan, Y., \& Chen, Q.
\newblock Latent video diffusion models for high-fidelity long video generation.
\newblock \emph{arXiv preprint arXiv:2211.13221}, 2022.

\bibitem[Ho et~al.(2020)]{ho2020denoising}
Ho, J., Jain, A., \& Abbeel, P.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Ho et~al.(2022)]{ho2022video}
Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., \& Fleet, D.J.
\newblock Video diffusion models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35, 2022.

\bibitem[Huang et~al.(2022)]{huang2022real}
Huang, Z., Zhang, T., Heng, W., Shi, B., \& Zhou, S.
\newblock Real-time intermediate flow estimation for video frame interpolation.
\newblock \emph{Proceedings of ECCV}, 2022.

\bibitem[Huang et~al.(2023)]{huang2023vbench}
Huang, Z., He, Y., Yu, J., Zhang, F., Si, C., Jiang, Y., \dots{} Liu, Z.
\newblock VBench: Comprehensive benchmark suite for video generative models.
\newblock \emph{Proceedings of CVPR}, 2024.

\bibitem[Kondratyuk et~al.(2023)]{kondratyuk2023videopoet}
Kondratyuk, D., Yu, L., Gu, X., Lezama, J., Huang, J., Schindler, G., \dots{} Irie, G.
\newblock VideoPoet: A large language model for zero-shot video generation.
\newblock \emph{arXiv preprint arXiv:2312.14125}, 2023.

\bibitem[LeCun et~al.(2006)]{lecun2006tutorial}
LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., \& Huang, F.
\newblock A tutorial on energy-based learning.
\newblock \emph{Predicting Structured Data}, 2006.

\bibitem[Liu et~al.(2022)]{liu2022music2video}
Liu, S., Chen, Y., Liu, Y., Hou, X., Yuan, M., \& Fan, J.
\newblock Measuring and improving semantic diversity of dialogue generation.
\newblock \emph{Proceedings of EMNLP}, 2022.

\bibitem[Luo et~al.(2023)]{luo2023videofusion}
Luo, Z., Chen, D., Zhang, Y., Huang, Y., Wang, L., Shen, Y., \dots{} Jiang, Y.
\newblock VideoFusion: Decomposed diffusion models for high-quality video generation.
\newblock \emph{Proceedings of CVPR}, 2023.

\bibitem[OpenAI(2024)]{openai2024sora}
OpenAI.
\newblock Sora: Creating video from text.
\newblock Technical Report, 2024.

\bibitem[Raffel et~al.(2020)]{raffel2020exploring}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., \dots{} Liu, P.J.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{Journal of Machine Learning Research}, 21:1--67, 2020.

\bibitem[Reda et~al.(2022)]{reda2022film}
Reda, F., Kontkanen, J., Tabellion, E., Sun, D., Pantofaru, C., \& Curless, B.
\newblock FILM: Frame interpolation for large motion.
\newblock \emph{Proceedings of ECCV}, 2022.

\bibitem[Rombach et~al.(2022)]{rombach2022highresolution}
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., \& Ommer, B.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock \emph{Proceedings of CVPR}, 2022.

\bibitem[Salimans \& Ho(2022)]{salimans2022progressive}
Salimans, T. \& Ho, J.
\newblock Progressive distillation for fast sampling of diffusion models.
\newblock \emph{Proceedings of ICLR}, 2022.

\bibitem[Schuhmann et~al.(2022)]{schuhmann2022laion}
Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., \dots{} Jitsev, J.
\newblock LAION-5B: An open large-scale dataset for training next generation image-text models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35, 2022.

\bibitem[Smaira et~al.(2020)]{smaira2020short}
Smaira, L., Carreira, J., Noland, E., Clancy, E., Wu, A., \& Zisserman, A.
\newblock A short note on the Kinetics-700-2020 human action dataset.
\newblock \emph{arXiv preprint arXiv:2010.10864}, 2020.

\bibitem[Song et~al.(2021)]{song2021scorebased}
Song, Y., Sohl-Dickstein, J., Kingma, D.P., Kumar, A., Ermon, S., \& Poole, B.
\newblock Score-based generative modeling through stochastic differential equations.
\newblock \emph{Proceedings of ICLR}, 2021.

\bibitem[Su et~al.(2022)]{su2022roformer}
Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., \& Liu, Y.
\newblock RoFormer: Enhanced transformer with rotary position embedding.
\newblock \emph{Neurocomputing}, 568:127063, 2022.

\bibitem[Teed \& Deng(2020)]{teed2020raft}
Teed, Z. \& Deng, J.
\newblock RAFT: Recurrent all-pairs field transforms for optical flow.
\newblock \emph{Proceedings of ECCV}, 2020.

\bibitem[Villegas et~al.(2023)]{villegas2023phenaki}
Villegas, R., Babaeizadeh, M., Kindermans, P.J., Moraldo, H., Zhang, H., Saffar, M.T.,
\dots{} Erhan, D.
\newblock Phenaki: Variable length video generation from open domain textual descriptions.
\newblock \emph{Proceedings of ICLR}, 2023.

\bibitem[Voleti et~al.(2022)]{voleti2022mcvd}
Voleti, V., Jolicoeur-Martineau, A., \& Pal, C.
\newblock MCVD -- Masked conditional video diffusion for prediction, generation, and interpolation.
\newblock \emph{Advances in Neural Information Processing Systems}, 35, 2022.

\bibitem[Wang et~al.(2018)]{wang2018esrgan}
Wang, X., Yu, K., Wu, S., Gu, J., Liu, Y., Dong, C., \dots{} Change Loy, C.
\newblock ESRGAN: Enhanced super-resolution generative adversarial networks.
\newblock \emph{Proceedings of ECCV Workshops}, 2018.

\bibitem[Wang et~al.(2023)]{wang2023gen}
Wang, W., Chen, J., Chen, X., Xie, Z., Lu, L., \& Bai, X.
\newblock Gen-L-video: Multi-text to long video generation via temporal co-denoising.
\newblock \emph{arXiv preprint arXiv:2305.18264}, 2023.

\bibitem[Wang et~al.(2023)]{wang2023videofactory}
Wang, W., Bao, J., Zhou, W., Hu, H., \& Li, H.
\newblock VideoFactory: Swap attention in spatiotemporal diffusions for text-to-video generation.
\newblock \emph{arXiv preprint arXiv:2305.10874}, 2023.

\bibitem[Wu et~al.(2022)]{wu2022wav2clip}
Wu, H., Seetharaman, P., Kumar, K., \& Bello, J.P.
\newblock Wav2CLIP: Learning robust audio representations from clip.
\newblock \emph{Proceedings of ICASSP}, 2022.

\bibitem[Yang et~al.(2024)]{yang2024cogvideox}
Yang, Z., Teng, J., Zheng, W., Ding, M., Huang, S., Xu, J., \dots{} Tang, J.
\newblock CogVideoX: Text-to-video diffusion models with an expert transformer.
\newblock \emph{arXiv preprint arXiv:2408.06072}, 2024.

\bibitem[Zhang et~al.(2018)]{zhang2018unreasonable}
Zhang, R., Isola, P., Efros, A.A., Shechtman, E., \& Wang, O.
\newblock The unreasonable effectiveness of deep features as a perceptual metric.
\newblock \emph{Proceedings of CVPR}, 2018.

\end{thebibliography}

\end{document}
