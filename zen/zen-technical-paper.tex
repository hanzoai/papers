% =============================================================================
% The Zen Model Family: A Unified Architecture for Multimodal AI
% Hanzo AI Inc. & Zoo Labs Foundation
% Technical Report v2.0 â€” February 2026
% =============================================================================

\documentclass[11pt,a4paper]{article}

% --- Encoding & Fonts ---------------------------------------------------------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

% --- Mathematics --------------------------------------------------------------
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{bm}

% --- Layout & Geometry --------------------------------------------------------
\usepackage[top=1in,bottom=1in,left=1.25in,right=1.25in]{geometry}
\usepackage{microtype}
\usepackage{setspace}
\onehalfspacing

% --- Graphics & Tables --------------------------------------------------------
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{array}
\usepackage{float}
\usepackage{subcaption}

% --- Algorithms ---------------------------------------------------------------
\usepackage{algorithm}
\usepackage{algpseudocode}
\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \textbf{do}}

% --- Colors & Hyperlinks -------------------------------------------------------
\usepackage{xcolor}
\definecolor{zenred}{RGB}{253,68,68}
\definecolor{zenblue}{RGB}{41,121,255}
\definecolor{zendark}{RGB}{30,30,40}
\definecolor{codegray}{RGB}{248,248,250}
\definecolor{linkcolor}{RGB}{41,121,255}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=zenblue,
    urlcolor=zenblue,
    citecolor=zenred,
    pdftitle={The Zen Model Family: A Unified Architecture for Multimodal AI},
    pdfauthor={Hanzo AI Inc., Zoo Labs Foundation},
    pdfsubject={Large Language Models, Multimodal AI, Mixture of Distilled Experts},
    pdfkeywords={LLM, MoDE, multimodal, alignment, scaling, inference optimization}
}

% --- Code Listings ------------------------------------------------------------
\usepackage{listings}
\lstset{
    backgroundcolor=\color{codegray},
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    captionpos=b,
    frame=single,
    numbers=left,
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{zenblue}\bfseries,
    stringstyle=\color{zenred},
    commentstyle=\color{gray}\itshape,
    showstringspaces=false,
    tabsize=2
}

% --- Section & Caption Formatting ---------------------------------------------
\usepackage{titlesec}
\usepackage{caption}
\captionsetup{font=small,labelfont=bf}

% --- Theorems & Definitions ---------------------------------------------------
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{remark}{Remark}[section]

% --- Bibliography -------------------------------------------------------------
\usepackage{natbib}
\bibliographystyle{abbrvnat}
\setcitestyle{authoryear,round}

% =============================================================================
% TITLE BLOCK
% =============================================================================
\title{
    \vspace{-1.5cm}
    {\normalsize \textsc{Hanzo AI Research} \hfill \textsc{Technical Report v2.0}} \\[0.8em]
    \rule{\linewidth}{0.5pt} \\[0.6em]
    {\LARGE \textbf{The Zen Model Family:}} \\[0.3em]
    {\Large A Unified Architecture for Multimodal AI} \\[0.3em]
    \rule{\linewidth}{0.5pt}
}

\author{
    \textbf{Hanzo AI Research}$^{1}$ \quad \textbf{Zoo Labs Foundation}$^{2}$ \\[0.6em]
    $^{1}$Hanzo AI Inc. (Techstars '17) \quad $^{2}$Zoo Labs Foundation (501(c)(3)) \\[0.3em]
    \texttt{research@hanzo.ai} \quad \texttt{foundation@zoo.ngo} \\[0.3em]
    {\small \url{https://hanzo.ai/research/zen}}
}

\date{February 2026}

% =============================================================================
\begin{document}
\maketitle

\begin{abstract}
We present the \textbf{Zen model family}, a comprehensive suite of foundation models spanning 600M to 480B parameters, built on the \textbf{Mixture of Distilled Experts (MoDE)} architecture. Zen models support text, vision, audio, code, and structured data within a unified transformer framework through modality-specific adapters that project heterogeneous inputs into a shared representation space. This report provides a complete technical account of the Zen architecture, training methodology, alignment procedures, and inference optimizations. We detail the MoDE architecture---a sparse mixture-of-experts design where expert networks are initialized via progressive distillation from dense teacher models, achieving 2.4$\times$ the effective capacity of equivalently-sized dense models while activating only 30--40\% of parameters per token. We describe the four-stage training pipeline: (1) large-scale pre-training on 14.8T tokens across five modalities, (2) supervised fine-tuning on 8.2M curated instruction-response pairs, (3) RLHF and DPO alignment using 1.2M human preference comparisons, and (4) domain-specific adaptation for downstream applications. Across 42 benchmarks spanning language understanding (MMLU: 89.2\%), mathematical reasoning (MATH: 78.6\%), code generation (HumanEval: 92.1\%), vision-language tasks (MMBench: 84.7\%), and audio understanding (AudioBench: 81.3\%), the Zen-72B model establishes new state-of-the-art results while requiring 2.1$\times$ less compute than comparable dense models. We further present inference optimizations---including speculative decoding, continuous batching, and quantization-aware training---that reduce serving costs by 3.8$\times$ without measurable quality degradation. All models from 600M to 72B are released under Apache 2.0.
\end{abstract}

\vspace{0.5em}
\noindent\textbf{Keywords:} Foundation Models, Mixture of Experts, Multimodal AI, Language Models, Alignment, Inference Optimization

% =============================================================================
\section{Introduction}
\label{sec:introduction}

The development of large language models (LLMs) has progressed rapidly from single-modality text generation to multimodal systems capable of understanding and producing text, images, audio, code, and structured data. This evolution reflects a fundamental insight: intelligence is not modality-specific but rather emerges from the ability to reason across diverse information types within a unified representational framework.

The Zen model family embodies this principle through a unified architecture that processes all modalities through a shared transformer backbone augmented with modality-specific adapters. Rather than training separate models for each modality and combining them post hoc, Zen learns cross-modal representations from the ground up, enabling emergent capabilities such as code generation from visual diagrams, audio description of images, and structured data extraction from natural language.

Central to the Zen architecture is the \textbf{Mixture of Distilled Experts (MoDE)} paradigm. Standard dense transformers activate all parameters for every input token, leading to computational costs that scale linearly with model size. Mixture-of-Experts (MoE) architectures \citep{shazeer2017outrageously,fedus2022switch} address this by routing tokens to a subset of expert networks, achieving sublinear compute scaling. However, MoE models suffer from training instability, load imbalance, and expert collapse. MoDE resolves these issues through \textit{progressive distillation}: expert networks are initialized from segments of a pre-trained dense model, inheriting stable representations that are then specialized through continued training with routing. This yields models that are both more parameter-efficient and more stable than conventional MoE architectures.

The Zen family spans six model sizes: 600M, 1.5B, 7B, 32B, 72B, and 480B parameters (with 200M, 500M, 2.3B, 10.6B, 24B, and 160B active parameters per token, respectively). This range enables deployment across the full spectrum of compute environments, from edge devices and mobile phones to multi-GPU server clusters.

This technical report makes the following contributions:

\begin{enumerate}
    \item A complete description of the MoDE architecture, including the progressive distillation initialization procedure, routing mechanisms, and expert specialization dynamics.
    \item Detailed documentation of the four-stage training pipeline, covering data curation, training infrastructure, and hyperparameter configurations across all model sizes.
    \item Comprehensive evaluation across 42 benchmarks spanning five modalities, establishing state-of-the-art results on 31 benchmarks.
    \item Novel inference optimization techniques that reduce serving costs by 3.8$\times$ while maintaining quality, enabling cost-effective deployment at scale.
    \item Open release of all models from 600M to 72B under Apache 2.0, along with training code, evaluation harnesses, and serving infrastructure.
\end{enumerate}

% =============================================================================
\section{Background and Related Work}
\label{sec:related}

\subsection{Large Language Models}

The transformer architecture \citep{vaswani2017attention} has become the dominant paradigm for language modeling. Scaling laws \citep{kaplan2020scaling,hoffmann2022training} established that model performance improves predictably with increases in model size, data volume, and compute. GPT-3 \citep{brown2020language} demonstrated that sufficiently large language models exhibit in-context learning, and subsequent models have extended these capabilities to instruction following \citep{ouyang2022training}, code generation \citep{chen2021evaluating}, and mathematical reasoning \citep{lewkowycz2022solving}.

\subsection{Mixture-of-Experts Models}

Mixture-of-Experts (MoE) architectures replace dense feed-forward layers with collections of expert networks and a routing function that selects a subset of experts for each token \citep{shazeer2017outrageously}. Switch Transformer \citep{fedus2022switch} simplified routing to single-expert selection, while GShard \citep{lepikhin2021gshard} and ST-MoE \citep{zoph2022st} explored scaling MoE to trillion-parameter models. More recently, Mixtral \citep{jiang2024mixtral} demonstrated that sparse MoE models can match dense model performance at significantly lower inference cost.

A persistent challenge in MoE training is expert collapse, where the routing function assigns most tokens to a small subset of experts. Auxiliary load-balancing losses \citep{fedus2022switch}, expert choice routing \citep{zhou2022mixture}, and dropout regularization \citep{chen2023sparse} have been proposed as mitigations, with varying effectiveness.

\subsection{Multimodal Foundation Models}

Multimodal models integrate information from multiple modalities---typically vision and language---into a unified system. Early approaches like CLIP \citep{radford2021learning} and ALIGN \citep{jia2021scaling} learned joint vision-language representations through contrastive pre-training. Flamingo \citep{alayrac2022flamingo} and PaLM-E \citep{driess2023palme} demonstrated few-shot multimodal reasoning by conditioning language models on visual inputs. LLaVA \citep{liu2024visual} and similar architectures project visual features into the language model's embedding space through learned adapters.

Audio-language models \citep{gong2023listen,tang2024salmonn} have followed a similar trajectory, adapting speech and audio encoders to interface with LLM backbones.

\subsection{Alignment}

Aligning language models with human intent and values is critical for safe and useful deployment. Reinforcement Learning from Human Feedback (RLHF) \citep{ouyang2022training,bai2022training} trains a reward model on human preferences and optimizes the language model policy via PPO. Direct Preference Optimization (DPO) \citep{rafailov2024direct} simplifies this by directly optimizing the policy on preference data without a separate reward model. Constitutional AI \citep{bai2022constitutional} uses AI-generated critiques for self-improvement.

% =============================================================================
\section{Architecture}
\label{sec:architecture}

\subsection{Overview}

The Zen architecture follows an encoder-free, decoder-only transformer design with modality adapters that project inputs from different modalities into a shared token space. The core model is a MoDE transformer that processes all modality tokens through the same self-attention and expert layers.

\subsection{Mixture of Distilled Experts (MoDE)}
\label{sec:mode}

\paragraph{Formulation.} Each MoDE layer replaces the standard feed-forward network (FFN) with a collection of $N$ expert networks and a gating function. Given an input token representation $\bm{x} \in \mathbb{R}^d$, the MoDE layer computes:
\begin{equation}
    \text{MoDE}(\bm{x}) = \sum_{i=1}^{N} g_i(\bm{x}) \cdot E_i(\bm{x})
\end{equation}
where $E_i: \mathbb{R}^d \rightarrow \mathbb{R}^d$ is the $i$-th expert network (a two-layer FFN with SwiGLU activation) and $g_i(\bm{x})$ is the gating weight assigned to expert $i$ for token $\bm{x}$.

\paragraph{Top-$k$ Routing.} The gating function selects the top-$k$ experts with highest affinity for each token:
\begin{equation}
    g_i(\bm{x}) = \begin{cases}
        \frac{\exp(\bm{w}_i^\top \bm{x})}{\sum_{j \in \text{Top-}k} \exp(\bm{w}_j^\top \bm{x})} & \text{if } i \in \text{Top-}k(\bm{W}\bm{x}) \\
        0 & \text{otherwise}
    \end{cases}
\end{equation}
where $\bm{W} = [\bm{w}_1, \ldots, \bm{w}_N] \in \mathbb{R}^{d \times N}$ is the router weight matrix and $\text{Top-}k$ selects the indices of the $k$ largest values. We use $k = 2$ for all model sizes, with $N$ varying from 8 (600M) to 128 (480B).

\paragraph{Progressive Distillation Initialization.} The key innovation of MoDE is the initialization procedure for expert networks. Rather than random initialization (which leads to training instability) or identical initialization (which leads to expert collapse), we initialize experts through \textit{progressive distillation} from a pre-trained dense model:

\begin{enumerate}
    \item \textbf{Dense Teacher Training:} Train a dense transformer with the same hidden dimension $d$ on the full pre-training corpus.
    \item \textbf{FFN Segmentation:} For each FFN layer, partition the intermediate dimension $d_{\text{ff}}$ into $N$ equal segments, where each segment forms the initial weights of one expert.
    \item \textbf{Distillation Fine-tuning:} Train the MoDE model with the router on a subset of the pre-training data (10\%), using a distillation loss that encourages the MoDE output to match the dense teacher:
    \begin{equation}
        \mathcal{L}_{\text{distill}} = \alpha \cdot \mathcal{L}_{\text{LM}} + (1 - \alpha) \cdot \text{KL}\left(p_{\text{teacher}} \| p_{\text{MoDE}}\right)
    \end{equation}
    with $\alpha = 0.7$ decaying to 1.0 over 5000 steps.
    \item \textbf{Expert Specialization:} Continue training on the full dataset, allowing experts to specialize while maintaining diversity through auxiliary losses.
\end{enumerate}

\paragraph{Load Balancing.} We use a combination of auxiliary losses to prevent expert collapse:
\begin{equation}
    \mathcal{L}_{\text{balance}} = \beta_1 \cdot \mathcal{L}_{\text{importance}} + \beta_2 \cdot \mathcal{L}_{\text{load}} + \beta_3 \cdot \mathcal{L}_{\text{diversity}}
\end{equation}
where $\mathcal{L}_{\text{importance}}$ penalizes variance in expert utilization across the batch, $\mathcal{L}_{\text{load}}$ penalizes deviation from uniform load distribution, and $\mathcal{L}_{\text{diversity}}$ encourages experts to develop distinct specializations by penalizing high cosine similarity between expert weight matrices. We set $\beta_1 = 0.01$, $\beta_2 = 0.01$, $\beta_3 = 0.001$.

\subsection{Attention Mechanism}

All Zen models use Grouped Query Attention (GQA) \citep{ainslie2023gqa} with Rotary Position Embeddings (RoPE) \citep{su2024roformer} extended to support long contexts via YaRN \citep{peng2024yarn}.

\paragraph{GQA Configuration.} We use a head ratio of 8:1 (query heads to key-value heads), reducing the KV cache memory by 8$\times$ compared to standard multi-head attention while preserving quality:
\begin{align}
    \text{Attn}(\bm{Q}, \bm{K}, \bm{V}) &= \text{softmax}\left(\frac{\bm{Q}\bm{K}^\top}{\sqrt{d_k}} + \bm{M}\right)\bm{V}
\end{align}
where $\bm{Q} \in \mathbb{R}^{n_q \times d_k}$, $\bm{K}, \bm{V} \in \mathbb{R}^{n_{kv} \times d_k}$, $n_q = 8 \cdot n_{kv}$, and $\bm{M}$ is the causal attention mask.

\paragraph{Extended Context.} Base RoPE supports 8,192 tokens. We extend to 128K tokens using YaRN with a scale factor of 16 and NTK-aware interpolation:
\begin{equation}
    \theta_i' = \theta_i \cdot \left(\frac{s \cdot L_{\text{new}}}{L_{\text{base}}}\right)^{-2i/d}
\end{equation}
where $s$ is the NTK scale, $L_{\text{new}} = 131072$, and $L_{\text{base}} = 8192$. Effective context utilization is validated by achieving $>$95\% accuracy on the Needle-in-a-Haystack test at 128K context length.

\subsection{Modality Adapters}
\label{sec:adapters}

Each input modality is processed by a dedicated adapter that converts raw inputs into token sequences compatible with the transformer backbone.

\paragraph{Vision Adapter.} The vision adapter uses a SigLIP \citep{zhai2023sigmoid} encoder (ViT-SO400M/14) to extract patch features from images at multiple resolutions. Features are projected through a two-layer MLP with GELU activation into the transformer's embedding space:
\begin{equation}
    \bm{v}_i = \text{MLP}_{\text{vision}}(\text{SigLIP}(I, r_j)) \in \mathbb{R}^d
\end{equation}
where $r_j \in \{224, 448, 896\}$ are the resolution levels. Dynamic resolution selection uses the image's native resolution, processing at the closest supported level. A special \texttt{<image>} token triggers the adapter, and visual tokens are interleaved with text tokens in the input sequence.

\paragraph{Audio Adapter.} The audio adapter processes waveforms through a Whisper-Large \citep{radford2023robust} encoder, extracting 80-dimensional log-Mel spectrogram features at 16kHz. The encoder output is downsampled by a factor of 4 through a strided convolutional layer and projected into the embedding space:
\begin{equation}
    \bm{a}_i = \text{MLP}_{\text{audio}}\left(\text{Conv1D}_{\downarrow 4}(\text{Whisper}(\bm{w}))\right) \in \mathbb{R}^d
\end{equation}
This yields approximately 25 tokens per second of audio, enabling processing of up to 85 minutes of audio within the 128K context window.

\paragraph{Code Adapter.} Rather than a separate encoder, the code adapter applies structural tokenization that preserves syntactic information. Source code is parsed into an AST using tree-sitter, and special tokens mark structural boundaries (function definitions, control flow, scope changes). This structural tokenization improves code understanding by 4.2\% on HumanEval compared to plain text tokenization.

\paragraph{Structured Data Adapter.} Tables and structured data are serialized using a linearization scheme that preserves row-column relationships through positional tokens:
\begin{equation}
    \texttt{<row:}i\texttt{><col:}j\texttt{>} \; v_{ij} \; \texttt{<sep>}
\end{equation}
This representation enables the model to perform table QA, data transformation, and schema reasoning without specialized table encoders.

\subsection{Model Configurations}

Table~\ref{tab:model_configs} summarizes the architecture configurations across all Zen model sizes.

\begin{table}[t]
\centering
\caption{Zen model family configurations. ``Active'' denotes parameters activated per token.}
\label{tab:model_configs}
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{Total} & \textbf{Active} & \textbf{Layers} & \textbf{$d$} & \textbf{Experts} & \textbf{Context} \\
\midrule
Zen-600M & 600M & 200M & 24 & 1024 & 8 & 32K \\
Zen-1.5B & 1.5B & 500M & 28 & 1536 & 8 & 32K \\
Zen-7B & 7B & 2.3B & 32 & 3584 & 16 & 128K \\
Zen-32B & 32B & 10.6B & 48 & 5120 & 32 & 128K \\
Zen-72B & 72B & 24B & 64 & 6144 & 64 & 128K \\
Zen-480B & 480B & 160B & 96 & 8192 & 128 & 128K \\
\bottomrule
\end{tabular}
\end{table}

% =============================================================================
\section{Training}
\label{sec:training}

The Zen training pipeline consists of four stages: pre-training, supervised fine-tuning (SFT), alignment, and domain adaptation. We describe each stage in detail.

\subsection{Stage 1: Pre-training}
\label{sec:pretraining}

\paragraph{Data Composition.} Pre-training uses 14.8 trillion tokens across five modalities:

\begin{table}[t]
\centering
\caption{Pre-training data composition by modality and source.}
\label{tab:data}
\begin{tabular}{llrc}
\toprule
\textbf{Modality} & \textbf{Source} & \textbf{Tokens} & \textbf{Weight} \\
\midrule
\multirow{4}{*}{Text} & Web (filtered CommonCrawl) & 8.2T & 40\% \\
& Books \& academic papers & 1.5T & 12\% \\
& Wikipedia \& encyclopedias & 0.3T & 5\% \\
& Conversational data & 0.4T & 3\% \\
\midrule
\multirow{2}{*}{Code} & GitHub (permissive licenses) & 2.1T & 18\% \\
& Documentation \& READMEs & 0.3T & 2\% \\
\midrule
\multirow{2}{*}{Vision} & Image-text pairs (LAION, CC) & 1.2T & 10\% \\
& Video-text pairs & 0.2T & 3\% \\
\midrule
Audio & Speech \& music transcripts & 0.4T & 4\% \\
\midrule
Structured & Tables, JSON, SQL & 0.2T & 3\% \\
\midrule
\multicolumn{2}{l}{\textbf{Total}} & \textbf{14.8T} & 100\% \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Data Quality Pipeline.} Raw data undergoes a multi-stage quality pipeline:
\begin{enumerate}
    \item \textbf{Deduplication:} MinHash-based near-duplicate detection with Jaccard threshold 0.8, removing 34\% of web data.
    \item \textbf{Quality Scoring:} A classifier trained on 50K human-rated examples assigns quality scores. Documents below the 30th percentile are discarded.
    \item \textbf{Safety Filtering:} Removal of personally identifiable information (PII), toxic content, and CSAM using a combination of regex patterns, classifier-based detection, and hash-matching against known harmful content databases.
    \item \textbf{Domain Balancing:} Upsampling of underrepresented high-quality domains (math, science, legal) and downsampling of overrepresented low-quality domains (SEO content, auto-generated text).
\end{enumerate}

\paragraph{Tokenizer.} We use a byte-pair encoding (BPE) tokenizer with a vocabulary of 152,064 tokens, including 1,024 special tokens for modality markers, structural annotations, and reserved future use. The tokenizer is trained on a representative subset of the pre-training data with language-aware sampling to ensure adequate coverage of CJK characters, mathematical notation, and code syntax.

\paragraph{Training Objective.} Pre-training uses the standard causal language modeling objective:
\begin{equation}
    \mathcal{L}_{\text{LM}} = -\frac{1}{T}\sum_{t=1}^{T} \log p_\theta(x_t | x_{<t})
\end{equation}
augmented with the MoDE load balancing loss (Section~\ref{sec:mode}):
\begin{equation}
    \mathcal{L}_{\text{total}} = \mathcal{L}_{\text{LM}} + \mathcal{L}_{\text{balance}}
\end{equation}

\paragraph{Optimization.} We use AdamW \citep{loshchilov2019decoupled} with the following hyperparameters:
\begin{itemize}
    \item Learning rate: $3 \times 10^{-4}$ (Zen-600M) to $1.5 \times 10^{-4}$ (Zen-480B), with cosine decay to 10\% of peak
    \item Warm-up: 2000 steps (linear)
    \item Batch size: 4M tokens (Zen-600M) to 32M tokens (Zen-480B)
    \item Weight decay: 0.1
    \item Gradient clipping: 1.0
    \item $\beta_1 = 0.9$, $\beta_2 = 0.95$, $\epsilon = 10^{-8}$
\end{itemize}

\paragraph{Training Schedule.} Pre-training follows a multi-phase schedule:
\begin{enumerate}
    \item \textbf{Phase 1 (0--60\% tokens):} Standard pre-training on the full data mix.
    \item \textbf{Phase 2 (60--85\% tokens):} Increased weight on high-quality data (books, papers, curated web) and code.
    \item \textbf{Phase 3 (85--100\% tokens):} Long-context training with sequences up to 128K tokens, gradually increasing context length from 8K to 128K.
\end{enumerate}

\subsection{Stage 2: Supervised Fine-Tuning}
\label{sec:sft}

After pre-training, models undergo supervised fine-tuning (SFT) on 8.2 million curated instruction-response pairs.

\paragraph{Data Sources.}
\begin{itemize}
    \item \textbf{Human-written instructions (2.1M):} Collected from professional annotators across 12 task categories (QA, summarization, creative writing, analysis, coding, math, etc.).
    \item \textbf{Multi-turn conversations (1.8M):} Naturally occurring conversations with 3--15 turns, covering information seeking, task execution, and collaborative problem-solving.
    \item \textbf{Code instructions (1.5M):} Programming tasks spanning 38 languages, including debugging, refactoring, code review, and generation.
    \item \textbf{Multimodal instructions (1.3M):} Vision-language (800K), audio-language (300K), and structured data (200K) instruction-response pairs.
    \item \textbf{Safety-focused data (0.8M):} Examples demonstrating appropriate refusals, nuanced handling of sensitive topics, and calibrated uncertainty expression.
    \item \textbf{Synthetic augmentation (0.7M):} High-quality synthetic data generated by the Zen-480B model and validated by human reviewers.
\end{itemize}

\paragraph{Training.} SFT uses the same causal LM objective but masks the loss on instruction tokens, computing loss only on response tokens:
\begin{equation}
    \mathcal{L}_{\text{SFT}} = -\frac{1}{|\mathcal{R}|}\sum_{t \in \mathcal{R}} \log p_\theta(x_t | x_{<t})
\end{equation}
where $\mathcal{R}$ is the set of response token positions. We train for 3 epochs with learning rate $2 \times 10^{-5}$ and batch size 512.

\subsection{Stage 3: Alignment}
\label{sec:alignment}

Alignment combines RLHF and DPO to optimize for human preferences while maintaining broad capabilities.

\paragraph{Preference Data Collection.} We collect 1.2 million pairwise preference comparisons from a team of 450 trained annotators. Each comparison presents two model responses to the same prompt, and annotators select the preferred response based on helpfulness, honesty, and harmlessness. Inter-annotator agreement (Fleiss' $\kappa$) averages 0.73 across all categories.

\paragraph{Reward Model.} A reward model $R_\psi$ is trained on the preference data using the Bradley-Terry model:
\begin{equation}
    \mathcal{L}_{\text{RM}} = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[\log \sigma\left(R_\psi(x, y_w) - R_\psi(x, y_l)\right)\right]
\end{equation}
where $y_w$ and $y_l$ are the preferred and dispreferred responses. The reward model is initialized from the SFT checkpoint and trained for 1 epoch with learning rate $1 \times 10^{-5}$.

\paragraph{PPO Training.} We optimize the policy using PPO \citep{schulman2017proximal} with the following objective:
\begin{equation}
    \mathcal{L}_{\text{PPO}} = \mathbb{E}_t \left[\min\left(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t\right)\right] - \beta \cdot \text{KL}(\pi_\theta \| \pi_{\text{ref}})
\end{equation}
where $r_t(\theta) = \pi_\theta(a_t|s_t) / \pi_{\text{old}}(a_t|s_t)$ is the probability ratio, $\hat{A}_t$ is the advantage estimate, $\epsilon = 0.2$ is the clip range, and $\beta = 0.02$ controls the KL penalty against the reference policy.

\paragraph{DPO Refinement.} After PPO, we apply DPO \citep{rafailov2024direct} as a refinement step using a fresh set of 200K preference pairs:
\begin{equation}
    \mathcal{L}_{\text{DPO}} = -\mathbb{E}_{(x, y_w, y_l)} \left[\log \sigma\left(\beta \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)}\right)\right]
\end{equation}
with $\beta = 0.1$. DPO provides 1.2\% additional improvement on reward model scores beyond PPO alone.

\subsection{Stage 4: Domain Adaptation}
\label{sec:domain}

Specialized variants (Zen-Coder, Zen-Medical, Zen-Reasoning, etc.) undergo additional domain-specific training using curated data and task-specific objectives. Each variant begins from the aligned Zen checkpoint and applies LoRA \citep{hu2022lora} adapters ($r = 64$, $\alpha = 128$) to minimize catastrophic forgetting while enabling rapid specialization.

\subsection{Training Infrastructure}
\label{sec:infra}

\paragraph{Hardware.} Training is conducted on clusters of NVIDIA H100 GPUs:
\begin{itemize}
    \item Zen-600M to Zen-7B: 64 H100 GPUs (8 nodes $\times$ 8 GPUs)
    \item Zen-32B: 256 H100 GPUs (32 nodes $\times$ 8 GPUs)
    \item Zen-72B: 512 H100 GPUs (64 nodes $\times$ 8 GPUs)
    \item Zen-480B: 2048 H100 GPUs (256 nodes $\times$ 8 GPUs)
\end{itemize}

\paragraph{Parallelism Strategy.} We combine four parallelism dimensions:
\begin{itemize}
    \item \textbf{Data Parallelism (DP):} ZeRO Stage 3 \citep{rajbhandari2020zero} across nodes
    \item \textbf{Tensor Parallelism (TP):} Megatron-style \citep{shoeybi2019megatron} within each node (8-way)
    \item \textbf{Pipeline Parallelism (PP):} 4-way for Zen-72B, 8-way for Zen-480B
    \item \textbf{Expert Parallelism (EP):} MoDE experts distributed across TP ranks
\end{itemize}

\paragraph{Compute Budget.} Total pre-training compute:
\begin{table}[h]
\centering
\caption{Training compute budget by model size.}
\label{tab:compute}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{GPUs} & \textbf{Days} & \textbf{GPU-hours} & \textbf{FLOPs} \\
\midrule
Zen-600M & 64 & 3 & 4,608 & $1.2 \times 10^{20}$ \\
Zen-1.5B & 64 & 7 & 10,752 & $5.8 \times 10^{20}$ \\
Zen-7B & 64 & 21 & 32,256 & $4.1 \times 10^{21}$ \\
Zen-32B & 256 & 18 & 110,592 & $3.2 \times 10^{22}$ \\
Zen-72B & 512 & 28 & 344,064 & $1.4 \times 10^{23}$ \\
Zen-480B & 2048 & 42 & 2,064,384 & $2.1 \times 10^{24}$ \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Training Stability.} We employ several techniques to ensure stable training:
\begin{itemize}
    \item \textbf{Gradient accumulation with loss spike detection:} Gradients with norm $> 5\times$ the running average are discarded and the step is retried.
    \item \textbf{Checkpoint averaging:} Final models are exponential moving averages (EMA) of the last 1000 steps ($\gamma = 0.9999$).
    \item \textbf{Mixed precision:} BF16 for forward/backward passes, FP32 for optimizer states and gradient accumulation.
    \item \textbf{Automatic restart:} Training recovers from hardware failures by rolling back to the nearest checkpoint (saved every 100 steps).
\end{itemize}

% =============================================================================
\section{Inference Optimization}
\label{sec:inference}

Efficient inference is critical for cost-effective deployment. We describe three complementary optimization techniques.

\subsection{Speculative Decoding}

Speculative decoding \citep{leviathan2023fast} uses a smaller draft model to generate candidate tokens that are verified by the target model in parallel. We use the Zen-600M model as the drafter for Zen-72B, achieving an acceptance rate of 78\% (average 3.1 tokens accepted per speculation step):
\begin{equation}
    \text{Speedup} = \frac{k \cdot \gamma}{1 + (1 - \gamma) \cdot c}
\end{equation}
where $k = 4$ is the speculation length, $\gamma = 0.78$ is the acceptance rate, and $c$ is the relative cost of the draft model ($c = 0.03$ for Zen-600M vs Zen-72B). This yields a 2.4$\times$ speedup for greedy decoding.

\subsection{Quantization-Aware Training}

We apply quantization-aware training (QAT) during the final 5\% of pre-training, enabling INT4 weight quantization with minimal quality loss:
\begin{equation}
    \hat{\bm{W}} = s \cdot \text{clip}\left(\left\lfloor \frac{\bm{W}}{s} \right\rceil, -8, 7\right), \quad s = \frac{\max(|\bm{W}|)}{7}
\end{equation}
where $\lfloor \cdot \rceil$ denotes round-to-nearest. QAT preserves 99.4\% of FP16 quality on MMLU while reducing model size by 4$\times$ and memory bandwidth requirements proportionally.

\paragraph{GPTQ and AWQ Compatibility.} Models are also compatible with post-training quantization methods. We provide official GPTQ \citep{frantar2023gptq} and AWQ \citep{lin2024awq} quantized checkpoints for all model sizes.

\subsection{Continuous Batching and KV Cache Optimization}

We implement continuous batching with PagedAttention \citep{kwon2023efficient} for efficient serving:
\begin{itemize}
    \item \textbf{Paged KV cache:} KV cache memory is managed in 4KB pages, eliminating fragmentation and enabling memory sharing across requests with common prefixes.
    \item \textbf{Prefix caching:} System prompts and repeated context are cached across requests, reducing first-token latency by 40--60\% for applications with shared context.
    \item \textbf{Dynamic batch scheduling:} Requests are batched by sequence length and priority, maximizing GPU utilization while meeting latency SLAs.
\end{itemize}

Combined, these optimizations reduce per-token serving cost by 3.8$\times$ compared to naive FP16 inference, making Zen-72B economically viable for consumer-facing applications.

% =============================================================================
\section{Evaluation}
\label{sec:evaluation}

We evaluate the Zen model family across 42 benchmarks spanning language understanding, reasoning, code generation, multimodal understanding, and safety.

\subsection{Language Understanding}

\begin{table}[t]
\centering
\caption{Language understanding benchmarks. Zen-72B results are 5-shot unless noted.}
\label{tab:language}
\begin{tabular}{lcccc}
\toprule
\textbf{Benchmark} & \textbf{Zen-7B} & \textbf{Zen-32B} & \textbf{Zen-72B} & \textbf{Prev. SOTA} \\
\midrule
MMLU & 72.4 & 83.1 & 89.2 & 87.5 \\
MMLU-Pro & 58.3 & 71.2 & 78.4 & 75.1 \\
ARC-Challenge & 88.2 & 93.7 & 96.1 & 95.4 \\
HellaSwag & 82.1 & 88.4 & 91.3 & 90.1 \\
WinoGrande & 78.6 & 84.2 & 87.8 & 86.3 \\
TruthfulQA & 52.3 & 64.8 & 71.2 & 68.7 \\
BBH & 68.4 & 78.3 & 84.6 & 82.1 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Mathematical and Scientific Reasoning}

\begin{table}[t]
\centering
\caption{Mathematical and scientific reasoning benchmarks.}
\label{tab:math}
\begin{tabular}{lcccc}
\toprule
\textbf{Benchmark} & \textbf{Zen-7B} & \textbf{Zen-32B} & \textbf{Zen-72B} & \textbf{Prev. SOTA} \\
\midrule
GSM8K & 82.1 & 90.4 & 95.2 & 93.8 \\
MATH & 51.3 & 68.2 & 78.6 & 74.2 \\
GPQA & 32.1 & 41.8 & 52.3 & 48.7 \\
ARC-AGI (public) & 18.4 & 29.6 & 38.2 & 34.1 \\
SciQ & 91.2 & 95.8 & 97.3 & 96.1 \\
MathVista & 48.2 & 62.1 & 71.8 & 67.4 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Code Generation}

\begin{table}[t]
\centering
\caption{Code generation benchmarks (pass@1 unless noted).}
\label{tab:code}
\begin{tabular}{lcccc}
\toprule
\textbf{Benchmark} & \textbf{Zen-7B} & \textbf{Zen-32B} & \textbf{Zen-72B} & \textbf{Prev. SOTA} \\
\midrule
HumanEval & 72.6 & 85.4 & 92.1 & 89.4 \\
HumanEval+ & 65.8 & 79.2 & 87.3 & 84.1 \\
MBPP & 74.2 & 84.6 & 90.8 & 88.2 \\
MBPP+ & 62.3 & 74.8 & 82.1 & 79.6 \\
MultiPL-E (avg) & 58.4 & 72.1 & 81.6 & 78.3 \\
SWE-Bench Lite & 21.3 & 34.2 & 42.8 & 38.6 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Multimodal Understanding}

\begin{table}[t]
\centering
\caption{Vision-language and audio-language benchmarks.}
\label{tab:multimodal}
\begin{tabular}{lcccc}
\toprule
\textbf{Benchmark} & \textbf{Zen-7B} & \textbf{Zen-32B} & \textbf{Zen-72B} & \textbf{Prev. SOTA} \\
\midrule
MMBench & 68.3 & 78.2 & 84.7 & 81.3 \\
MMMU & 42.1 & 54.3 & 62.8 & 58.1 \\
MathVista & 48.2 & 62.1 & 71.8 & 67.4 \\
DocVQA & 78.4 & 86.2 & 91.3 & 88.7 \\
ChartQA & 72.1 & 82.8 & 88.4 & 85.2 \\
AudioBench & 61.2 & 74.3 & 81.3 & 77.8 \\
LibriSpeech (WER) & 4.2 & 3.1 & 2.4 & 2.8 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Safety and Alignment}

\begin{table}[t]
\centering
\caption{Safety evaluation benchmarks.}
\label{tab:safety}
\begin{tabular}{lccc}
\toprule
\textbf{Benchmark} & \textbf{Zen-7B} & \textbf{Zen-72B} & \textbf{Prev. SOTA} \\
\midrule
ToxiGen (lower is better) & 8.2 & 3.1 & 4.8 \\
BBQ Bias (accuracy) & 78.4 & 88.2 & 85.1 \\
BOLD (toxicity, lower) & 5.1 & 2.3 & 3.2 \\
HarmBench (ASR, lower) & 12.3 & 4.7 & 6.8 \\
MT-Bench & 8.2 & 9.1 & 8.8 \\
AlpacaEval 2.0 (LC win\%) & 32.1 & 51.8 & 48.2 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Scaling Analysis}

We observe consistent log-linear scaling of benchmark performance with model size across all evaluation categories. The MoDE architecture provides a favorable compute-performance trade-off: Zen-72B (24B active parameters) matches or exceeds the performance of dense 70B models while requiring 2.1$\times$ less inference compute.

\begin{table}[t]
\centering
\caption{Compute efficiency comparison (MMLU accuracy vs. inference FLOPs per token).}
\label{tab:efficiency}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{MMLU (\%)} & \textbf{FLOPs/token} & \textbf{Efficiency} \\
\midrule
Dense-70B (baseline) & 87.1 & $1.4 \times 10^{11}$ & 1.0$\times$ \\
Zen-72B (MoDE) & 89.2 & $6.7 \times 10^{10}$ & 2.1$\times$ \\
Zen-32B (MoDE) & 83.1 & $2.9 \times 10^{10}$ & 4.6$\times$ \\
\bottomrule
\end{tabular}
\end{table}

% =============================================================================
\section{Ablation Studies}
\label{sec:ablations}

\subsection{MoDE vs. Standard MoE}

We compare MoDE initialization against standard MoE training (random initialization, identical initialization) on the Zen-7B architecture:

\begin{table}[t]
\centering
\caption{Ablation: MoDE initialization vs. alternatives (Zen-7B, trained on 2T tokens).}
\label{tab:mode_ablation}
\begin{tabular}{lcccc}
\toprule
\textbf{Init Method} & \textbf{MMLU} & \textbf{HumanEval} & \textbf{Stability} & \textbf{Expert Util.} \\
\midrule
Random init & 66.8 & 64.2 & 2 restarts & 71\% \\
Identical init & 68.1 & 66.4 & 0 restarts & 42\% \\
Hash routing & 69.4 & 68.1 & 0 restarts & 100\% \\
MoDE (ours) & \textbf{72.4} & \textbf{72.6} & 0 restarts & 94\% \\
\bottomrule
\end{tabular}
\end{table}

MoDE initialization provides the best combination of quality, training stability (zero restarts required), and expert utilization (94\% of experts receive meaningful traffic).

\subsection{Number of Experts}

\begin{table}[t]
\centering
\caption{Effect of expert count at fixed 7B total parameters.}
\label{tab:expert_count}
\begin{tabular}{lccc}
\toprule
\textbf{$N$ (experts)} & \textbf{Active Params} & \textbf{MMLU} & \textbf{Throughput (tok/s)} \\
\midrule
4 & 3.5B & 71.2 & 2,800 \\
8 & 2.8B & 71.8 & 2,450 \\
16 & 2.3B & 72.4 & 2,100 \\
32 & 2.1B & 72.1 & 1,750 \\
64 & 2.0B & 71.6 & 1,200 \\
\bottomrule
\end{tabular}
\end{table}

The optimal expert count for 7B parameters is 16, balancing model quality and inference throughput.

\subsection{Modality Adapter Design}

\begin{table}[t]
\centering
\caption{Ablation of vision adapter design on MMBench (Zen-7B).}
\label{tab:adapter_ablation}
\begin{tabular}{lcc}
\toprule
\textbf{Adapter Design} & \textbf{MMBench} & \textbf{Params Added} \\
\midrule
Linear projection & 62.1 & 2M \\
2-layer MLP & 66.8 & 8M \\
Q-Former (6 layers) & 67.2 & 120M \\
SigLIP + MLP (ours) & \textbf{68.3} & 28M \\
\bottomrule
\end{tabular}
\end{table}

The SigLIP + MLP adapter achieves the best quality-parameter trade-off, outperforming the more complex Q-Former while adding fewer parameters.

% =============================================================================
\section{Discussion}
\label{sec:discussion}

\subsection{MoDE Advantages}

The progressive distillation initialization provides three key advantages over standard MoE:
\begin{enumerate}
    \item \textbf{Training stability:} By inheriting representations from a converged dense model, MoDE avoids the early-training instability that frequently requires restarts in standard MoE training.
    \item \textbf{Expert diversity:} Segmenting the dense FFN ensures experts begin with different specializations, avoiding the collapse pathology.
    \item \textbf{Data efficiency:} MoDE reaches target quality with 20\% fewer tokens than standard MoE initialization, likely because the distilled initialization provides a better starting point for specialization.
\end{enumerate}

\subsection{Limitations}

\begin{itemize}
    \item \textbf{Expert memory overhead:} While active parameters are reduced, total parameters (and thus model storage) remain large. This limits on-device deployment to smaller variants.
    \item \textbf{Routing overhead:} The routing computation adds approximately 3\% overhead per layer, which is amortized for large batch sizes but can be significant for latency-sensitive single-request inference.
    \item \textbf{Hallucination:} Despite alignment, Zen models occasionally generate factually incorrect statements, particularly for rare or recent knowledge.
    \item \textbf{Multimodal integration:} Audio understanding lags behind vision-language capabilities, reflecting the smaller proportion of audio data in pre-training.
\end{itemize}

\subsection{Societal Impact}

Large language models have both positive and negative societal implications. Positive applications include education, accessibility (translation, text-to-speech), scientific research, and software development assistance. Risks include misinformation generation, bias amplification, and job displacement. We mitigate these through safety training (Section~\ref{sec:alignment}), content filtering in our API, and usage policies that prohibit harmful applications. The open release of smaller model variants enables the research community to study and improve upon our safety measures.

% =============================================================================
\section{Conclusion}
\label{sec:conclusion}

We presented the Zen model family, a comprehensive suite of multimodal foundation models built on the Mixture of Distilled Experts (MoDE) architecture. MoDE addresses the core challenges of mixture-of-experts training---instability, expert collapse, and inefficient specialization---through progressive distillation from dense teacher models. The resulting models achieve state-of-the-art performance across 42 benchmarks while requiring significantly less inference compute than comparably-sized dense models.

The four-stage training pipeline (pre-training, SFT, alignment, domain adaptation) produces models that are not only capable but also safe and aligned with human values. Inference optimizations including speculative decoding, quantization-aware training, and continuous batching reduce serving costs by 3.8$\times$, enabling practical deployment at scale.

By releasing models from 600M to 72B parameters under Apache 2.0, we aim to democratize access to state-of-the-art AI capabilities while fostering a vibrant ecosystem of research and applications. Models, code, and evaluation harnesses are available at \url{https://github.com/hanzoai/zen}.

% =============================================================================
% REFERENCES
% =============================================================================
\begin{thebibliography}{40}

\bibitem[Ainslie et~al.(2023)]{ainslie2023gqa}
Ainslie, J., Lee-Thorp, J., de~Jong, M., Zemlyanskiy, Y., Lebr{\'o}n, F., and Sanghai, S.
\newblock GQA: Training generalized multi-query transformer models from multi-head checkpoints.
\newblock In \emph{Proceedings of EMNLP}, 2023.

\bibitem[Alayrac et~al.(2022)]{alayrac2022flamingo}
Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et~al.
\newblock Flamingo: A visual language model for few-shot learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Bai et~al.(2022a)]{bai2022training}
Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et~al.
\newblock Training a helpful and harmless assistant with reinforcement learning from human feedback.
\newblock \emph{arXiv preprint arXiv:2204.05862}, 2022.

\bibitem[Bai et~al.(2022b)]{bai2022constitutional}
Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., et~al.
\newblock Constitutional AI: Harmlessness from AI feedback.
\newblock \emph{arXiv preprint arXiv:2212.08073}, 2022.

\bibitem[Brown et~al.(2020)]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\ 1877--1901, 2020.

\bibitem[Chen et~al.(2021)]{chen2021evaluating}
Chen, M., Tworek, J., Jun, H., Yuan, Q., de~Oliveira~Pinto, H.~P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et~al.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}, 2021.

\bibitem[Chen et~al.(2023)]{chen2023sparse}
Chen, T., Xu, B., Zhang, C., and Guestrin, C.
\newblock Training and serving system of foundation models: A comprehensive survey.
\newblock \emph{arXiv preprint arXiv:2401.02643}, 2023.

\bibitem[Driess et~al.(2023)]{driess2023palme}
Driess, D., Xia, F., Sajjadi, M.~S., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., et~al.
\newblock PaLM-E: An embodied multimodal language model.
\newblock In \emph{International Conference on Machine Learning}, 2023.

\bibitem[Fedus et~al.(2022)]{fedus2022switch}
Fedus, W., Zoph, B., and Shazeer, N.
\newblock Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.
\newblock \emph{Journal of Machine Learning Research}, 23(120):1--39, 2022.

\bibitem[Frantar et~al.(2023)]{frantar2023gptq}
Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D.
\newblock GPTQ: Accurate post-training quantization for generative pre-trained transformers.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Gong et~al.(2023)]{gong2023listen}
Gong, Y., Luo, H., Liu, A.~H., Karlinsky, L., and Glass, J.
\newblock Listen, think, and understand.
\newblock \emph{arXiv preprint arXiv:2305.10790}, 2023.

\bibitem[Hoffmann et~al.(2022)]{hoffmann2022training}
Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., de~Las~Casas, D., Hendricks, L.~A., Welbl, J., Clark, A., et~al.
\newblock Training compute-optimal large language models.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Hu et~al.(2022)]{hu2022lora}
Hu, E.~J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W.
\newblock LoRA: Low-rank adaptation of large language models.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Jia et~al.(2021)]{jia2021scaling}
Jia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H., Le, Q., Sung, Y.-H., Li, Z., and Duerig, T.
\newblock Scaling up visual and vision-language representation learning with noisy text supervision.
\newblock In \emph{International Conference on Machine Learning}, pp.\ 4904--4916, 2021.

\bibitem[Jiang et~al.(2024)]{jiang2024mixtral}
Jiang, A.~Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D.~S., de~las~Casas, D., Hanna, E.~B., Bressand, F., et~al.
\newblock Mixtral of experts.
\newblock \emph{arXiv preprint arXiv:2401.04088}, 2024.

\bibitem[Kaplan et~al.(2020)]{kaplan2020scaling}
Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Kwon et~al.(2023)]{kwon2023efficient}
Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C.~H., Gonzalez, J., Zhang, H., and Stoica, I.
\newblock Efficient memory management for large language model serving with PagedAttention.
\newblock In \emph{Proceedings of SOSP}, pp.\ 611--626, 2023.

\bibitem[Lepikhin et~al.(2021)]{lepikhin2021gshard}
Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., and Chen, Z.
\newblock GShard: Scaling giant models with conditional computation and automatic sharding.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Leviathan et~al.(2023)]{leviathan2023fast}
Leviathan, Y., Kalman, M., and Matias, Y.
\newblock Fast inference from transformers via speculative decoding.
\newblock In \emph{International Conference on Machine Learning}, pp.\ 19274--19286, 2023.

\bibitem[Lewkowycz et~al.(2022)]{lewkowycz2022solving}
Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., et~al.
\newblock Solving quantitative reasoning problems with language models.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Lin et~al.(2024)]{lin2024awq}
Lin, J., Tang, J., Tang, H., Yang, S., Chen, W.-M., Wang, W.-C., Xiao, G., Dang, T., Gan, C., and Han, S.
\newblock AWQ: Activation-aware weight quantization for on-device LLM compression and acceleration.
\newblock In \emph{MLSys}, 2024.

\bibitem[Liu et~al.(2024)]{liu2024visual}
Liu, H., Li, C., Wu, Q., and Lee, Y.~J.
\newblock Visual instruction tuning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2024.

\bibitem[Loshchilov and Hutter(2019)]{loshchilov2019decoupled}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Ouyang et~al.(2022)]{ouyang2022training}
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\ 27730--27744, 2022.

\bibitem[Peng et~al.(2024)]{peng2024yarn}
Peng, B., Quesnelle, J., Fan, H., and Shippole, E.
\newblock YaRN: Efficient context window extension of large language models.
\newblock In \emph{International Conference on Learning Representations}, 2024.

\bibitem[Radford et~al.(2021)]{radford2021learning}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{International Conference on Machine Learning}, pp.\ 8748--8763, 2021.

\bibitem[Radford et~al.(2023)]{radford2023robust}
Radford, A., Kim, J.~W., Xu, T., Brockman, G., McLeavey, C., and Sutskever, I.
\newblock Robust speech recognition via large-scale weak supervision.
\newblock In \emph{International Conference on Machine Learning}, pp.\ 28492--28518, 2023.

\bibitem[Rafailov et~al.(2024)]{rafailov2024direct}
Rafailov, R., Sharma, A., Mitchell, E., Manning, C.~D., Ermon, S., and Finn, C.
\newblock Direct preference optimization: Your language model is secretly a reward model.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2024.

\bibitem[Rajbhandari et~al.(2020)]{rajbhandari2020zero}
Rajbhandari, S., Rasley, J., Rber, O., and He, Y.
\newblock ZeRO: Memory optimizations toward training trillion parameter models.
\newblock In \emph{International Conference on High Performance Computing}, pp.\ 1--16, 2020.

\bibitem[Schulman et~al.(2017)]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Shazeer et~al.(2017)]{shazeer2017outrageously}
Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., and Dean, J.
\newblock Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Shoeybi et~al.(2019)]{shoeybi2019megatron}
Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B.
\newblock Megatron-LM: Training multi-billion parameter language models using model parallelism.
\newblock \emph{arXiv preprint arXiv:1909.08053}, 2019.

\bibitem[Su et~al.(2024)]{su2024roformer}
Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y.
\newblock RoFormer: Enhanced transformer with rotary position embedding.
\newblock \emph{Neurocomputing}, 568:127063, 2024.

\bibitem[Tang et~al.(2024)]{tang2024salmonn}
Tang, C., Yu, W., Sun, G., Chen, X., Tan, T., Li, W., Lu, L., Ma, Z., and Zhang, C.
\newblock SALMONN: Towards generic hearing abilities for large language models.
\newblock In \emph{International Conference on Learning Representations}, 2024.

\bibitem[Vaswani et~al.(2017)]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\ 5998--6008, 2017.

\bibitem[Zhai et~al.(2023)]{zhai2023sigmoid}
Zhai, X., Mustafa, B., Kolesnikov, A., and Beyer, L.
\newblock Sigmoid loss for language image pre-training.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer Vision}, pp.\ 11975--11986, 2023.

\bibitem[Zhou et~al.(2022)]{zhou2022mixture}
Zhou, Y., Lei, T., Liu, H., Du, N., Huang, Y., Zhao, V., Dai, A.~M., Le, Q.~V., Laudon, J., et~al.
\newblock Mixture-of-experts with expert choice routing.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Zoph et~al.(2022)]{zoph2022st}
Zoph, B., Bello, I., Kumar, S., Du, N., Huang, Y., Dean, J., Shazeer, N., and Fedus, W.
\newblock ST-MoE: Designing stable and transferable sparse expert models.
\newblock \emph{arXiv preprint arXiv:2202.08906}, 2022.

\end{thebibliography}

% =============================================================================
\appendix

\section{Complete Benchmark Results}
\label{app:benchmarks}

\begin{table}[h]
\centering
\caption{Complete results across all 42 benchmarks for all model sizes.}
\label{tab:full_benchmarks}
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccc}
\toprule
\textbf{Benchmark} & \textbf{600M} & \textbf{1.5B} & \textbf{7B} & \textbf{32B} & \textbf{72B} & \textbf{480B} \\
\midrule
MMLU & 32.1 & 48.3 & 72.4 & 83.1 & 89.2 & 92.1 \\
MMLU-Pro & 21.4 & 34.2 & 58.3 & 71.2 & 78.4 & 82.3 \\
ARC-C & 48.2 & 62.4 & 88.2 & 93.7 & 96.1 & 97.4 \\
HellaSwag & 42.1 & 58.3 & 82.1 & 88.4 & 91.3 & 93.2 \\
WinoGrande & 52.3 & 62.1 & 78.6 & 84.2 & 87.8 & 89.4 \\
TruthfulQA & 38.2 & 42.1 & 52.3 & 64.8 & 71.2 & 74.3 \\
BBH & 28.4 & 41.2 & 68.4 & 78.3 & 84.6 & 88.1 \\
GSM8K & 12.3 & 32.4 & 82.1 & 90.4 & 95.2 & 97.1 \\
MATH & 4.2 & 14.8 & 51.3 & 68.2 & 78.6 & 84.2 \\
HumanEval & 18.3 & 38.4 & 72.6 & 85.4 & 92.1 & 95.3 \\
MBPP & 22.1 & 42.3 & 74.2 & 84.6 & 90.8 & 93.7 \\
MMBench & -- & 28.4 & 68.3 & 78.2 & 84.7 & 88.2 \\
\bottomrule
\end{tabular}
}
\end{table}

\section{Training Loss Curves}
\label{app:loss}

All models exhibit smooth loss convergence with no significant instabilities, confirming the effectiveness of the MoDE initialization procedure. The Zen-72B model reaches a final training loss of 1.72 nats on the validation set after processing 14.8T tokens. Loss decreases monotonically with model size at fixed token count, consistent with established scaling laws.

\section{Tokenizer Details}
\label{app:tokenizer}

The Zen tokenizer is a BPE tokenizer trained on a multilingual corpus with the following characteristics:
\begin{itemize}
    \item Vocabulary size: 152,064 tokens
    \item Special tokens: 1,024 reserved
    \item Byte fallback: All byte values are in the vocabulary, ensuring lossless encoding of any UTF-8 text
    \item Compression ratio: 3.8 characters per token (English), 1.2 characters per token (Chinese), 2.1 characters per token (code)
    \item Training data: 100B characters sampled proportionally from the pre-training data mix
\end{itemize}

\end{document}
