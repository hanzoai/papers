\documentclass[11pt,twocolumn]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}

\lstset{
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red},
  breaklines=true,
  frame=single
}

\title{Hanzo: An AI-First Commerce Platform}

\author{Zach Kelling\\
Hanzo Industries\\
\texttt{zach@hanzo.ai}}

\date{2017}

\begin{document}

\maketitle

\begin{abstract}
We present Hanzo, an AI-first commerce platform that integrates machine learning throughout the customer journey. Unlike traditional e-commerce systems that bolt AI capabilities onto existing architectures, Hanzo was designed from inception with artificial intelligence as the core organizing principle. The platform provides real-time personalized recommendations, intelligent search ranking, dynamic pricing optimization, automated fraud detection, and predictive inventory management through a unified AI infrastructure. We describe the system architecture, detail the machine learning models powering each capability, and evaluate performance across production deployments. Results demonstrate significant improvements: 28\% increase in average order value through personalization, 45\% reduction in fraud losses, and 18\% improvement in inventory turnover. Hanzo represents a new paradigm for commerce platforms where AI is not an add-on but the foundational layer.
\end{abstract}

\section{Introduction}

The history of e-commerce platforms follows a pattern of accretion. Systems designed for catalog management acquire shopping carts, then payment processing, then analytics, then---belatedly---AI capabilities. Each addition creates integration complexity while the core architecture, optimized for earlier requirements, constrains what AI can achieve.

Hanzo inverts this pattern. We designed an AI-first commerce platform where machine learning models are not peripheral services but central infrastructure. Every customer interaction---search, browse, cart, checkout---flows through AI systems that personalize, predict, and optimize in real time.

This architectural decision yields compounding benefits:
\begin{enumerate}
\item \textbf{Unified data model}: All events feed into a shared feature store, enabling cross-functional learning.
\item \textbf{Consistent inference}: A single model serving infrastructure ensures low-latency predictions across all surfaces.
\item \textbf{Rapid iteration}: Standardized ML pipelines accelerate model development and deployment.
\item \textbf{Emergent intelligence}: Models learn from each other's predictions, creating feedback loops that improve system-wide performance.
\end{enumerate}

Our contributions are:
\begin{enumerate}
\item An AI-first architecture for commerce platforms with unified feature stores and model serving.
\item Production implementations of personalization, search ranking, fraud detection, and dynamic pricing.
\item Empirical evaluation demonstrating significant improvements across key commerce metrics.
\item Open discussion of challenges and lessons learned in deploying AI at commerce scale.
\end{enumerate}

\section{Background}

\subsection{E-Commerce AI Evolution}

AI in e-commerce has evolved through three generations:

\textbf{First generation (1995--2005): Rule-based systems.} Simple heuristics drive recommendations (``customers who bought X also bought Y''), fraud rules (velocity checks, blacklists), and pricing (cost-plus margins).

\textbf{Second generation (2005--2015): Specialized ML.} Machine learning models improve specific functions. Amazon's item-to-item collaborative filtering \cite{linden2003} revolutionized recommendations. PayPal deployed neural networks for fraud detection \cite{paypal2013}. These systems operate independently with limited cross-functional learning.

\textbf{Third generation (2015--present): Integrated AI.} Modern systems unify ML infrastructure across functions. Alibaba's AI platform \cite{alibaba2017} demonstrates system-wide intelligence. Hanzo extends this approach to a platform available to any merchant.

\subsection{Machine Learning Infrastructure}

Production ML requires infrastructure beyond model training:

\begin{itemize}
\item \textbf{Feature stores}: Centralized repositories for feature computation and serving.
\item \textbf{Model registries}: Version control for trained models.
\item \textbf{Serving infrastructure}: Low-latency prediction at scale.
\item \textbf{Monitoring}: Drift detection and performance tracking.
\end{itemize}

Hanzo builds upon emerging standards (Feast, MLflow, TensorFlow Serving) while adding commerce-specific extensions.

\section{System Architecture}

\subsection{Design Principles}

Hanzo adheres to four architectural principles:

\textbf{Principle 1: Events as First-Class Citizens.} Every user action, system event, and model prediction is an event in a unified stream. Events are immutable, ordered, and available for both real-time and batch processing.

\textbf{Principle 2: Shared Feature Store.} All models draw features from a centralized store. This ensures consistency (the same feature computation serves training and inference) and enables feature reuse across models.

\textbf{Principle 3: Online-Offline Parity.} Models train on historical data and serve in real-time. Infrastructure ensures the same feature transformations apply in both contexts, eliminating training-serving skew.

\textbf{Principle 4: Continuous Learning.} Models update continuously as new data arrives. Feedback loops from predictions to outcomes enable rapid adaptation.

\subsection{Core Components}

\subsubsection{Event Stream}

All platform activity flows through Apache Kafka:

\begin{lstlisting}[language=Python,caption=Event schema]
@dataclass
class Event:
    event_id: str
    event_type: str
    timestamp: datetime
    user_id: Optional[str]
    session_id: str
    payload: Dict[str, Any]
    context: Dict[str, Any]  # device, location, etc.
\end{lstlisting}

Event types include:
\begin{itemize}
\item \texttt{page\_view}: User views a page.
\item \texttt{product\_view}: User views a product.
\item \texttt{add\_to\_cart}: User adds item to cart.
\item \texttt{purchase}: Transaction completed.
\item \texttt{recommendation\_shown}: Model prediction served.
\item \texttt{recommendation\_clicked}: User engaged with recommendation.
\end{itemize}

\subsubsection{Feature Store}

The feature store provides:

\textbf{Feature computation.} Declarative feature definitions:

\begin{lstlisting}[language=Python,caption=Feature definition]
@feature
def user_purchase_count_7d(user_id: str) -> int:
    """Count of purchases in last 7 days."""
    return events.filter(
        event_type="purchase",
        user_id=user_id,
        timestamp__gte=now() - days(7)
    ).count()

@feature
def product_view_to_purchase_rate(product_id: str) -> float:
    """Conversion rate from view to purchase."""
    views = events.filter(event_type="product_view", product_id=product_id).count()
    purchases = events.filter(event_type="purchase", product_id=product_id).count()
    return purchases / max(views, 1)
\end{lstlisting}

\textbf{Online serving.} Low-latency feature retrieval (p99 < 5ms):

\begin{lstlisting}[language=Python,caption=Feature retrieval]
features = feature_store.get_features(
    feature_names=["user_purchase_count_7d", "user_avg_order_value"],
    entity_ids={"user_id": "user_123"}
)
\end{lstlisting}

\textbf{Offline training.} Point-in-time correct feature joins for training data:

\begin{lstlisting}[language=Python,caption=Training data generation]
training_data = feature_store.get_historical_features(
    feature_names=["user_purchase_count_7d", "product_popularity"],
    entity_df=labels_df,  # Contains user_id, product_id, timestamp, label
)
\end{lstlisting}

\subsubsection{Model Serving}

Models deploy to a unified serving layer:

\begin{lstlisting}[language=Python,caption=Model serving]
class RecommendationModel:
    def __init__(self, model_path: str):
        self.model = tf.saved_model.load(model_path)
        self.feature_spec = load_feature_spec(model_path)

    def predict(self, user_id: str, candidate_ids: List[str]) -> List[float]:
        features = feature_store.get_features(
            feature_names=self.feature_spec,
            entity_ids={"user_id": user_id, "product_ids": candidate_ids}
        )
        return self.model.predict(features)
\end{lstlisting}

\subsection{AI Capabilities}

\subsubsection{Personalized Recommendations}

Recommendations power multiple surfaces: homepage, product pages, cart, and email.

\textbf{Model architecture.} We employ a two-tower neural network \cite{youtube2016}:

\begin{align}
u &= f_{\text{user}}(x_{\text{user}}) \\
v &= f_{\text{item}}(x_{\text{item}}) \\
\text{score} &= u \cdot v
\end{align}

The user tower $f_{\text{user}}$ encodes user features (browsing history, purchase history, demographics). The item tower $f_{\text{item}}$ encodes product features (category, price, description embeddings). Dot product scoring enables efficient retrieval via approximate nearest neighbors.

\textbf{Features.} Key features include:
\begin{itemize}
\item User: Recent views (sequence), purchase history, session duration, device type.
\item Item: Category hierarchy, price percentile, description embedding, popularity.
\item Context: Time of day, day of week, referral source.
\end{itemize}

\textbf{Training.} The model trains on implicit feedback:
\begin{itemize}
\item Positive: Purchases, add-to-carts, extended views (> 30 seconds).
\item Negative: Sampled non-interactions, quick bounces.
\end{itemize}

We use batch negatives with in-batch sampling for efficient training.

\subsubsection{Intelligent Search}

Search ranking combines text relevance with personalization.

\textbf{Architecture.} A learning-to-rank model \cite{liu2009} re-ranks candidates from Elasticsearch:

\begin{enumerate}
\item \textbf{Retrieval}: Elasticsearch returns top 1000 candidates by BM25.
\item \textbf{Scoring}: Neural ranker scores candidates using user context.
\item \textbf{Re-ranking}: Final ordering by learned scores.
\end{enumerate}

\textbf{Model.} We use a cross-attention transformer:

\begin{lstlisting}[language=Python,caption=Search ranking model]
class SearchRanker(nn.Module):
    def __init__(self, config):
        self.query_encoder = TransformerEncoder(config)
        self.product_encoder = TransformerEncoder(config)
        self.cross_attention = CrossAttention(config)
        self.scorer = nn.Linear(config.hidden_size, 1)

    def forward(self, query_tokens, product_features, user_features):
        query_emb = self.query_encoder(query_tokens)
        product_emb = self.product_encoder(product_features)
        cross_emb = self.cross_attention(query_emb, product_emb, user_features)
        return self.scorer(cross_emb)
\end{lstlisting}

\textbf{Training data.} Click-through logs provide training signal:
\begin{itemize}
\item Queries with clicked results (positive).
\item Queries with skipped results (negative).
\item Purchase after search (strong positive).
\end{itemize}

\subsubsection{Fraud Detection}

Fraud detection operates at multiple stages: account creation, checkout, and post-transaction.

\textbf{Model architecture.} An ensemble of gradient boosted trees (XGBoost) and neural networks:

\begin{equation}
\text{fraud\_score} = \alpha \cdot \text{XGB}(x) + (1-\alpha) \cdot \text{NN}(x)
\end{equation}

XGBoost captures interpretable rules; the neural network captures complex patterns.

\textbf{Features.} Fraud features span multiple categories:
\begin{itemize}
\item \textbf{Velocity}: Orders per hour, distinct cards per user, shipping addresses per card.
\item \textbf{Device}: Device fingerprint, IP geolocation, browser anomalies.
\item \textbf{Behavioral}: Time on site, navigation patterns, form fill speed.
\item \textbf{Network}: Graph features connecting users, devices, and addresses.
\end{itemize}

\textbf{Decision engine.} Fraud scores trigger actions:
\begin{itemize}
\item Score < 0.3: Approve automatically.
\item 0.3 $\leq$ Score < 0.7: Additional verification (3D Secure, phone verification).
\item Score $\geq$ 0.7: Manual review or automatic decline.
\end{itemize}

\subsubsection{Dynamic Pricing}

Dynamic pricing optimizes prices in real-time based on demand, competition, and inventory.

\textbf{Model.} A demand model predicts quantity sold at each price point:

\begin{equation}
q(p) = \exp(\beta_0 + \beta_1 \log p + \beta_2 x)
\end{equation}

where $p$ is price and $x$ are contextual features (day of week, competitor prices, inventory level).

\textbf{Optimization.} Given demand model, optimize for profit:

\begin{equation}
p^* = \arg\max_p (p - c) \cdot q(p)
\end{equation}

where $c$ is cost. Constraints include:
\begin{itemize}
\item Minimum margin: $p \geq (1 + m) \cdot c$.
\item Price consistency: $|p - p_{\text{prev}}| \leq \delta$.
\item Competitor parity: $p \leq \max(\text{competitor prices}) + \epsilon$.
\end{itemize}

\subsubsection{Inventory Optimization}

Predictive inventory management reduces stockouts and overstock.

\textbf{Demand forecasting.} A hierarchical time series model \cite{hyndman2011} predicts demand:

\begin{equation}
y_{t+h} = \text{trend}_t + \text{season}_t + \text{promo}_t + \epsilon_t
\end{equation}

The model forecasts at multiple aggregation levels (SKU, category, store) with reconciliation ensuring consistency.

\textbf{Reorder optimization.} Given demand forecast and lead times:

\begin{equation}
\text{reorder\_point} = \mu_L + z_\alpha \cdot \sigma_L
\end{equation}

where $\mu_L$ is expected demand during lead time, $\sigma_L$ is demand standard deviation, and $z_\alpha$ is the safety stock factor for target service level $\alpha$.

\section{Implementation}

\subsection{Technology Stack}

\begin{itemize}
\item \textbf{Event streaming}: Apache Kafka.
\item \textbf{Feature store}: Custom implementation on Redis (online) + BigQuery (offline).
\item \textbf{Model training}: TensorFlow, XGBoost.
\item \textbf{Model serving}: TensorFlow Serving, custom Python services.
\item \textbf{Orchestration}: Kubernetes, Airflow.
\end{itemize}

\subsection{Latency Requirements}

Commerce AI must meet strict latency requirements:

\begin{table}[h]
\centering
\caption{Latency requirements by surface}
\label{tab:latency}
\begin{tabular}{lrr}
\hline
Surface & p50 (ms) & p99 (ms) \\
\hline
Search ranking & 20 & 50 \\
Recommendations & 15 & 40 \\
Fraud scoring & 100 & 250 \\
Dynamic pricing & 50 & 150 \\
\hline
\end{tabular}
\end{table}

We achieve these through:
\begin{itemize}
\item Feature caching with 1-minute TTL.
\item Batch prediction for non-real-time surfaces.
\item Model quantization and optimization.
\item Geographic distribution (models deployed to edge).
\end{itemize}

\subsection{Model Updates}

Models update through a continuous training pipeline:

\begin{enumerate}
\item New events flow to training data store.
\item Nightly jobs retrain models on recent data.
\item Shadow deployment compares new model to production.
\item Automatic promotion if metrics improve.
\item Rollback if degradation detected.
\end{enumerate}

\begin{lstlisting}[language=Python,caption=Model promotion logic]
def should_promote(shadow_metrics, prod_metrics):
    # Require improvement on primary metric
    if shadow_metrics.conversion_rate < prod_metrics.conversion_rate:
        return False

    # Guard rails on secondary metrics
    if shadow_metrics.latency_p99 > prod_metrics.latency_p99 * 1.1:
        return False

    # Statistical significance
    if not is_significant(shadow_metrics, prod_metrics, alpha=0.05):
        return False

    return True
\end{lstlisting}

\section{Evaluation}

\subsection{Experimental Setup}

We evaluated Hanzo across three production deployments:

\begin{itemize}
\item \textbf{Fashion retailer}: 2M monthly active users, 50K SKUs.
\item \textbf{Electronics store}: 500K MAU, 10K SKUs.
\item \textbf{Marketplace}: 1M MAU, 200K SKUs.
\end{itemize}

Each deployment ran for 6 months. We measured against pre-Hanzo baselines.

\subsection{Recommendation Performance}

\begin{table}[h]
\centering
\caption{Recommendation metrics improvement}
\label{tab:reco}
\begin{tabular}{lrrr}
\hline
Metric & Fashion & Electronics & Marketplace \\
\hline
CTR & +45\% & +38\% & +52\% \\
Conversion & +22\% & +18\% & +31\% \\
AOV & +28\% & +24\% & +32\% \\
\hline
\end{tabular}
\end{table}

Recommendations drive significant revenue improvement. Average order value increases as recommendations surface complementary products.

\subsection{Search Quality}

\begin{table}[h]
\centering
\caption{Search metrics improvement}
\label{tab:search}
\begin{tabular}{lrrr}
\hline
Metric & Fashion & Electronics & Marketplace \\
\hline
NDCG@10 & +35\% & +28\% & +42\% \\
Zero-result rate & -62\% & -55\% & -71\% \\
Search-to-purchase & +24\% & +19\% & +28\% \\
\hline
\end{tabular}
\end{table}

Intelligent ranking improves search relevance. Query understanding reduces zero-result searches through spelling correction and synonym expansion.

\subsection{Fraud Detection}

\begin{table}[h]
\centering
\caption{Fraud metrics}
\label{tab:fraud}
\begin{tabular}{lrrr}
\hline
Metric & Fashion & Electronics & Marketplace \\
\hline
Fraud loss reduction & -42\% & -51\% & -45\% \\
False positive rate & -35\% & -28\% & -38\% \\
Manual review rate & -55\% & -48\% & -52\% \\
\hline
\end{tabular}
\end{table}

Improved fraud detection reduces both losses and operational burden. Lower false positive rates mean fewer legitimate customers face friction.

\subsection{Inventory Optimization}

\begin{table}[h]
\centering
\caption{Inventory metrics improvement}
\label{tab:inventory}
\begin{tabular}{lrrr}
\hline
Metric & Fashion & Electronics & Marketplace \\
\hline
Stockout rate & -38\% & -42\% & -35\% \\
Inventory turnover & +18\% & +22\% & +15\% \\
Markdown rate & -25\% & -18\% & -28\% \\
\hline
\end{tabular}
\end{table}

Better demand forecasting reduces both stockouts (lost sales) and overstock (markdowns).

\subsection{System Performance}

\begin{table}[h]
\centering
\caption{Production latency (ms)}
\label{tab:prod_latency}
\begin{tabular}{lrr}
\hline
Component & p50 & p99 \\
\hline
Feature retrieval & 2 & 8 \\
Recommendation inference & 12 & 35 \\
Search re-ranking & 18 & 45 \\
Fraud scoring & 45 & 180 \\
\hline
\end{tabular}
\end{table}

All components meet latency requirements in production.

\section{Lessons Learned}

\subsection{Data Quality Trumps Model Complexity}

Early iterations focused on sophisticated model architectures. We found greater improvements from:
\begin{itemize}
\item Fixing data pipeline bugs.
\item Adding missing features (device type had been dropped).
\item Correcting label noise (returns flagged as purchases).
\end{itemize}

Simple models on clean data outperformed complex models on noisy data.

\subsection{Feedback Loops Require Careful Design}

Recommendations create feedback loops: recommended items get more views, which makes them appear more popular, which increases recommendation probability. We address this through:
\begin{itemize}
\item Exploration-exploitation balancing.
\item Position debiasing in training.
\item Diversity constraints in serving.
\end{itemize}

\subsection{Explain ability Builds Trust}

Merchants hesitated to trust AI pricing and fraud decisions. Adding explanations (``Price increased due to low inventory and high demand'') increased adoption from 34\% to 78\% of eligible merchants.

\section{Related Work}

Amazon's personalization \cite{linden2003} pioneered item-based collaborative filtering. Netflix Prize \cite{netflix2009} advanced matrix factorization methods. YouTube's recommendation system \cite{youtube2016} introduced deep learning at scale. Hanzo synthesizes these advances into an integrated platform.

Alibaba's AI platform \cite{alibaba2017} demonstrates similar system-wide integration. Our contribution extends these capabilities to a platform available to any merchant, not just large enterprises.

\section{Conclusion}

Hanzo demonstrates that AI-first architecture yields significant improvements across commerce metrics. By designing machine learning into the platform foundation rather than bolting it on, we achieve:
\begin{itemize}
\item 28\% average order value increase.
\item 45\% fraud loss reduction.
\item 18\% inventory turnover improvement.
\end{itemize}

The key insight is architectural: unified event streams, shared feature stores, and consistent model serving create compounding benefits impossible with siloed AI additions.

Future work will extend Hanzo with conversational commerce (chatbots with full platform context), visual search (find similar products from images), and automated merchandising (AI-generated product descriptions and imagery).

\begin{thebibliography}{9}

\bibitem{linden2003}
G. Linden, B. Smith, and J. York, ``Amazon.com Recommendations: Item-to-Item Collaborative Filtering,'' \textit{IEEE Internet Computing}, vol. 7, no. 1, pp. 76--80, 2003.

\bibitem{paypal2013}
PayPal Engineering, ``Fraud Detection with Machine Learning,'' \textit{PayPal Engineering Blog}, 2013.

\bibitem{alibaba2017}
Alibaba Group, ``Artificial Intelligence at Alibaba,'' \textit{Alibaba Tech Blog}, 2017.

\bibitem{youtube2016}
P. Covington, J. Adams, and E. Sargin, ``Deep Neural Networks for YouTube Recommendations,'' \textit{RecSys}, 2016.

\bibitem{liu2009}
T.-Y. Liu, ``Learning to Rank for Information Retrieval,'' \textit{Foundations and Trends in Information Retrieval}, vol. 3, no. 3, pp. 225--331, 2009.

\bibitem{hyndman2011}
R. J. Hyndman and G. Athanasopoulos, \textit{Forecasting: Principles and Practice}, OTexts, 2011.

\bibitem{netflix2009}
J. Bennett and S. Lanning, ``The Netflix Prize,'' \textit{KDD Cup and Workshop}, 2007.

\end{thebibliography}

\end{document}
