% Hanzo Network Whitepaper â€” Hamiltonian Market Maker (HMM) L1 for AI Compute
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{xcolor}
\hypersetup{colorlinks=true,linkcolor=black,citecolor=blue,urlcolor=blue}

\title{Hanzo Network: A Hamiltonian Market Maker Layer-1 for Decentralized AI Compute and Semantic Learning}
\author{
    Zach Kelling\textsuperscript{1,2,3}\thanks{Corresponding author: research@hanzo.ai} \\
    \\
    \textsuperscript{1}\textit{Hanzo Industries Inc (Techstars '17), Los Angeles, CA} \\
    \textsuperscript{2}\textit{Lux Partners, \texttt{research@lux.network}} \\
    \textsuperscript{3}\textit{Zoo Labs Foundation (501(c)(3)), \texttt{research@zoo.ngo}}
}
\date{October 2025}

\begin{document}
\maketitle

\begin{abstract}
We present \emph{Hanzo Network}, a specialized Layer-1 (L1) blockchain for AI compute exchange and decentralized semantic learning. Hanzo introduces a \textbf{Hamiltonian Market Maker (HMM)}---a provably-stable, oracle-minimal automated market maker that prices heterogeneous compute resources via a Hamiltonian invariant. Hanzo's semantic learning layer executes \textbf{Decentralized Semantic Optimization (DSO)} and \textbf{Active Semantic Optimization (ASO)}: training-free adaptation that shares token/embedding-level \emph{experiential priors} across models and nodes. Contributions include: (i) an HMM with invariant \(\mathcal H\) for multi-asset compute markets and continuous-time price dynamics, (ii) a \emph{Proof of AI} (PoAI) consensus-extension for verifiable inference/training work, (iii) a Training-Free GRPO (TF-GRPO) scheme formalized as Bayesian product-of-experts (PoE) decoding, and (iv) a BitDelta-inspired 1-bit semantic compression enabling 29.5\,$\times$ storage and multi-tenant serving efficiency. We detail protocols, security, and token economics for the \$AI open protocol token used for staking, fees, rewards, and settlement.
\end{abstract}

\section{Introduction}
Modern AI systems are bottlenecked by (1) scarce, dynamic supply of compute (GPUs, memory, bandwidth), and (2) high-cost, siloed model adaptation. Hanzo addresses both by (a) making compute a first-class on-chain asset with a Hamiltonian AMM that clears resource markets without fragile oracles, and (b) providing decentralized, \emph{zero-training} semantic learning (TF-GRPO) so all participants benefit from shared experiences without finetuning.

\paragraph{Vision.} Hanzo Network integrates both the \textbf{compute L1} and \textbf{semantic learning layer} into a unified protocol. Nodes earn \$AI by (i) providing compute and (ii) contributing high-quality experiential priors validated on-chain. The result is a transparent, efficient, and privacy-preserving substrate where any LLM can improve using cross-LLM experiences while jobs are priced and cleared in real time.

\section{Design Goals}
\begin{itemize}[leftmargin=1.1em]
  \item \textbf{Oracle-minimal pricing:} resource prices arise endogenously from a conservative invariant; external price feeds are optional.
  \item \textbf{Verifiable work:} inference/training attestations via TEE-anchored receipts and/or succinct proofs.
  \item \textbf{Zero-training adaptation:} TF-GRPO \& PoE decoding with compressed priors (1-bit deltas) for cheap personalization.
  \item \textbf{Byzantine robustness:} median-based aggregation for experiences; slashing for fraudulent attestations.
  \item \textbf{Composable L1:} EVM compatibility for DeFi/primitives; modules for markets, registry, staking.
\end{itemize}

\section{System Overview}
\textbf{Roles.} Validators; \emph{Workers} (GPU, CPU, RAM, storage); \emph{Routers} (batching/scheduling); \emph{Curators} (experience quality signals). \textbf{Assets.} \(C_{gpu}, C_{vram}, C_{ram}, C_{net}, C_{disk}\) (resource tokens); \(Q\) (demand credits); \$AI (settlement/staking). \textbf{Architecture.} Hanzo L1 hosts HMM, registry, DSO/ASO, and P2P sync for experiential priors.

\section{Hamiltonian Market Maker (HMM)}
\subsection{Invariant and State}
Let reserve vector \(\bm R = (\Psi, \Theta)\) denote effective supply of compute capacity \(\Psi\) (e.g., GPU-seconds weighted by quality) and an aggregate demand credit pool \(\Theta\). A minimal HMM uses the \textbf{bilinear} Hamiltonian
\begin{equation}\label{eq:cpmm}
\mathcal H(\Psi,\Theta) = \Psi\,\Theta = \kappa, \quad \kappa>0,
\end{equation}
which matches the constant-product AMM as a special case. For multi-asset resources \(\bm \Psi=(\Psi_1,\dots,\Psi_m)\) and credits \(\bm \Theta\), we use
\begin{equation}
\mathcal H(\bm\Psi,\bm\Theta) = \sum_{i=1}^m w_i\,\Psi_i\,\Theta_i + \lambda \sum_{i=1}^m \tfrac{1}{2}(\Psi_i^2+\Theta_i^2), \quad w_i,\lambda>0.
\end{equation}
The quadratic term controls curvature (inventory risk), yielding smoother quotes.

\subsection{Prices, Flows, and Fees}
Define the conjugate price for compute class \(i\):
\begin{equation}
 p_i \equiv \frac{\partial \mathcal H/\partial \Psi_i}{\partial \mathcal H/\partial \Theta_i} = \frac{w_i\,\Theta_i + \lambda\,\Psi_i}{w_i\,\Psi_i + \lambda\,\Theta_i}.
\end{equation}
A swap \(\Delta\bm\Theta<0, \Delta\bm\Psi>0\) (buy compute) preserves \(\mathcal H\) up to fee \(f\). We charge a split fee \(f=f_m+f_r\): market fee \(f_m\) (LP/treasury) and \emph{risk fee} \(f_r\propto \|\Delta\bm\Psi\|\) to compensate inventory risk. In continuous time, inventory evolves via
\begin{equation}
\dot{\Psi}_i = s_i - u_i,\quad \dot{\Theta}_i = d_i - v_i,\quad \text{s.t. } \frac{d}{dt}\mathcal H(\bm\Psi,\bm\Theta)=0 \, (\text{net of fees})
\end{equation}
with supply inflow \(s_i\) (workers) and demand \(d_i\) (jobs). Stability follows from convexity of \(\mathcal H\) in each orthant and fee dissipation.

\subsection{Composable Market Objects}
Each resource class instantiates an HMM pool; cross-resource jobs route via a \emph{path solver} minimizing total cost under \(\mathcal H\)-preserving constraints. Jobs specify an SLA vector (latency, jitter, region), encoded as Lagrange multipliers in the solver; quotes reflect SLA shadow prices.

\section{Proof of AI (PoAI) and Job Settlement}
\subsection{Task Lifecycle}
(1) Client escrows \$AI and mints a credit \(\Delta\Theta\). (2) Router clears against HMM to allocate \(\Delta\Psi\). (3) Workers execute and emit \emph{attestations}: TEE report + Merkle commitments of I/O + optional succinct proof. (4) Verifiers sample-check; (5) Settlement releases \$AI to workers, rebates unused capacity to pool, distributes fees.

\subsection{Attestation Primitives}
\emph{TEE path:} enclave measurements + signed runtime traces. \emph{ZK path:} SNARK-friendly kernels for small circuits; \emph{Batch audit:} randomized canary prompts or seed-replay for LLM inference. Misbehavior triggers slashing and denial windows.

\section{Decentralized Semantic Optimization (DSO)}
\subsection{Experience Priors}
Each agent/node maintains an \emph{experience prior} \(E\): token/embedding-level memory distilled from rollouts. Locally, nodes run \textbf{Active Semantic Optimization (ASO)} to extract \emph{semantic advantages} from groups of rollouts (TF-GRPO). Priors are compressed (\S\ref{sec:bitdelta}) and written to the on-chain \emph{ExperienceRegistry} with Merkle proofs.

\subsection{Training-Free GRPO as Bayesian PoE}
For a base model with conditional \(p_\theta(y\mid x)\) and a set of experiences \(\{e_k\}\) mapping to token-level factors \(\phi_k(y\mid x)\), decoding uses a \emph{product-of-experts}:
\begin{equation}\label{eq:poe}
 p(y\mid x,E) \propto p_\theta(y\mid x)\,\prod_{k} \phi_k(y\mid x)^{\alpha_k},\quad \alpha_k\ge 0.
\end{equation}
Here \(\phi_k\) are distilled from group-relative semantic advantage; weights \(\alpha_k\) are learned by introspective calibration without gradient updates to \(\theta\).

\subsection{Distributed Aggregation}
Hanzo Network aggregates \emph{priors, not gradients}. Let node priors be \(\{E_i\}\). We publish hashes and quality scores; the chain computes a byzantine-robust aggregate \(\tilde E = \operatorname{median}\_q\{E_i\}\) under a fixed schema (token bins / embedding centroids). Conflicting contributions resolve by stake-weighted quorum plus quality caps.

\section{1-Bit Semantic Compression}\label{sec:bitdelta}
Inspired by BitDelta, we store only the \emph{signs} of per-bucket deltas plus per-matrix scales. For an experience matrix \(\Delta \in \mathbb R^{n\times m}\),
\begin{equation}
 \widehat{\Delta} = \alpha\,\operatorname{Sign}(\Delta),\quad \alpha = \tfrac{1}{nm}\sum\limits\_{ij} |\Delta\_{ij}|.
\end{equation}
Scales are distilled by matching logits to a teacher rollout. We observe \(\approx 29.5\times\) storage savings with negligible loss in downstream utility, enabling multi-tenant caching and rapid hot-swaps of personalizations.

\section{ExperienceRegistry and P2P Sync}
\textbf{Registry.} On-chain contract stores: content-addressed CID, Merkle root, schema version, quality vector, submitter, slashing bond. \textbf{Storage.} Off-chain IPFS/Arweave; local SQLite+LanceDB with Merkle verification. \textbf{Sync.} Gossip protocol with CRDT merge; priority given to high-quality shards (fee rebates bias peers to propagate them).

\section{Token Economics (\$AI)}
\subsection{Utility}
\$AI is the protocol token for staking, market fees, job settlement, and governance. \emph{Compute credits} \(\Theta\) are minted by locking \$AI at current HMM rate and burned on settlement.

\subsection{Emissions and Rewards}
Per block, distribute \(R\) \$AI: validators \(\beta R\), workers \(\gamma R\) pro-rata verified work, curators \(\delta R\) by experience quality shares, treasury \((1-\beta-\gamma-\delta)R\). A PoAI bonus applies: for job \(j\) with value \(V_j\) and verified cost \(K_j\), reward \(\rho V_j\) (\(\rho\le 0.1\)) split among parties. Slashing burns a fraction \(\sigma\) of bonds on fraud.

\subsection{Fees and Burns}
HMM fees split to LPs and treasury; a fixed fraction \(\zeta\) of market fees is burned to offset emissions. Experience submissions pay a deposit \(D\); refunds scale with measured utility.

\section{Security and Governance}
\textbf{Flash-\& MEV-resistance:} HMM quotes include dynamic risk fees; frequent batch auctions for large jobs; commitment-reveal for order flow. \textbf{Oracle bypass:} endogenous pricing limits oracle risk; optional TWAP oracles for cross-chain settlement. \textbf{Governance:} \$AI holders elect parameter councils with guarded timelocks; security council can pause attesters.

\section{Implementation Plan}
\textbf{Phase 0 (week 0--2):} HMM single-pool prototype; ExperienceRegistry (Solidity); IPFS/Arweave sink. \textbf{Phase 1 (week 3--6):} Multi-asset HMM; PoAI receipts (TEE path); Zoo DSO local optimizer; GPU-accelerated retrieval (Candle tensors). \textbf{Phase 2 (week 7--12):} Verifier network; batch auctions; DAO UI; 100+ node load test. \textbf{Phase 3 (week 13+):} ZK path pilots; security audit; mainnet.

\section{Relation to Active Inference}
Active inference views each agent as performing Bayesian updates; sharing beliefs resembles multiplying priors. Our TF-GRPO matches this: Eq.~\eqref{eq:poe} is a product-of-experts over experiential beliefs, yielding principled, decentralized Bayesian belief propagation without weight updates.

\section{Related Work}
Constant-product AMMs; inventory-risk AMMs; TEEs and verifiable compute; parameter-efficient adaptation; delta compression; in-context RL; training-free alignment. (Surveyed qualitatively; implementation choices are original here.)

\section{Conclusion}
Hanzo Network integrates a Hamiltonian AMM for compute with decentralized, zero-training semantic learning. The result is a practical L1 for AI where market-cleared compute and pooled experiential priors compound to deliver cheaper, better, and safer AI.

\appendix
\section{HMM Mechanics and Proof Sketches}
\paragraph{No-arbitrage under invariant.} For any feasible swap that preserves \(\mathcal H\) net of fees, marginal price equals the gradient ratio; convex curvature and risk fees prevent cyclical arbitrage in continuous time.

\paragraph{Multi-asset routing.} With convex \(\mathcal H\), the path solver is a convex program; KKT multipliers interpret as SLA shadow prices.

\section{Solidity Interfaces (Sketch)}
\begin{verbatim}
interface IExperienceRegistry {
  struct Entry {
    bytes32 merkleRoot;
    string cid; // IPFS/Arweave
    uint64  schema;
    uint64  quality; // quantized
    address submitter;
    uint256 bond;    // slashing collateral
  }
  function submit(Entry calldata e) external payable returns (uint256 id);
  function voteQuality(uint256 id, uint64 score) external;
  function slash(uint256 id, address challenger, bytes calldata proof) external;
  function get(uint256 id) external view returns (Entry memory);
}

interface IHMM {
  function quoteBuy(uint256 poolId, uint256 dTheta)
    external view returns (uint256 dPsi, uint256 fee);
  function swap(uint256 poolId, uint256 dTheta, uint256 minPsi)
    external payable returns (uint256 dPsi);
  function addLiquidity(uint256 poolId, uint256 dPsi, uint256 dTheta)
    external returns (uint256 lpShares);
}
\end{verbatim}

\section{Algorithms}
\subsection*{TF-GRPO (Training-Free) with PoE Decoding}
\begin{algorithm}[H]
\caption{Local ASO/TF-GRPO Step}
\begin{algorithmic}[1]
\State \textbf{input:} query set \(\mathcal D\), group size \(G\), base model \(p_\theta\), current prior bank \(E\)
\For{\(x\in\mathcal D\)}
  \State Generate group rollouts \(\{y^{(g)}\}\_{g=1}^G\) with PoE decoding using current \(E\)
  \State Score with reward model / tools to get \(\{r^{(g)}\}\)
  \State Extract semantic advantage text \(A\) via LLM introspection over the group
  \State Distill \(A\) into token/embedding buckets to produce \(\Delta E\)
  \State Compress \(\Delta E\) to (signs, scales); append to local bank \(E\)
\EndFor
\State Return compressed shard for registry submission
\end{algorithmic}
\end{algorithm}

\subsection*{PoE Decoding}
\begin{algorithm}[H]
\caption{Product-of-Experts Decoding}
\begin{algorithmic}[1]
\State logits \(\bm z = \log p_\theta(\cdot\mid x)\)
\For{expert \(k\)}
  \State compute expert log-factor \(\bm h_k = \log \phi_k(\cdot\mid x)\)
  \State \(\bm z \leftarrow \bm z + \alpha_k \bm h_k\)
\EndFor
\State sample or argmax from \(\operatorname{softmax}(\bm z)\)
\end{algorithmic}
\end{algorithm}

\section{Default Parameters (Initial Mainnet)}
\begin{table}[h]
\centering
\begin{tabular}{lll}
\toprule
Symbol & Meaning & Default \\
\midrule
\(f_m\) & market fee & 30 bps \\
\(f_r\) & risk fee coeff. & 5--20 bps per \% inventory move \\
\(\lambda\) & curvature & 0.05 \\
\(\beta,\gamma,\delta\) & emissions split & 0.35/0.50/0.10 \\
\(\zeta\) & fee burn & 0.25 \\
\(D\) & registry bond & 25 \$AI \\
\bottomrule
\end{tabular}
\end{table}

\vspace{1em}
\noindent\textit{Disclaimer.} This document describes a proposed protocol. Parameters and mechanisms may evolve with audit and community input.

\end{document}
