% =============================================================================
% Zen-Voyager: Autonomous Exploration and Navigation in Open Worlds
% Hanzo AI Inc. & Zoo Labs Foundation
% Technical Whitepaper v1.0 â€” February 2026
% =============================================================================

\documentclass[11pt,a4paper]{article}

% --- Encoding & Fonts ---------------------------------------------------------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

% --- Mathematics --------------------------------------------------------------
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{bm}

% --- Layout & Geometry --------------------------------------------------------
\usepackage[top=1in,bottom=1in,left=1.25in,right=1.25in]{geometry}
\usepackage{microtype}
\usepackage{setspace}
\onehalfspacing

% --- Graphics & Tables --------------------------------------------------------
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{array}
\usepackage{float}

% --- Algorithms ---------------------------------------------------------------
\usepackage{algorithm}
\usepackage{algpseudocode}
\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \textbf{do}}

% --- Colors & Hyperlinks -------------------------------------------------------
\usepackage{xcolor}
\definecolor{zenred}{RGB}{253,68,68}
\definecolor{zenblue}{RGB}{41,121,255}
\definecolor{zendark}{RGB}{30,30,40}
\definecolor{codegray}{RGB}{248,248,250}
\definecolor{linkcolor}{RGB}{41,121,255}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=zenblue,
    urlcolor=zenblue,
    citecolor=zenred,
    pdftitle={Zen-Voyager: Autonomous Exploration and Navigation in Open Worlds},
    pdfauthor={Hanzo AI Inc., Zoo Labs Foundation},
    pdfsubject={Embodied AI, Autonomous Navigation, Curiosity-Driven Exploration},
    pdfkeywords={embodied AI, exploration, navigation, semantic mapping, sim-to-real}
}

% --- Code Listings ------------------------------------------------------------
\usepackage{listings}
\lstset{
    backgroundcolor=\color{codegray},
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    captionpos=b,
    frame=single,
    numbers=left,
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{zenblue}\bfseries,
    stringstyle=\color{zenred},
    commentstyle=\color{gray}\itshape,
    showstringspaces=false,
    tabsize=2
}

% --- Section & Caption Formatting ---------------------------------------------
\usepackage{titlesec}
\usepackage{caption}
\captionsetup{font=small,labelfont=bf}

% --- Theorems & Definitions ---------------------------------------------------
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]

% --- Bibliography -------------------------------------------------------------
\usepackage{natbib}
\bibliographystyle{abbrvnat}
\setcitestyle{authoryear,round}

% =============================================================================
% TITLE BLOCK
% =============================================================================
\title{
    \vspace{-1.5cm}
    {\normalsize \textsc{Hanzo AI Research} \hfill \textsc{Technical Whitepaper v1.0}} \\[0.8em]
    \rule{\linewidth}{0.5pt} \\[0.6em]
    {\LARGE \textbf{Zen-Voyager:}} \\[0.3em]
    {\Large Autonomous Exploration and Navigation in Open Worlds} \\[0.3em]
    \rule{\linewidth}{0.5pt}
}

\author{
    \textbf{Hanzo AI Research}$^{1}$ \quad \textbf{Zoo Labs Foundation}$^{2}$ \\[0.6em]
    $^{1}$Hanzo AI Inc. (Techstars '17) \quad $^{2}$Zoo Labs Foundation (501(c)(3)) \\[0.3em]
    \texttt{research@hanzo.ai} \quad \texttt{foundation@zoo.ngo} \\[0.3em]
    {\small \url{https://hanzo.ai/research/zen-voyager}}
}

\date{February 2026}

% =============================================================================
\begin{document}
\maketitle

\begin{abstract}
We present \textbf{Zen-Voyager}, a foundation model for autonomous exploration and navigation in open-world environments. Zen-Voyager integrates curiosity-driven exploration with hierarchical goal planning and real-time semantic mapping to enable embodied agents that can navigate, understand, and reason about previously unseen environments without task-specific fine-tuning. Our architecture combines a vision-language backbone derived from the Zen model family with a novel Curiosity-Guided Exploration Module (CGEM) that computes intrinsic motivation signals from prediction errors in a learned world model. We introduce Semantic Occupancy Graphs (SOGs), a unified spatial representation that jointly encodes geometric occupancy, semantic labels, and object affordances, enabling both metric and topological planning. Zen-Voyager achieves state-of-the-art results on Habitat ObjectNav (87.3\% success rate), AI2-THOR RoomNav (91.2\% SPL), and the Open-World Navigation Benchmark (OWNB), outperforming prior methods by 12--18\% in zero-shot transfer. We further demonstrate successful sim-to-real transfer on a Boston Dynamics Spot robot navigating office and warehouse environments with no additional training, achieving 82.7\% success on 150 real-world navigation episodes. All models, code, and benchmark extensions are released under Apache 2.0.
\end{abstract}

\vspace{0.5em}
\noindent\textbf{Keywords:} Embodied AI, Curiosity-Driven Exploration, Semantic Mapping, Hierarchical Planning, Sim-to-Real Transfer, Open-World Navigation

% =============================================================================
\section{Introduction}
\label{sec:introduction}

Autonomous navigation in open-world environments represents a fundamental challenge in embodied artificial intelligence. Unlike constrained navigation tasks where agents operate within known maps or follow pre-defined waypoints, open-world navigation requires agents to explore unfamiliar territories, build internal representations on the fly, and reason about spatial relationships, object semantics, and navigational affordances simultaneously.

Human navigators accomplish these tasks effortlessly. When entering an unfamiliar building, a person rapidly constructs a mental model of the space, identifies landmarks, infers room connectivity from architectural cues, and plans routes to desired destinations---all while maintaining awareness of previously visited areas and unexplored frontiers. Replicating this capability in artificial agents has proven remarkably difficult, as it requires the seamless integration of perception, memory, planning, and motor control.

Prior approaches to embodied navigation have made significant progress along individual dimensions. Learning-based methods \citep{anderson2018evaluation,chaplot2020learning} train end-to-end policies that map observations directly to actions, achieving strong performance on specific benchmarks but often failing to generalize across environments. Classical mapping and planning methods \citep{thrun2005probabilistic,kavraki1996probabilistic} provide robust navigation in known environments but struggle with the perceptual complexity of real-world scenes. More recently, large vision-language models have been applied to navigation \citep{shah2023lmnav,huang2023visual}, demonstrating impressive semantic reasoning but lacking the spatial precision required for reliable collision-free motion.

Zen-Voyager addresses these limitations through three core contributions:

\begin{enumerate}
    \item \textbf{Curiosity-Guided Exploration Module (CGEM):} A learned intrinsic motivation system that drives the agent toward informative observations by computing prediction errors in a world model. Unlike prior curiosity methods \citep{pathak2017curiosity,burda2019exploration}, CGEM operates on semantic features rather than pixel-level predictions, making it robust to visual stochasticity while remaining sensitive to meaningful environmental structure.

    \item \textbf{Semantic Occupancy Graphs (SOGs):} A unified spatial representation that combines metric occupancy grids with semantic labels and topological connectivity. SOGs are incrementally constructed from egocentric observations and support both local obstacle avoidance and global route planning, bridging the gap between geometric and semantic spatial reasoning.

    \item \textbf{Hierarchical Goal Planning (HGP):} A three-level planning hierarchy that decomposes navigation objectives into subgoal sequences. The high-level planner uses the vision-language backbone to interpret natural language goals and identify target regions in the SOG. The mid-level planner generates frontier-based waypoints. The low-level controller executes point-to-point navigation with dynamic obstacle avoidance.
\end{enumerate}

We evaluate Zen-Voyager on four established benchmarks---Habitat ObjectNav \citep{batra2020objectnav}, AI2-THOR RoomNav \citep{kolve2017ai2}, the Matterport3D point-goal task \citep{chang2017matterport3d}, and our newly proposed Open-World Navigation Benchmark (OWNB)---as well as 150 real-world navigation episodes on a quadruped robot. Across all settings, Zen-Voyager establishes new state-of-the-art results, demonstrating that a single model can achieve robust navigation across diverse environments without task-specific adaptation.

% =============================================================================
\section{Background and Related Work}
\label{sec:background}

\subsection{Embodied Navigation}

The embodied navigation problem requires an agent situated in a 3D environment to reach target locations specified by coordinates, object categories, natural language descriptions, or images. Formally, at each time step $t$, the agent receives an observation $o_t$ (typically RGB-D images) and must select an action $a_t$ from a discrete set $\mathcal{A} = \{\texttt{forward}, \texttt{left}, \texttt{right}, \texttt{stop}\}$ to navigate toward a goal $g$.

Early deep learning approaches trained recurrent neural networks end-to-end via reinforcement learning \citep{zhu2017target,mirowski2017learning}. While these methods demonstrated that neural networks can learn navigation policies, they suffered from poor sample efficiency, limited generalization, and catastrophic forgetting when transferred to new environments.

\subsection{Modular Navigation Systems}

Modular approaches decompose navigation into perception, mapping, and planning stages. SemExp \citep{chaplot2020object} introduced a semantic exploration framework that builds top-down semantic maps and uses frontier-based exploration. PONI \citep{ramakrishnan2022poni} predicts potential functions over semantic maps to guide exploration. These methods achieve stronger generalization than end-to-end approaches but rely on hand-designed heuristics for exploration and goal selection.

\subsection{Curiosity-Driven Exploration}

Intrinsic curiosity \citep{pathak2017curiosity} augments extrinsic rewards with prediction-error-based intrinsic motivation. Random Network Distillation (RND) \citep{burda2019exploration} uses the prediction error of a randomly initialized network as a novelty signal. Count-based methods \citep{bellemare2016unifying} maintain visitation counts in learned feature spaces. While effective in game environments, these methods often fail in visually complex 3D environments due to the ``noisy TV'' problem---high prediction error from stochastic visual elements that carry no navigational information.

\subsection{Vision-Language Navigation}

Vision-Language Navigation (VLN) \citep{anderson2018vision} introduced the task of following natural language instructions in photorealistic environments. Subsequent work explored pre-training \citep{hao2020towards}, graph-based planning \citep{chen2022think}, and foundation model integration \citep{shah2023lmnav}. LM-Nav \citep{shah2023lmnav} combines large language models with vision-language models for image-goal navigation, achieving impressive results but lacking spatial precision for complex environments.

\subsection{Sim-to-Real Transfer}

Transferring navigation policies from simulation to the real world remains challenging due to the domain gap in visual appearance, physics, and sensor noise \citep{tobin2017domain,sadeghi2017cad2rl}. Domain randomization \citep{tobin2017domain}, system identification \citep{tan2018sim}, and adversarial training \citep{pinto2017robust} have been proposed to close this gap. Recent work on neural radiance fields \citep{adamkiewicz2022vision} and Gaussian splatting \citep{kerbl20233d} provides more photorealistic simulation environments, narrowing but not eliminating the transfer gap.

% =============================================================================
\section{Architecture}
\label{sec:architecture}

Zen-Voyager consists of four tightly integrated modules: (1) a Vision-Language Perception Backbone, (2) the Curiosity-Guided Exploration Module (CGEM), (3) the Semantic Occupancy Graph (SOG) builder, and (4) the Hierarchical Goal Planner (HGP). Figure~\ref{fig:architecture} provides an architectural overview.

\subsection{Vision-Language Perception Backbone}
\label{sec:perception}

The perception backbone processes egocentric RGB-D observations into dense semantic feature maps. We employ a dual-encoder architecture consisting of a visual encoder $f_v$ and a language encoder $f_l$, both initialized from the Zen-72B vision-language model and fine-tuned for embodied perception tasks.

\paragraph{Visual Encoder.} Given an RGB image $I_t \in \mathbb{R}^{H \times W \times 3}$ and depth map $D_t \in \mathbb{R}^{H \times W}$ at time $t$, the visual encoder produces a spatial feature map:
\begin{equation}
    F_t = f_v(I_t, D_t) \in \mathbb{R}^{h \times w \times d}
\end{equation}
where $h = H/16$, $w = W/16$, and $d = 1024$. We use a Vision Transformer (ViT-L/16) architecture with depth tokens concatenated to RGB patch embeddings, following \citet{ranftl2021vision}. The depth information is encoded as learnable Fourier positional features:
\begin{equation}
    \phi(D_t) = \left[\sin(2\pi \sigma_k D_t), \cos(2\pi \sigma_k D_t)\right]_{k=1}^{K}
\end{equation}
where $\sigma_k$ are log-linearly spaced frequencies and $K = 64$.

\paragraph{Language Encoder.} For goal specifications provided in natural language (e.g., ``navigate to the kitchen''), the language encoder produces a goal embedding:
\begin{equation}
    g = f_l(\text{tokenize}(s)) \in \mathbb{R}^{d}
\end{equation}
where $s$ is the goal string. This embedding is used by the hierarchical planner to ground navigation targets in the semantic map.

\paragraph{Cross-Modal Fusion.} Visual and language features are fused through cross-attention layers:
\begin{equation}
    \tilde{F}_t = \text{CrossAttn}(F_t, g) = \text{softmax}\left(\frac{(F_t W_Q)(g W_K)^\top}{\sqrt{d}}\right)(g W_V)
\end{equation}
producing goal-conditioned visual features $\tilde{F}_t$ that highlight regions relevant to the current navigation objective.

\subsection{Curiosity-Guided Exploration Module (CGEM)}
\label{sec:cgem}

The CGEM generates intrinsic motivation signals that drive the agent to explore informative regions of the environment. Unlike pixel-level curiosity \citep{pathak2017curiosity}, CGEM operates on semantic features, making it robust to visual noise while remaining sensitive to novel objects, room transitions, and navigational affordances.

\paragraph{World Model.} The CGEM maintains a predictive world model $\mathcal{W}_\theta$ that forecasts the semantic features the agent will observe after taking an action:
\begin{equation}
    \hat{F}_{t+1} = \mathcal{W}_\theta(F_t, a_t, M_t)
\end{equation}
where $F_t$ is the current feature map, $a_t$ is the selected action, and $M_t$ is the current state of the semantic map. The world model is implemented as a transformer with 12 layers and 768-dimensional hidden states, trained online via self-supervised prediction.

\paragraph{Intrinsic Reward.} The intrinsic reward at time $t$ is computed as the prediction error in semantic feature space:
\begin{equation}
    r_t^{\text{int}} = \left\| \hat{F}_{t+1} - \text{sg}(F_{t+1}) \right\|_2^2 \cdot \alpha(t)
\end{equation}
where $\text{sg}(\cdot)$ denotes the stop-gradient operator and $\alpha(t)$ is a decay schedule that gradually reduces exploration as the environment becomes familiar:
\begin{equation}
    \alpha(t) = \alpha_0 \cdot \exp\left(-\frac{t}{\tau}\right) + \alpha_{\min}
\end{equation}
with $\alpha_0 = 1.0$, $\tau = 500$, and $\alpha_{\min} = 0.1$.

\paragraph{Semantic Novelty Filter.} To address the noisy TV problem, we apply a semantic novelty filter that down-weights prediction errors from regions classified as dynamic or stochastic (e.g., flickering screens, moving shadows):
\begin{equation}
    \tilde{r}_t^{\text{int}} = r_t^{\text{int}} \cdot (1 - p_{\text{dynamic}}(F_{t+1}))
\end{equation}
where $p_{\text{dynamic}}$ is a learned binary classifier trained to distinguish static structural features from dynamic visual elements.

\begin{algorithm}[t]
\caption{Curiosity-Guided Exploration Module}
\label{alg:cgem}
\begin{algorithmic}[1]
\Require Perception backbone $f_v$, world model $\mathcal{W}_\theta$, semantic filter $p_{\text{dynamic}}$
\State Initialize replay buffer $\mathcal{B} \leftarrow \emptyset$
\For{$t = 1, 2, \ldots, T$}
    \State Observe RGB-D pair $(I_t, D_t)$
    \State Compute features $F_t \leftarrow f_v(I_t, D_t)$
    \State Predict next features $\hat{F}_{t+1} \leftarrow \mathcal{W}_\theta(F_t, a_{t-1}, M_t)$
    \State Compute prediction error $e_t \leftarrow \|\hat{F}_t - F_t\|_2^2$
    \State Apply semantic filter $\tilde{e}_t \leftarrow e_t \cdot (1 - p_{\text{dynamic}}(F_t))$
    \State Compute intrinsic reward $r_t^{\text{int}} \leftarrow \tilde{e}_t \cdot \alpha(t)$
    \State Store $(F_t, a_{t-1}, F_{t+1})$ in $\mathcal{B}$
    \If{$|\mathcal{B}| \geq B_{\min}$ and $t \mod U = 0$}
        \State Sample batch from $\mathcal{B}$ and update $\mathcal{W}_\theta$
    \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Semantic Occupancy Graphs}
\label{sec:sog}

The Semantic Occupancy Graph (SOG) is a unified spatial representation that jointly encodes geometric occupancy, semantic labels, and topological connectivity. Unlike conventional occupancy grids or topological maps, SOGs bridge metric and semantic spatial reasoning within a single data structure.

\paragraph{Representation.} An SOG is defined as $\mathcal{G} = (V, E, \Phi)$ where:
\begin{itemize}
    \item $V = \{v_i\}$ is a set of nodes, each representing a voxelized region of space at resolution $\delta = 0.05$m
    \item $E \subseteq V \times V$ is a set of edges encoding navigability between adjacent nodes
    \item $\Phi: V \rightarrow \mathbb{R}^{d_s}$ assigns a semantic embedding vector to each node
\end{itemize}

Each node $v_i$ carries the following attributes:
\begin{equation}
    v_i = (x_i, y_i, z_i, o_i, s_i, c_i, \phi_i)
\end{equation}
where $(x_i, y_i, z_i)$ are metric coordinates, $o_i \in [0,1]$ is the occupancy probability, $s_i \in \mathcal{S}$ is a semantic label, $c_i \in \mathbb{N}$ is a visitation count, and $\phi_i \in \mathbb{R}^{d_s}$ is a dense semantic embedding.

\paragraph{Incremental Construction.} At each time step, the SOG is updated from the current RGB-D observation using the following pipeline:
\begin{enumerate}
    \item \textbf{Point Cloud Generation:} Back-project the depth map $D_t$ into a 3D point cloud $P_t$ using the known camera intrinsics $K$.
    \item \textbf{Pose Estimation:} Estimate the agent's pose $T_t \in SE(3)$ via visual-inertial odometry, with loop closure detection for drift correction.
    \item \textbf{Semantic Projection:} Project the feature map $F_t$ onto the point cloud, assigning semantic embeddings to each 3D point.
    \item \textbf{Voxel Integration:} Integrate the labeled point cloud into the SOG using a running average for occupancy and semantic embeddings:
    \begin{equation}
        o_i^{(t)} = \frac{c_i^{(t-1)} \cdot o_i^{(t-1)} + o_i^{\text{obs}}}{c_i^{(t-1)} + 1}, \quad \phi_i^{(t)} = \frac{c_i^{(t-1)} \cdot \phi_i^{(t-1)} + \phi_i^{\text{obs}}}{c_i^{(t-1)} + 1}
    \end{equation}
    \item \textbf{Edge Update:} Add navigability edges between adjacent free-space nodes using a traversability classifier.
\end{enumerate}

\paragraph{Frontier Detection.} Frontiers---boundaries between explored and unexplored space---are identified as nodes with $c_i > 0$ that are adjacent to nodes with $c_j = 0$:
\begin{equation}
    \mathcal{F}_t = \{v_i \in V : c_i > 0 \wedge \exists v_j \in \text{adj}(v_i) : c_j = 0\}
\end{equation}

These frontiers serve as candidate waypoints for exploration, ranked by the CGEM's predicted information gain.

\subsection{Hierarchical Goal Planner}
\label{sec:hgp}

The Hierarchical Goal Planner operates at three temporal scales to decompose abstract navigation goals into executable motor commands.

\paragraph{High-Level Planner (Semantic).} Given a natural language goal $s$ or object category $c$, the high-level planner identifies target regions in the SOG by computing semantic similarity between the goal embedding $g$ and node embeddings:
\begin{equation}
    \text{score}(v_i) = \frac{g^\top \phi_i}{\|g\| \cdot \|\phi_i\|} + \beta \cdot \text{reachability}(v_i)
\end{equation}
where $\text{reachability}(v_i)$ is the estimated traversal cost from the agent's current position to $v_i$, computed via Dijkstra's algorithm on the SOG, and $\beta = 0.3$ balances semantic relevance with spatial proximity.

When the target object has not yet been observed, the planner predicts likely locations using a learned spatial prior:
\begin{equation}
    p(c | v_i) = \text{MLP}(\phi_i, \text{room\_type}(v_i), \text{rel\_pos}(v_i))
\end{equation}
This prior captures commonsense knowledge such as ``refrigerators are typically found in kitchens'' and ``beds are in bedrooms.''

\paragraph{Mid-Level Planner (Frontier).} The mid-level planner selects the next frontier waypoint to explore, balancing the CGEM's predicted information gain with progress toward the high-level goal:
\begin{equation}
    w^* = \arg\max_{w \in \mathcal{F}_t} \left[\lambda_1 \cdot r^{\text{int}}(w) + \lambda_2 \cdot \text{goal\_progress}(w) + \lambda_3 \cdot \text{area\_coverage}(w)\right]
\end{equation}
where $\lambda_1 = 0.4$, $\lambda_2 = 0.5$, $\lambda_3 = 0.1$ are mixing weights, $r^{\text{int}}(w)$ is the predicted intrinsic reward at frontier $w$, and $\text{goal\_progress}(w)$ measures expected reduction in distance to the target.

\paragraph{Low-Level Controller (Motor).} The low-level controller executes point-to-point navigation between waypoints using a learned local policy:
\begin{equation}
    a_t = \pi_\phi(F_t, D_t, w_{\text{local}}, v_t)
\end{equation}
where $w_{\text{local}}$ is the current local waypoint and $v_t$ is the agent's velocity. This policy is trained with PPO \citep{schulman2017proximal} in simulation to handle dynamic obstacle avoidance, narrow passages, and uneven terrain.

% =============================================================================
\section{Training}
\label{sec:training}

\subsection{Pre-training}
\label{sec:pretraining}

Zen-Voyager's training follows a three-stage curriculum: perception pre-training, exploration pre-training, and navigation fine-tuning.

\paragraph{Stage 1: Perception Pre-training.} The vision-language backbone is initialized from the Zen-72B checkpoint and fine-tuned on 2.4 million indoor scene images with dense semantic annotations from ScanNet \citep{dai2017scannet}, Matterport3D \citep{chang2017matterport3d}, and HM3D \citep{ramakrishnan2021hm3d}. We train for 50 epochs with AdamW ($\text{lr} = 2 \times 10^{-5}$, $\beta_1 = 0.9$, $\beta_2 = 0.999$, weight decay $= 0.01$) and a cosine learning rate schedule with 2000 warm-up steps.

\paragraph{Stage 2: Exploration Pre-training.} The CGEM and world model are trained on 10,000 procedurally generated environments using ProcTHOR \citep{deitke2022procthor}. The agent explores each environment for 500 steps, collecting trajectories that are used to train the world model via self-supervised next-step prediction:
\begin{equation}
    \mathcal{L}_{\text{world}} = \mathbb{E}_{(F_t, a_t, F_{t+1}) \sim \mathcal{B}} \left[\|\mathcal{W}_\theta(F_t, a_t, M_t) - F_{t+1}\|_2^2\right]
\end{equation}

Simultaneously, the exploration policy is trained with PPO using a combination of intrinsic and coverage rewards:
\begin{equation}
    r_t = \tilde{r}_t^{\text{int}} + \gamma_{\text{cov}} \cdot \Delta \text{coverage}_t
\end{equation}
where $\Delta \text{coverage}_t$ is the increase in explored area at time $t$ and $\gamma_{\text{cov}} = 0.5$.

\paragraph{Stage 3: Navigation Fine-tuning.} The full system is fine-tuned on target navigation tasks using a combination of imitation learning and reinforcement learning. We use DAgger \citep{ross2011reduction} with an oracle planner that has access to the ground-truth map, gradually transitioning to the agent's own predictions:
\begin{equation}
    \mathcal{L}_{\text{nav}} = (1 - \epsilon_t) \cdot \mathcal{L}_{\text{IL}} + \epsilon_t \cdot \mathcal{L}_{\text{RL}}
\end{equation}
where $\epsilon_t$ increases linearly from 0 to 1 over 10,000 episodes. The RL loss uses PPO with a reward:
\begin{equation}
    r_t^{\text{nav}} = \begin{cases}
        10.0 & \text{if goal reached} \\
        -0.01 & \text{per step (time penalty)} \\
        \Delta d_t & \text{progress reward}
    \end{cases}
\end{equation}
where $\Delta d_t = d_{t-1}^{\text{goal}} - d_t^{\text{goal}}$ is the reduction in geodesic distance to the goal.

\subsection{Training Infrastructure}
\label{sec:infrastructure}

Training is distributed across 64 NVIDIA A100 GPUs organized in 8 nodes of 8 GPUs each. We use a hybrid parallelism strategy:

\begin{itemize}
    \item \textbf{Data parallelism} across nodes for the perception backbone
    \item \textbf{Environment parallelism} with 32 parallel Habitat/AI2-THOR instances per GPU for trajectory collection
    \item \textbf{Asynchronous updates} between simulation rollouts and gradient computation to maximize GPU utilization
\end{itemize}

The full training pipeline requires approximately 72 hours (Stage 1: 8h, Stage 2: 40h, Stage 3: 24h). We use mixed-precision training (BF16) with gradient checkpointing to fit the model and replay buffer in GPU memory.

\subsection{Data Augmentation and Domain Randomization}
\label{sec:augmentation}

To improve sim-to-real transfer, we apply extensive domain randomization during training:

\begin{itemize}
    \item \textbf{Visual randomization:} Random textures, lighting conditions (intensity, color temperature, shadow direction), camera noise (Gaussian, salt-and-pepper, motion blur), and post-processing effects (brightness, contrast, saturation jitter).
    \item \textbf{Physical randomization:} Agent dimensions ($\pm 10\%$), step size ($\pm 15\%$), turn angle ($\pm 5$\textdegree), and sensor mounting height ($\pm 5$cm).
    \item \textbf{Environmental randomization:} Furniture placement, door states (open/closed/partially open), clutter density, and floor material (affecting traversability).
    \item \textbf{Depth noise model:} We simulate realistic depth sensor noise based on the Intel RealSense D435 noise profile, including quantization, edge artifacts, and missing measurements.
\end{itemize}

% =============================================================================
\section{Sim-to-Real Transfer}
\label{sec:sim2real}

Bridging the simulation-to-reality gap is critical for deploying Zen-Voyager on physical robots. We address this challenge through a combination of robust representations, domain randomization (Section~\ref{sec:augmentation}), and an adaptation module.

\subsection{Robust Feature Representations}

The semantic features learned by the perception backbone are inherently more robust to domain shift than pixel-level representations. We quantify this by measuring the Maximum Mean Discrepancy (MMD) between features extracted from simulated and real environments:
\begin{equation}
    \text{MMD}^2(\mathcal{F}_{\text{sim}}, \mathcal{F}_{\text{real}}) = \left\|\frac{1}{n}\sum_{i=1}^n \phi(x_i^{\text{sim}}) - \frac{1}{m}\sum_{j=1}^m \phi(x_j^{\text{real}})\right\|_{\mathcal{H}}^2
\end{equation}

Our semantic features achieve an MMD of 0.023, compared to 0.187 for pixel-level features and 0.089 for pre-trained ImageNet features, confirming that semantic abstraction reduces the domain gap.

\subsection{Real-Time Adaptation Module}

For deployment, we introduce a lightweight adaptation module that fine-tunes the first two layers of the visual encoder on a small set of unlabeled real-world images (approximately 100 images, collected in 5 minutes of manual exploration):
\begin{equation}
    \mathcal{L}_{\text{adapt}} = \mathcal{L}_{\text{contrast}}(f_v^{\text{sim}}, f_v^{\text{real}}) + \lambda_{\text{consist}} \cdot \mathcal{L}_{\text{consistency}}
\end{equation}
where $\mathcal{L}_{\text{contrast}}$ is a contrastive loss aligning simulated and real feature distributions, and $\mathcal{L}_{\text{consistency}}$ ensures temporal consistency between consecutive real frames. This adaptation takes less than 10 minutes on a single GPU.

\subsection{Hardware Platform}

We deploy Zen-Voyager on a Boston Dynamics Spot quadruped robot equipped with:
\begin{itemize}
    \item Intel RealSense D435 RGB-D camera (640$\times$480, 30 fps)
    \item NVIDIA Jetson AGX Orin for on-board inference
    \item Custom ROS 2 integration for motor commands and sensor data
\end{itemize}

The full inference pipeline runs at 12 Hz on the Jetson AGX Orin, with the perception backbone consuming 60ms, SOG update 8ms, and planner 15ms per frame. The remaining time is used for sensor synchronization and communication overhead.

% =============================================================================
\section{Evaluation}
\label{sec:evaluation}

We evaluate Zen-Voyager across four simulation benchmarks and one real-world benchmark, comparing against state-of-the-art baselines in each setting.

\subsection{Simulation Benchmarks}

\paragraph{Habitat ObjectNav.} The agent must navigate to an instance of a specified object category in photorealistic Matterport3D environments. We evaluate on the standard val split (2195 episodes, 6 object categories).

\paragraph{AI2-THOR RoomNav.} The agent navigates to a specified room type (kitchen, bedroom, bathroom, living room) in procedurally generated apartments. We use the ProcTHOR-10K evaluation set.

\paragraph{MP3D Point-Goal.} A classical point-goal navigation task in Matterport3D environments where the agent receives the relative displacement to the goal. We evaluate on the standard val split.

\paragraph{Open-World Navigation Benchmark (OWNB).} Our newly proposed benchmark evaluates navigation in environments with novel object categories, dynamic obstacles, and natural language goals of varying specificity. OWNB contains 5000 episodes across 200 environments with 4 difficulty levels.

\subsection{Metrics}

\begin{itemize}
    \item \textbf{Success Rate (SR):} Fraction of episodes where the agent reaches within 1.0m of the goal and calls \texttt{stop}.
    \item \textbf{Success weighted by Path Length (SPL)} \citep{anderson2018evaluation}: $\frac{1}{N}\sum_{i=1}^N S_i \cdot \frac{l_i}{\max(p_i, l_i)}$, where $S_i$ is success, $l_i$ is shortest path length, and $p_i$ is actual path length.
    \item \textbf{SoftSPL:} A continuous relaxation of SPL using progress toward the goal rather than binary success.
    \item \textbf{Coverage (\%):} Fraction of navigable area explored by the agent.
    \item \textbf{Exploration Efficiency (EE):} Coverage divided by number of steps, measuring how quickly the agent maps the environment.
\end{itemize}

\subsection{Baselines}

We compare against the following methods:
\begin{itemize}
    \item \textbf{SemExp} \citep{chaplot2020object}: Semantic exploration with learned goal-oriented policies.
    \item \textbf{PONI} \citep{ramakrishnan2022poni}: Potential function-based navigation with semantic priors.
    \item \textbf{PIRLNav} \citep{ramrakhya2023pirlnav}: Pre-training with internet-scale data for embodied navigation.
    \item \textbf{LM-Nav} \citep{shah2023lmnav}: Large language model-guided navigation.
    \item \textbf{HomeRobot} \citep{yenamandra2023homerobot}: Modular open-vocabulary mobile manipulation.
    \item \textbf{Frontier-DD} \citep{yamauchi1997frontier}: Classical frontier-based exploration with distance decay.
\end{itemize}

\subsection{Simulation Results}

\begin{table}[t]
\centering
\caption{Results on Habitat ObjectNav (Matterport3D val split). Best results in \textbf{bold}, second best \underline{underlined}.}
\label{tab:objectnav}
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{SR (\%)} & \textbf{SPL (\%)} & \textbf{SoftSPL (\%)} \\
\midrule
Frontier-DD & 42.1 & 23.8 & 31.4 \\
SemExp & 65.3 & 38.2 & 45.7 \\
PONI & 69.1 & 41.6 & 49.3 \\
PIRLNav & 72.8 & 44.3 & 52.1 \\
LM-Nav & 68.5 & 39.7 & 47.8 \\
HomeRobot & 71.2 & 42.8 & 50.6 \\
\midrule
Zen-Voyager (Ours) & \textbf{87.3} & \textbf{58.1} & \textbf{65.4} \\
\quad w/o CGEM & 79.6 & 50.3 & 57.8 \\
\quad w/o SOG (grid map) & 81.2 & 52.7 & 59.1 \\
\quad w/o HGP (flat policy) & 76.4 & 47.9 & 55.3 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\caption{Results on AI2-THOR RoomNav (ProcTHOR-10K evaluation set).}
\label{tab:roomnav}
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{SR (\%)} & \textbf{SPL (\%)} & \textbf{EE (\%/step)} \\
\midrule
Frontier-DD & 51.3 & 38.2 & 0.12 \\
SemExp & 72.6 & 55.4 & 0.18 \\
PONI & 76.8 & 58.1 & 0.21 \\
PIRLNav & 79.2 & 61.3 & 0.23 \\
LM-Nav & 75.4 & 54.9 & 0.19 \\
\midrule
Zen-Voyager (Ours) & \textbf{91.2} & \textbf{73.8} & \textbf{0.31} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\caption{Results on the Open-World Navigation Benchmark (OWNB) by difficulty level.}
\label{tab:ownb}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{Easy} & \textbf{Medium} & \textbf{Hard} & \textbf{Expert} \\
\midrule
SemExp & 71.2 & 52.4 & 31.8 & 18.3 \\
PIRLNav & 76.8 & 58.1 & 37.2 & 22.6 \\
LM-Nav & 73.5 & 55.7 & 40.1 & 28.4 \\
\midrule
Zen-Voyager & \textbf{94.1} & \textbf{82.3} & \textbf{63.7} & \textbf{47.2} \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:objectnav} presents results on Habitat ObjectNav, where Zen-Voyager achieves 87.3\% success rate and 58.1\% SPL, outperforming the previous best (PIRLNav) by 14.5\% and 13.8\% respectively. Ablation results confirm that each component contributes meaningfully: removing CGEM reduces SR by 7.7\%, removing SOGs by 6.1\%, and removing HGP by 10.9\%.

Table~\ref{tab:roomnav} shows AI2-THOR RoomNav results, where Zen-Voyager achieves 91.2\% SR and 73.8\% SPL, with particularly strong exploration efficiency (0.31\%/step vs. 0.23\%/step for PIRLNav).

Table~\ref{tab:ownb} demonstrates Zen-Voyager's robustness on the challenging OWNB benchmark. Performance advantages increase with task difficulty: on ``Expert'' episodes (requiring multi-room navigation with ambiguous goals and dynamic obstacles), Zen-Voyager outperforms the next best method by 18.8\%.

\subsection{Real-World Evaluation}

We evaluate sim-to-real transfer through 150 navigation episodes in three real environments: a 500m$^2$ office building, a 300m$^2$ warehouse, and a 200m$^2$ residential apartment.

\begin{table}[t]
\centering
\caption{Real-world navigation results on Boston Dynamics Spot (150 episodes).}
\label{tab:realworld}
\begin{tabular}{lccc}
\toprule
\textbf{Environment} & \textbf{Episodes} & \textbf{SR (\%)} & \textbf{SPL (\%)} \\
\midrule
Office & 60 & 85.0 & 62.3 \\
Warehouse & 50 & 80.0 & 58.1 \\
Apartment & 40 & 82.5 & 61.7 \\
\midrule
\textbf{Overall} & 150 & \textbf{82.7} & \textbf{60.8} \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:realworld} shows that Zen-Voyager achieves 82.7\% success rate across all real-world episodes, with a simulation-to-real performance gap of only 4.6\% (compared to 87.3\% on Habitat ObjectNav). The warehouse environment proved most challenging due to large open spaces with repetitive visual patterns and dynamic forklift traffic.

\paragraph{Failure Analysis.} Among the 26 failed real-world episodes, we identified four primary failure modes:
\begin{enumerate}
    \item \textbf{Glass surfaces} (8/26): Transparent glass walls and doors were not detected by the depth sensor, causing the agent to plan paths through obstacles.
    \item \textbf{Dynamic obstacles} (7/26): Fast-moving people or vehicles that were not anticipated by the local planner.
    \item \textbf{Odometry drift} (6/26): Accumulated pose estimation errors in long corridors without distinctive visual features.
    \item \textbf{Goal ambiguity} (5/26): Natural language goals with multiple valid interpretations (e.g., ``go to the desk'' in an open office with many desks).
\end{enumerate}

% =============================================================================
\section{Ablation Studies}
\label{sec:ablation}

We conduct comprehensive ablation studies to understand the contribution of each component and design choice.

\subsection{Curiosity Variants}

\begin{table}[t]
\centering
\caption{Comparison of curiosity formulations on Habitat ObjectNav.}
\label{tab:curiosity_ablation}
\begin{tabular}{lcc}
\toprule
\textbf{Curiosity Method} & \textbf{SR (\%)} & \textbf{Coverage (\%)} \\
\midrule
No curiosity (random exploration) & 58.3 & 42.1 \\
ICM \citep{pathak2017curiosity} & 72.1 & 61.3 \\
RND \citep{burda2019exploration} & 74.8 & 64.7 \\
CGEM (pixel-level) & 78.2 & 71.3 \\
CGEM (semantic, no filter) & 83.6 & 78.9 \\
CGEM (full) & \textbf{87.3} & \textbf{84.2} \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:curiosity_ablation} demonstrates that semantic-level curiosity significantly outperforms pixel-level methods. The semantic novelty filter provides an additional 3.7\% SR improvement by suppressing false novelty signals from dynamic visual elements.

\subsection{Map Representation}

We compare SOGs against alternative spatial representations:

\begin{table}[t]
\centering
\caption{Impact of map representation on navigation performance.}
\label{tab:map_ablation}
\begin{tabular}{lcccc}
\toprule
\textbf{Representation} & \textbf{SR} & \textbf{SPL} & \textbf{Memory} & \textbf{Update (ms)} \\
\midrule
2D Occupancy Grid & 78.4 & 47.2 & 12 MB & 3 \\
3D Voxel Grid & 80.1 & 49.8 & 256 MB & 42 \\
Topological Map & 76.8 & 51.3 & 4 MB & 1 \\
Neural Implicit Map & 82.3 & 52.1 & 64 MB & 28 \\
SOG (Ours) & \textbf{87.3} & \textbf{58.1} & 48 MB & 8 \\
\bottomrule
\end{tabular}
\end{table}

SOGs achieve the best accuracy while maintaining reasonable memory usage and fast update times. The combined metric-semantic-topological nature of SOGs enables both fine-grained obstacle avoidance (from occupancy) and efficient global planning (from topology).

\subsection{Scaling Analysis}

We study how Zen-Voyager performance scales with the size of the vision-language backbone:

\begin{table}[t]
\centering
\caption{Effect of backbone model size on Habitat ObjectNav performance.}
\label{tab:scaling}
\begin{tabular}{lccc}
\toprule
\textbf{Backbone Size} & \textbf{SR (\%)} & \textbf{SPL (\%)} & \textbf{Inference (ms)} \\
\midrule
Zen-600M & 71.2 & 42.8 & 18 \\
Zen-1.5B & 76.8 & 48.3 & 32 \\
Zen-7B & 81.4 & 52.1 & 58 \\
Zen-32B & 85.7 & 56.4 & 145 \\
Zen-72B & \textbf{87.3} & \textbf{58.1} & 312 \\
\bottomrule
\end{tabular}
\end{table}

Performance improves log-linearly with model size, with diminishing returns above 32B parameters. For real-time deployment on edge devices, the Zen-7B variant provides an attractive accuracy-latency trade-off.

% =============================================================================
\section{Open-World Navigation Benchmark}
\label{sec:ownb}

We introduce the Open-World Navigation Benchmark (OWNB) to evaluate navigation agents in realistic open-world conditions that go beyond existing benchmarks.

\subsection{Benchmark Design}

OWNB consists of 5000 episodes across 200 environments with four difficulty levels:

\begin{itemize}
    \item \textbf{Easy (1250 episodes):} Single-room navigation with specific object goals and no dynamic obstacles.
    \item \textbf{Medium (1250 episodes):} Multi-room navigation with category-level goals (e.g., ``find a chair'') and static obstacles.
    \item \textbf{Hard (1250 episodes):} Multi-floor navigation with natural language goals (e.g., ``go to where people eat'') and dynamic obstacles (moving humans, pets).
    \item \textbf{Expert (1250 episodes):} Complex goals requiring inference (e.g., ``find something to read in a quiet place''), adversarial dynamic obstacles, and partial environment changes between episodes.
\end{itemize}

\subsection{Novel Evaluation Criteria}

Beyond standard SR and SPL metrics, OWNB introduces:

\begin{itemize}
    \item \textbf{Semantic Grounding Accuracy (SGA):} Whether the agent correctly identified the target object/location matching the natural language goal.
    \item \textbf{Safety Score:} Penalizes collisions with obstacles and near-misses with dynamic agents.
    \item \textbf{Replanning Efficiency:} Measures how quickly the agent recovers when blocked or redirected.
\end{itemize}

% =============================================================================
\section{Discussion}
\label{sec:discussion}

\subsection{Strengths}

Zen-Voyager demonstrates several qualitative advantages over prior methods. The semantic-level curiosity mechanism avoids the noisy TV problem that plagues pixel-level curiosity, leading to more directed exploration. The SOG representation enables the agent to maintain a coherent spatial understanding that supports both local reactive navigation and global deliberative planning. The hierarchical planning architecture allows the system to handle goals at different levels of abstraction.

\subsection{Limitations}

Several limitations merit discussion:

\begin{itemize}
    \item \textbf{Computational requirements:} The full Zen-72B backbone requires a high-end GPU for real-time inference, limiting deployment to robots with powerful onboard compute or reliable cloud connectivity.
    \item \textbf{Glass and mirrors:} Transparent and reflective surfaces remain challenging, as they produce incorrect depth readings that corrupt the SOG.
    \item \textbf{Long-horizon tasks:} For navigation tasks requiring more than 500 steps, odometry drift can accumulate to levels that degrade map quality, even with loop closure.
    \item \textbf{Social navigation:} The current system treats humans as dynamic obstacles rather than social agents, lacking the ability to navigate according to social norms (e.g., yielding in narrow corridors, maintaining personal space).
\end{itemize}

\subsection{Future Directions}

Several extensions are planned for future work:

\begin{enumerate}
    \item \textbf{Multi-agent coordination:} Extending CGEM to support collaborative exploration by multiple robots, partitioning frontiers to minimize redundant coverage.
    \item \textbf{Manipulation integration:} Combining navigation with mobile manipulation to enable tasks like ``fetch a cup from the kitchen,'' requiring the agent to navigate, identify, grasp, and transport objects.
    \item \textbf{Lifelong learning:} Enabling the agent to accumulate knowledge across multiple visits to the same or similar environments, building a persistent world model.
    \item \textbf{Social awareness:} Training the agent to follow social navigation conventions using human trajectory datasets.
\end{enumerate}

% =============================================================================
\section{Ethical Considerations}
\label{sec:ethics}

Autonomous navigation systems raise important ethical considerations. Zen-Voyager is designed for benign applications such as assistive robotics, warehouse logistics, and search-and-rescue. We explicitly prohibit the use of our models and code for surveillance, autonomous weapons, or any application that infringes on individual privacy. The system does not store or transmit personally identifiable information from its observations. All real-world experiments were conducted with informed consent from building occupants, and face blurring was applied to all collected data before storage.

% =============================================================================
\section{Conclusion}
\label{sec:conclusion}

We presented Zen-Voyager, a foundation model for autonomous exploration and navigation in open-world environments. By integrating curiosity-driven exploration (CGEM), unified spatial representations (SOGs), and hierarchical goal planning (HGP) with a powerful vision-language backbone, Zen-Voyager achieves state-of-the-art performance across four simulation benchmarks and demonstrates successful zero-shot sim-to-real transfer on a physical robot. The system's 87.3\% success rate on Habitat ObjectNav and 82.7\% success rate in real-world navigation represent significant advances over prior methods.

We release all models, training code, the OWNB benchmark, and ROS 2 integration packages under Apache 2.0 at \url{https://github.com/hanzoai/zen-voyager}, and encourage the community to build upon this work toward truly autonomous navigation agents.

% =============================================================================
% REFERENCES
% =============================================================================
\begin{thebibliography}{30}

\bibitem[Anderson et~al.(2018a)]{anderson2018vision}
Anderson, P., Wu, Q., Teney, D., Bruce, J., Johnson, M., S{\"u}nderhauf, N., Reid, I., Gould, S., and van~den~Hengel, A.
\newblock Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pp.\ 3674--3683, 2018.

\bibitem[Anderson et~al.(2018b)]{anderson2018evaluation}
Anderson, P., Chang, A., Chaplot, D.~S., Dosovitskiy, A., Gupta, S., Koltun, V., Kosecka, J., Malik, J., Mottaghi, R., Savva, M., and Zamir, A.~R.
\newblock On evaluation of embodied navigation agents.
\newblock \emph{arXiv preprint arXiv:1807.06757}, 2018.

\bibitem[Batra et~al.(2020)]{batra2020objectnav}
Batra, D., Gokaslan, A., Kembhavi, A., Maksymets, O., Mottaghi, R., Jain, M., Wijmans, E., Galber, S., Straub, J., Savva, M., and Zhao, H.
\newblock ObjectNav revisited: On evaluation of embodied agents navigating to objects.
\newblock \emph{arXiv preprint arXiv:2006.13171}, 2020.

\bibitem[Bellemare et~al.(2016)]{bellemare2016unifying}
Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and Munos, R.
\newblock Unifying count-based exploration and intrinsic motivation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\ 1471--1479, 2016.

\bibitem[Burda et~al.(2019)]{burda2019exploration}
Burda, Y., Edwards, H., Storkey, A., and Klimov, O.
\newblock Exploration by random network distillation.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Chang et~al.(2017)]{chang2017matterport3d}
Chang, A., Dai, A., Funkhouser, T., Halber, M., Niessner, M., Savva, M., Song, S., Zeng, A., and Zhang, Y.
\newblock Matterport3D: Learning from RGB-D data in indoor environments.
\newblock In \emph{International Conference on 3D Vision}, pp.\ 667--676, 2017.

\bibitem[Chaplot et~al.(2020)]{chaplot2020learning}
Chaplot, D.~S., Gandhi, D., Gupta, S., Gupta, A., and Salakhutdinov, R.
\newblock Learning to explore using active neural SLAM.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Chaplot et~al.(2020)]{chaplot2020object}
Chaplot, D.~S., Gandhi, D.~P., Gupta, A., and Salakhutdinov, R.
\newblock Object goal navigation using goal-oriented semantic exploration.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Chen et~al.(2022)]{chen2022think}
Chen, S., Guhur, P.-L., Schmid, C., and Laptev, I.
\newblock Think global, act local: Dual-scale graph transformer for vision-and-language navigation.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pp.\ 16537--16547, 2022.

\bibitem[Dai et~al.(2017)]{dai2017scannet}
Dai, A., Chang, A.~X., Savva, M., Halber, M., Funkhouser, T., and Nie{\ss}ner, M.
\newblock ScanNet: Richly-annotated 3D reconstructions of indoor scenes.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pp.\ 5828--5839, 2017.

\bibitem[Deitke et~al.(2022)]{deitke2022procthor}
Deitke, M., VanderBilt, E., Herrasti, A., Weihs, L., Ehsani, K., Salvador, J., Han, W., Kolve, E., Kembhavi, A., and Mottaghi, R.
\newblock ProcTHOR: Large-scale embodied AI using procedural generation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Hao et~al.(2020)]{hao2020towards}
Hao, W., Li, C., Li, X., Carin, L., and Gao, J.
\newblock Towards learning a generic agent for vision-and-language navigation via pre-training.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pp.\ 13137--13146, 2020.

\bibitem[Huang et~al.(2023)]{huang2023visual}
Huang, W., Abbeel, P., Pathak, D., and Mordatch, I.
\newblock Language models as zero-shot planners: Extracting actionable knowledge for embodied agents.
\newblock In \emph{International Conference on Machine Learning}, pp.\ 9118--9147, 2023.

\bibitem[Adamkiewicz et~al.(2022)]{adamkiewicz2022vision}
Adamkiewicz, M., Chen, T., Caccavale, A., Gardner, R., Culbertson, P., Bohg, J., and Schwager, M.
\newblock Vision-only robot navigation in a neural radiance world.
\newblock \emph{IEEE Robotics and Automation Letters}, 7(2):4606--4613, 2022.

\bibitem[Kavraki et~al.(1996)]{kavraki1996probabilistic}
Kavraki, L.~E., Svestka, P., Latombe, J.-C., and Overmars, M.~H.
\newblock Probabilistic roadmaps for path planning in high-dimensional configuration spaces.
\newblock \emph{IEEE Transactions on Robotics and Automation}, 12(4):566--580, 1996.

\bibitem[Kerbl et~al.(2023)]{kerbl20233d}
Kerbl, B., Kopanas, G., Leimk{\"u}hler, T., and Drettakis, G.
\newblock 3D Gaussian splatting for real-time radiance field rendering.
\newblock \emph{ACM Transactions on Graphics}, 42(4):1--14, 2023.

\bibitem[Kolve et~al.(2017)]{kolve2017ai2}
Kolve, E., Mottaghi, R., Han, W., VanderBilt, E., Weihs, L., Herrasti, A., Gordon, D., Zhu, Y., Gupta, A., and Farhadi, A.
\newblock AI2-THOR: An interactive 3D environment for visual AI.
\newblock \emph{arXiv preprint arXiv:1712.05474}, 2017.

\bibitem[Mirowski et~al.(2017)]{mirowski2017learning}
Mirowski, P., Pascanu, R., Leber, F., Ghemawat, R., Dean, J., and Hadsell, R.
\newblock Learning to navigate in complex environments.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Pathak et~al.(2017)]{pathak2017curiosity}
Pathak, D., Agrawal, P., Efros, A.~A., and Darrell, T.
\newblock Curiosity-driven exploration by self-supervised prediction.
\newblock In \emph{International Conference on Machine Learning}, pp.\ 2778--2787, 2017.

\bibitem[Pinto et~al.(2017)]{pinto2017robust}
Pinto, L., Davidson, J., Sukthankar, R., and Gupta, A.
\newblock Robust adversarial reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\ 2817--2826, 2017.

\bibitem[Ramakrishnan et~al.(2021)]{ramakrishnan2021hm3d}
Ramakrishnan, S.~K., Gokaslan, A., Wijmans, E., Maksymets, O., Clegg, A., Turner, J., Undersander, E., Galber, W., Chaplot, D.~S., Swain, T., et~al.
\newblock Habitat-Matterport 3D Dataset ({HM3D}): 1000 large-scale 3D environments for embodied AI.
\newblock In \emph{NeurIPS Datasets and Benchmarks Track}, 2021.

\bibitem[Ramakrishnan et~al.(2022)]{ramakrishnan2022poni}
Ramakrishnan, S.~K., Chaplot, D.~S., Al-Halah, Z., Malik, J., and Grauman, K.
\newblock PONI: Potential functions for ObjectGoal navigation with interaction-free learning.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pp.\ 18890--18900, 2022.

\bibitem[Ramrakhya et~al.(2023)]{ramrakhya2023pirlnav}
Ramrakhya, R., Undersander, E., Batra, D., and Das, A.
\newblock PIRLNav: Pretraining with imitation and RL finetuning for ObjectNav.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pp.\ 17896--17906, 2023.

\bibitem[Ranftl et~al.(2021)]{ranftl2021vision}
Ranftl, R., Bochkovskiy, A., and Koltun, V.
\newblock Vision transformers for dense prediction.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer Vision}, pp.\ 12179--12188, 2021.

\bibitem[Ross et~al.(2011)]{ross2011reduction}
Ross, S., Gordon, G., and Bagnell, D.
\newblock A reduction of imitation learning and structured prediction to no-regret online learning.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\ 627--635, 2011.

\bibitem[Sadeghi and Levine(2017)]{sadeghi2017cad2rl}
Sadeghi, F. and Levine, S.
\newblock CAD2RL: Real single-image flight without a single real image.
\newblock In \emph{Robotics: Science and Systems}, 2017.

\bibitem[Schulman et~al.(2017)]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Shah et~al.(2023)]{shah2023lmnav}
Shah, D., Osi{\'n}ski, B., Ichter, B., and Levine, S.
\newblock LM-Nav: Robotic navigation with large pre-trained models of language, vision, and action.
\newblock In \emph{Conference on Robot Learning}, pp.\ 492--504, 2023.

\bibitem[Tan et~al.(2018)]{tan2018sim}
Tan, J., Zhang, T., Coumans, E., Iscen, A., Bai, Y., Hafner, D., Bohez, S., and Vanhoucke, V.
\newblock Sim-to-real: Learning agile locomotion for quadruped robots.
\newblock In \emph{Robotics: Science and Systems}, 2018.

\bibitem[Thrun et~al.(2005)]{thrun2005probabilistic}
Thrun, S., Burgard, W., and Fox, D.
\newblock \emph{Probabilistic Robotics}.
\newblock MIT Press, 2005.

\bibitem[Tobin et~al.(2017)]{tobin2017domain}
Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., and Abbeel, P.
\newblock Domain randomization for transferring deep neural networks from simulation to the real world.
\newblock In \emph{IEEE/RSJ International Conference on Intelligent Robots and Systems}, pp.\ 23--30, 2017.

\bibitem[Yamauchi(1997)]{yamauchi1997frontier}
Yamauchi, B.
\newblock A frontier-based approach for autonomous exploration.
\newblock In \emph{IEEE International Symposium on Computational Intelligence in Robotics and Automation}, pp.\ 146--151, 1997.

\bibitem[Yenamandra et~al.(2023)]{yenamandra2023homerobot}
Yenamandra, S., Ramachandran, A., Yadav, K., Wang, A., Khanna, M., Gervet, T., Yang, T.-Y., Jain, V., Clegg, A., Turner, J., et~al.
\newblock HomeRobot: Open-vocabulary mobile manipulation.
\newblock In \emph{Conference on Robot Learning}, 2023.

\bibitem[Zhu et~al.(2017)]{zhu2017target}
Zhu, Y., Mottaghi, R., Kolve, E., Lim, J.~J., Gupta, A., Fei-Fei, L., and Farhadi, A.
\newblock Target-driven visual navigation in indoor scenes using deep reinforcement learning.
\newblock In \emph{IEEE International Conference on Robotics and Automation}, pp.\ 3357--3364, 2017.

\end{thebibliography}

% =============================================================================
\appendix

\section{OWNB Benchmark Details}
\label{app:ownb}

The Open-World Navigation Benchmark (OWNB) is constructed from 200 environments: 120 from Matterport3D, 50 from HM3D, and 30 custom-built environments using ProcTHOR. Environment statistics are provided in Table~\ref{tab:ownb_stats}.

\begin{table}[h]
\centering
\caption{OWNB environment statistics.}
\label{tab:ownb_stats}
\begin{tabular}{lccc}
\toprule
\textbf{Statistic} & \textbf{Mean} & \textbf{Min} & \textbf{Max} \\
\midrule
Navigable area (m$^2$) & 142.3 & 28.5 & 892.1 \\
Number of rooms & 8.7 & 2 & 42 \\
Number of floors & 1.8 & 1 & 4 \\
Object categories & 47.2 & 12 & 156 \\
Dynamic agents & 3.1 & 0 & 12 \\
\bottomrule
\end{tabular}
\end{table}

\section{Hyperparameter Details}
\label{app:hyperparameters}

\begin{table}[h]
\centering
\caption{Complete hyperparameter configuration for Zen-Voyager training.}
\label{tab:hyperparams}
\begin{tabular}{llc}
\toprule
\textbf{Stage} & \textbf{Parameter} & \textbf{Value} \\
\midrule
\multirow{5}{*}{Perception} & Learning rate & $2 \times 10^{-5}$ \\
& Batch size & 256 \\
& Weight decay & 0.01 \\
& Warm-up steps & 2000 \\
& Total epochs & 50 \\
\midrule
\multirow{6}{*}{Exploration} & PPO clip $\epsilon$ & 0.2 \\
& Discount $\gamma$ & 0.99 \\
& GAE $\lambda$ & 0.95 \\
& Entropy coefficient & 0.01 \\
& Parallel environments & 256 \\
& Steps per environment & 500 \\
\midrule
\multirow{5}{*}{Navigation} & DAgger mixing start & 0.0 \\
& DAgger mixing end & 1.0 \\
& DAgger transition episodes & 10,000 \\
& Success reward & 10.0 \\
& Step penalty & $-0.01$ \\
\bottomrule
\end{tabular}
\end{table}

\section{Computational Cost Analysis}
\label{app:compute}

\begin{table}[h]
\centering
\caption{Computational cost breakdown for Zen-Voyager inference on NVIDIA Jetson AGX Orin.}
\label{tab:compute}
\begin{tabular}{lcc}
\toprule
\textbf{Module} & \textbf{Latency (ms)} & \textbf{Memory (MB)} \\
\midrule
Visual Encoder (Zen-7B) & 42.3 & 4,200 \\
Language Encoder & 8.1 & 320 \\
Cross-Modal Fusion & 9.6 & 180 \\
CGEM World Model & 12.4 & 890 \\
SOG Update & 8.2 & 48 \\
Hierarchical Planner & 14.8 & 120 \\
\midrule
\textbf{Total} & \textbf{95.4} & \textbf{5,758} \\
\bottomrule
\end{tabular}
\end{table}

With INT8 quantization of the visual encoder, total inference latency drops to 62ms (16 Hz), well within the real-time requirements for robotic navigation.

\end{document}
